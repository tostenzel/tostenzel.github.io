<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://www.tobiasstenzel.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.tobiasstenzel.com/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-09-10T12:36:17+00:00</updated><id>https://www.tobiasstenzel.com/feed.xml</id><title type="html">blank</title><subtitle>Tobias&apos; homepage.
</subtitle><entry><title type="html">8. Transformer</title><link href="https://www.tobiasstenzel.com/blog/2023/dl-transformer/" rel="alternate" type="text/html" title="8. Transformer" /><published>2023-03-08T00:00:00+00:00</published><updated>2023-03-08T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/dl-transformer</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/dl-transformer/"><![CDATA[<h2 id="transformer">Transformer</h2>

<p>In the last section we learned that RNNs have problems with learning
relations between the first parts of an input sequence with the output
and later parts of the input sequence. An architecture without this
structural problem is the transformer. The architecture was published by <d-cite key="vaswani_attention_2017"></d-cite> and applied to machine translation. We will
develop this model step-by-step, starting with its core component,
attention.</p>

<h3 id="attention">Attention</h3>

<p>Let us introduce this concept with an example. Figure
<a href="#fig:attention">7</a> shows an encoder-decoder network with attention similar to the previous
encoder-decoder RNN (Figure
<a href="https://www.tobiasstenzel.com/blog/2023/dl-rnn/#fig:encoder-decoder-rnn">6</a>). The core idea of attention is
defining the hidden state of the decoder-RNN as a function of every
hidden state from the encoder-RNN for every time period without
recursion. The result of the attention function is the context vector.
We will use this vector for every output element. In the specific
network in Figure <a href="#fig:attention">7</a>, the context vector is a function of both the
decoder states \(s\) and the encoder states \(h\). Further, it is
additionally concatenated with \(s\) to predict the output layer.</p>

<figure id="fig:attention">
<center><img src="/assets/img/dl-series/2h-attention.png" style="width:50%" /></center>
</figure>
<p><b>Figure 7. Encoder-Decoder with attention.</b> In contrast to the
encoder-decoder RNN, the output layer is a function of the concatenation
of the hidden states and a time-dependent context vector (black boxes).
The main idea is that the context vector <span class="math inline"><em>c</em><sub><em>t</em></sub></span> is a function
of all hidden states <span class="math inline">{<em>h</em>}<sub><em>t</em> = 1</sub><sup><em>T</em></sup></span>
instead of only the last one (and the previous state <span class="math inline"><em>s</em><sub><em>t</em> − 1</sub></span>). This
function is called <em>attention</em> (red). The black box represents
vector concatenation. <span class="math inline"><em>c</em><sub>1</sub></span> is initialized with
<span class="math inline"><em>h</em><sub>4</sub></span>, <span class="math inline"><em>s</em><sub>1</sub></span> with arbitrary values,
and <span class="math inline"><em>o</em><sub>1</sub></span> is
discarded.<br />
<br /></p>

<p>The attention function that returns the context vector for output \(y_t\)
wit attention scores for each input is given by the following
expression:</p>

\[\begin{align}c_t = \sum_{i=1}^n \alpha_{t,i} h_i\end{align}\]

<p>\(\alpha_{t,i}\) is a softmax function of another function <em>score</em> that
measure how well output \(y_t\) and input \(x_i\) are aligned through the
encoder state \(h_i\):</p>

\[\begin{align}\alpha_{t,i} = \text{align}(y_t,x_i) = \frac{\exp \text{score}(s_{t-1}, h_i)}{\sum_{j=1}^n\exp\text{score} (s_{t-1}, h_j)}.\end{align}\]

<p>There are many different scoring functions <d-cite key="weng_attention_2018"></d-cite>. A
common choice is the scaled dot-product score
\((s_t, h_i)=\frac{s_t^T h_i}{\sqrt{d}}\), where \(d\) is the hidden state
dimension of both encoder and decoder states. Here, the alignment score
for one sequence element is given by a relative score of the dot-product
between the respective encoder hidden state and the current decoder
hidden state. We scale down the dot product to prevent vanishing
gradients from a pass to a softmax layer. After training the model, we
can analyze how much each output element depends on, or <em>attends</em> to,
each input. We do this by assembling a table with outputs as columns and
the output-specific alignment scores for each input as rows.</p>

<p>Another option for an encoder-decoder with attention is using a
self-attention mechanism to compute the context vector, for example,
with score \((h_j, h_i)=\frac{h_j^T h_i}{\sqrt{d}}\). We can use the
scores, for instance, in machine translation, to model how important the
previous words are for translating the current word in a sentence. Note
that we can execute many of these operations in parallel for the whole
input and output sequences using matrix operations.</p>

<p>Next, we will discuss an expansion of the attention mechanism and how it
is applied to the transformer’s encoder and decoder, separately.
Finally, we will look at the complete model, and how it replaces the
positional information from the encoder RNN in a simple way without any
recurrence.</p>

<h3 id="key-value-and-query">Key, value and query</h3>

<p>As we do not use recurrence of single sequence
elements anymore, let us denote the whole sequence of input embeddings
by \(X \in \mathbb{R}^{L \times D^{(x)}}\). \(L\) can either be the complete
input length \(T_x\) or later only a fraction of it. \(D^{(x)}\) is the
input embedding’s length. Let us denoted the sequence of output
embeddings by \(Y \in \mathbb{R}^{M \times D^{(y)}}\). The transformer
uses an extension of the attention mechanism, the multi-head attention,
as its core building block. The first step is that, instead of using the
softmax of the scaled dot-product between encoder states \(h\) and decoder
states \(s\) directly as in the last section, it uses the scaled
dot-product with two different input encodings,
\(K=XW^k \in \mathbb{R}^{L \times D_k}\) and
\(V=XW^v \in \mathbb{R}^{L \times D_v}\), and an output encoding
\(Q=YW^q \in \mathbb{R}^{M \times D_k}\), with
\(W^k \in \mathbb{R}^{D^{(x)} \times D_k}, W^q \in \mathbb{R}^{D^{(y)} \times D_k}\)
and \(W^v \in \mathbb{R}^{D^{(x)} \times D_v}\). Note that source and
target embeddings are projected linearly into the same space. We compute
attention with</p>

\[\begin{align}c(Q,K,V)=\text{Softmax}\big(\frac{QK^T}{\sqrt{n}}\big)V.\end{align}\]

<p>We call (K,V) key-value pairs and Q the query. Using the interpretation
of the dot product as a similarity measure, the context matrix shows the
(self-)similarity between the input and a representation of the input
that is weighted by its similarity to the output (so far). \(c(Q,K,V)\) is
a matrix because we now compute the attention scores for every target
(query) dimension at once. However, we mask embeddings for unseen target
elements in every period. In the transformer encoder, there is an
important layer where the queries are also source representations and in
the decoder, there is layer where keys and values are also target
representation.</p>

<h3 id="multi-head-attention">Multi-head attention</h3>

<p>Instead of computing the attention once, the
multi-head approach splits the three input matrices into smaller parts
and then computes the scaled dot-product attention for each part in
parallel. The independent attention outputs are then concatenated and
linearly transformed into the next layer’s input dimension. This allows
us to learn from different representations of the current information
simultaneously with high efficiency.</p>

\[\begin{align}\text{MultiHead}(X_q, X_k, X_v)= [ \text{head}_1;...;\text{head}_h ] W^o,\end{align}\]

<p>where \(\text{head}_i=\)Attention\((X_q W^q_i, X_k W^k_i, X_v W^v_i)\) and \(W_i^q  \in \mathbb{R}^{D^{(y)} \times D_v /H}\), \(W_i^k \in \mathbb{R}^{D^{(x)} \times D_k / H}\), \(W_i^v \in \mathbb{R}^{D^{(x)} \times D_v /H}\)
are matrices to map input embeddings of chunk size \(L \times D\) into
query, key and value matrices. \(W^o \in \mathbb{R}^{D_v \times D}\) is
the linear transformation in the output dimensions. These four weight
matrices are learned during training. Target self-attention and cross
attention layers compute outputs in \(\mathbb{R}^{M \times D}\), and
source self-attention calculates outputs in \(\mathbb{R}^{L \times D}\).</p>

<h3 id="transformer-encoder">Transformer encoder</h3>

<p>Figure <a href="#fig:transformer-encoder">8</a> depicts the encoder network. It
computes an input representation based on the self-attention mechanism
that allows it to locate particular pieces of information from a large
context at all positions.</p>

<figure id="fig:transformer-encoder">
<center><img src="/assets/img/dl-series/2i-transformer-encoder.png" style="width:33%" /></center>
</figure>
<p><b>Figure 8. Transformer encoder.</b> In the original form, the encoder is a
stack of <span class="math inline"><em>N</em> = 6</span> identical
layers but with different parameters. It consists of two similar
components. The first sub-layer starts with a multi-head
<em>self</em>-attention layer (orange) and the second with a
<em>point-wise</em> fully-connected feed forward network (blue).
Point-wise means that the same weights are applied to each input
element. Afterwards, the respective previous input vector is added to
both outputs and the results are normalized by the normalized residual
layers (yellow). Crucially, in the self-attention layer, the queries are
also functions of the input embeddings. Adapted from <d-cite key="vaswani_attention_2017"></d-cite>.</p>

<h3 id="transformer-decoder">Transformer decoder</h3>

<p>Figure <a href="#fig:transformer-decoder">9</a> shows the decoder network. It is
able to retrieve relevant information from the encoded source
representation to compute feature representations for generating the new
target sequence in autoregressive fashion. The key component is the
multi-head <em>cross</em>-attention layer (in contrast to the other multi-head
<em>self</em>-attention blocks).</p>

<figure id="fig:transformer-decoder">
<center><img src="/assets/img/dl-series/2j-transformer-decoder.png" style="width:33%" /></center>
</figure>
<p><b>Figure 9. Transformer decoder. </b> In the original form, the encoder is a
stack of <span class="math inline"><em>N</em> = 6</span> identical
layers. It first encodes the output sequence in the masked multi-head
<em>self</em>-attention layer (orange). The masked elements are the
target representations that are not generated so far. Next, it passes
these target representations as queries to the multi-head attention
layer (orange) together with the output input representations as keys
and values. Finally, the results pass through a fully-connected feed
forward network (blue). Each of these three layers is subsequently
transformed by a normalized residual layer (yellow). Adapted from <d-cite key="vaswani_attention_2017"></d-cite>.</p>

<h3 id="the-complete-transformer-architecture">The complete transformer architecture</h3>

<p>Figure <a href="#fig:transformer-complete">10</a> shows the complete architecture. It
has the following properties:</p>

<ul>
  <li>
    <p><strong>Inductive bias for self-similarity:</strong> The self-attention layers
allow the model to detect reoccuring themes independent of their
distances from each other. This works well in many applications
becaue reoccurence is an important pattern in numerous real-world
domains.</p>
  </li>
  <li>
    <p><strong>Expressive forward pass:</strong> The transformer interacts all elements
of the input and output with themselves and each other in relatively
simple and direct connections. This allows the model to learn many
algorithms in just a few steps.</p>
  </li>
  <li>
    <p><strong>Wide and shallow compute graph:</strong> Due to the residual layers and
the matrix products in the attention layers the compute graph is
wide and shallow. This makes forward and backward passes fast on
parallel hardware. Furthermore, together with layer normalization
and dot product scaling, this mitigates vanishing or exploding
gradients in backpropagation.</p>
  </li>
</ul>

<figure id="fig:transformer-complete">
<center><img src="/assets/img/dl-series/2k-transformer-complete.png" style="width:66%" /></center>
</figure>
<p><b>Figure 10. The complete transformer.</b> In the original form, both the
source and target sequence are passed to embedding layers to produce a
vector of length <span class="math inline"><em>D</em> = 512</span> for
every element. To preserve the ordering information of the inputs, we
add a respective sinusoidal positional encoding vector to every
embedding. To compute the probabilities for each element in the output
space at every position, we pass the decoder output through a linear and
a softmax layer. Adapted from <d-cite key="vaswani_attention_2017"></d-cite>.</p>
<h3 id="complexity-comparison">Complexity comparison</h3>

<p>Let us conclude this chapter by comparing the complexities of the three main architectures in deep learning. Table
<a href="#fig:comparison">2</a> shows the differences in the number of FLOPS (floating point operations)
for the main units in the tree main architectures that we have discussed
so far. We observe that attention scales better than the other units if
the length of the input sequence is much smaller than the depth of each
elements embeddings. This applies to tasks like machine translation in <d-cite key="vaswani_attention_2017"></d-cite> or question-answering. However, for direct
applications to image data, this property does not hold. For instance,
the length from a CIFAR image equals \(32 \cdot 32 \cdot 3 = 3072\).
Therefore, applying a transformer to image data in order to profit from
its advantages requires architectures that reduce the input length
beforehand.</p>

<figure id="fig:comparison">
<center><img src="/assets/img/dl-series/complexity-comparison.png" style="width:55%" /></center>
</figure>
<p><b>Table 2. Comparison of computation complexity between models.</b> length refers
to the number of elements in the input sequence and dim to the
embedding depth for each element. Entries adapted from <d-cite key="vaswani_transformers_2021"></d-cite>.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = &quot;Deep Learning Series&quot;,
  author  = &quot;Stenzel, Tobias&quot;,
  year    = &quot;2023&quot;,
  url     = &quot;https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>]]></content><author><name>Tobias Stenzel</name></author><category term="dl-fundamentals" /><category term="neural-net-archetype" /><category term="attention" /><summary type="html"><![CDATA[⏮️ I recommend reading the RNN post first for the encoder-decoder architecture.]]></summary></entry><entry><title type="html">Trackformer — Multi-Object Tracking with Transformers</title><link href="https://www.tobiasstenzel.com/blog/2023/trackformer/" rel="alternate" type="text/html" title="Trackformer — Multi-Object Tracking with Transformers" /><published>2023-03-08T00:00:00+00:00</published><updated>2023-03-08T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/trackformer</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/trackformer/"><![CDATA[<p>This blog post provides a detailed explanation of the transformer-based tracking model Trackformer <d-cite key="meinhardt_trackformer_2022"></d-cite>. Trackformer not only achieved state-of-the art results but has also a comparably simple architecture that facilitates implementation and maintenance for practitioners. The coolest thing about this blog post is the inclusion of two self-made images that depict Trackformer’s training process for different scenarios where objects are (re-)entering, leaving, or moving in a camera-recorded, dynamic scene.<br /></p>

<p>Multi-object tracking (MOT) is an important task in the Computer Vision domain. MOT is closely related to object detection because tracking models utilize the detections in each frame of a video with the aim of assigning the same ID to the same objects across different frames over time. Trackformer uses the transformer-based object detector DETR <d-cite key="carion_end--end_2020"></d-cite>. The primary concept behind DETR is to learn a larger number of transformer-decoder embeddings that generate bounding boxes (by cross-attending to the image features), with a set of these embeddings successfully matched to the actual objects in an image. The fundamental idea behind Trackformer is to feed the output embeddings from successfully matched embeddings to the transformer for the next image in a video sequence and train these embeddings to match the same object again. In this configuration, the intertemporally matched embeddings carry the identity of an object over to the next image.<br /></p>

<p>We will delve into both models in more detail separately. First, we will explore DETR <d-cite key="carion_end--end_2020"></d-cite>. I will explain the complete model during inference time, then during training time, including the loss function, and subsequently, we will discuss advantages, disadvantages, and results. Second, we will repeat the same process for Trackformer <d-cite key="meinhardt_trackformer_2022"></d-cite>. Finally, we will learn about Deformable DETR <d-cite key="zhu_deformable_2021"></d-cite>, which is a more efficient version of DETR. Deformable DETR employs deformable attention instead of regular attention. Trackformer actually utilizes Deformable DETR for its detections. The Deformable DETR paper is quite technical, so feel free to skip this part!<br /><br /></p>

<figure id="fig:object-queries">
<center><img src="/assets/img/dl-series/paper-detr.png" style="width:66.6%" /></center>
</figure>

<h3 id="1-detr-as-object-detector">1) DETR as Object Detector</h3>

<p>Previous object detection systems incorporate various manually designed
elements, such as anchor generation, rule-based training target
assignment, and non-maximum suppression (NMS) post-processing <d-cite key="liu_deep_2018"></d-cite>. These components do not constitute fully end-to-end
solutions. In a recent study, <d-cite key="carion_end--end_2020"></d-cite> introduced DETR,
an object detection approach that eliminates the need for such manual
components. DETR represents the first fully end-to-end object detector
and achieved highly competitive results on the COCO 2017 detection
dataset <d-cite key="lin_microsoft_2014"></d-cite>. The two main components are a set-based
global loss that enforces a subset of unique class predictions via
bipartite matching, and a transformer encoder-decoder architecture with
learned embeddings called <em>object queries</em>. In contrast to <d-cite key="vaswani_attention_2017"></d-cite>, these queries are learned encodings of
potential objects on the image instead of rule-based encoded
representations of the target sequence. This section explains first the
transformer at inference time, and second the bipartite matching loss
with which we train the model.</p>

<h3 id="complete-transformer-based-detr-architecture">Complete transformer-based DETR architecture</h3>

<p>The overall model has
three components as depicted by Figure
<a href="#fig:object-queries">1</a>. The first component is a conventional
CNN backbone that generates a lower-resolution activation map many
channels. The transformation from large embedding length (image height
\(\cdot\) width) to large embedding depth is crucial for the efficiency of
the attention modules (see
Table <a href="https://www.tobiasstenzel.com/blog/2023/dl-transformer/#complexity-comparison">2</a>. The second component is the transformer
encoder-decoder. In contrast to the original decoder by <d-cite key="vaswani_attention_2017"></d-cite>, DETR decodes \(N\) objects at once at each
decoder layer instead of predicting the output sequence in an
autoregressive manner by masking later elements. Because the decoder is
permutation-invariant like the encoder, we also require \(N\) different
decoder input embeddings to generate different results. We achieve this
by feeding learned embeddings with dimension \(d\) that we call <em>object
queries</em> to the decoder. In contrast to that, the original transformer
in <d-cite key="vaswani_attention_2017"></d-cite> takes the decoder outputs from the previous
iteration as decoder inputs. The decoder transforms the \(N\) object
queries into \(N\) output embeddings. The third component is a two-headed
prediction network that is shared for all output embeddings. It is
defined as an FFN, i.e. a a 3-layer vanilla neural net with ReLU
activations for the normalized bounding box values and hidden dimension
\(d\). We normalize the input to the prediction FFNs with a shared
layer-norm. The one head is a linear projection layer to predict the
bounding box values.
The other head is a softmax layer that predicts the
class labels.<br /></p>

<figure id="fig:object-queries">
<center><img src="/assets/img/dl-series/4b-detr-object-queries.png" style="width:100.0%" /></center>
</figure>
<p><b>Figure 1. High-level DETR architecture with CNN backbone,
transformer-encoder decoder and FFN prediction heads.</b> In contrast to the
autoregressive sequence generation in <d-cite key="vaswani_attention_2017"></d-cite>, the outputs are computed at
once by feeding different learned positional encodings, called
<em>object queries</em>, into the transformer decoder. Image source: <d-cite key="carion_end--end_2020"></d-cite>.<br /><br /></p>

<p>The transformer allows the model to use self- and cross-attention over
the object queries to include the information about all potential
objects with pair-wise relations in their prediction of only one
potential object, while using the whole image as context.</p>

<h3 id="object-detection-set-prediction-loss">Object Detection Set Prediction Loss</h3>

<p>During training, we want to
learn predictions that have the right number of no classes and the right
number of classes with the right class label and bounding box. Formally,
we achieve this the following way: From the transformer decoder
prediction head, DETR infers \(N\) predictions for the tuples of bounding
box coordinates and the object class. \(N\) has to be at least as large as
the maximal number of objects on one image. The first tuple element are
the bounding box coordinates, denoted by \(b \in [0,1]^4\). These are four
values: the image-size normalized center coordinates, and the normalized
height and width of the box w.r.t. the input image’s borders. Using
center coordinates and normalization help us to deal with images of
different sizes. Examples for the second tuple element, class \(c\), are
"person A", for person detection, or "car" for object detection. The
remaining class is the "no object" class, denoted by \(\emptyset\). For
every image, we pad the target class vector \(c\) of length \(N\) with
\(\emptyset\) if the image contains less than \(N\) objects. To score
predicted tuples
\(\hat{y}=\{\hat{y}_i\}_{i=1}^N=\{(\hat{c}_i,\hat{b}_i)\}_{i=1}^N\) with
respect to the targets \(y\), <d-cite key="carion_end--end_2020"></d-cite> apply a loss
function that we apply to a class permutation based on an <em>optimal</em>
bipartite matching between the predicted and target tuples. With this
loss function, we jointly maximize the log likelihood of the class
permutation and minimize the bounding box losses.
Figure <a href="#fig:bipartite-matching">2</a> depicts an example of bipartite
matching between predictions and ground truth for a picture of two
seagulls at a shore during training time. We observe that the matching
procedure selects a unique permutation that directly maps exactly one
prediction to one target. Thus, with that approach we do not have to
handle mappings of many similar predicted bounding boxes and classes to
one target, for instance, to the same seagull.</p>

<p><br /></p>

<figure id="fig:bipartite-matching">
<center><img src="/assets/img/dl-series/4a-detr-bipartite-matching.png" style="width:100.0%" /></center>
</figure>
<p><b>Figure 2. DETR training pipeline with bipartite matching loss.</b> The
loss function generates an optimal one-to-one mapping between
predictions and targets according to bounding box and object class
similarity (colors). In this unique permutation of <span class="math inline"><em>N</em></span> classes, class predictions with no
match among the class targets are regarded as “no object” predictions,
too (green). Image source: <d-cite key="carion_end--end_2020"></d-cite>.</p>

<p><br /><br /></p>

<p>The next section describes the loss function required for the bipartite
matching between predicted and target detections that is optimal in
terms of bounding box and class similarity. We find the optimal
permutation of predicted detections
\(\hat{\sigma}\in\mathcal{\mathfrak{S}}_N\) of \(N\) elements from:</p>

\[\begin{align}
    \hat{\sigma} = arg\,min_{\sigma\in\mathcal{\mathfrak{S}}_N} \sum_{i}^{N} \text{L}_{\text{match}}(y_i, \hat{y}_{\sigma(i)}),
\end{align}\]

<p>where \(\text{L}_{\text{match}}\) is a pair-wise matching cost between
target \(y_i\) and prediction \(\hat{y}\) with index \(\sigma(i)\). We compute
the assignment efficiently with the Hungarian matching algorithm <d-cite key="kuhn_hungarian_1955"></d-cite> instead of brute force.</p>

<p>We want to assign predictions and targets that are close in terms of
class and bounding box. Thus, the matching considers both the class
score for every target and the similarity of predicted and target box
coordinates. We reward a high class score for the target class and a
small bounding box discrepancy. To this end, let us denote the index function by \(\text{I}\), the
probability of predicting class \(c_i\) for detection with permutation
index \(\sigma(i)\) by \(\hat{p}_{\sigma(i)}(c_i)\) and the predicted box
by \(\hat{b}_{\sigma(i)}\). With that, we define the pair-wise matching
loss as</p>

\[\begin{align}
\text{L}_{\text{match}}(y_i, \hat{y}_{\sigma(i)}) = -\text{I}_{c_i\neq\emptyset} \hat{p}_{\sigma (i)}(c_i) + \text{I}_{c_i\neq\emptyset} \text{L}_{\text{box}}(b_{i}, \hat{b}_{\sigma(i)}).
\end{align}\]

<p>Here, our objective is to find the best matching for the "real"
classes \(c \neq \emptyset\) by ignoring class predictions that are
directly mapped to "no class" by the prediction. However, the model
still has to learn not to predict too many real classes. Given our
optimal matching \(\hat{\sigma}\), we achieve this by minimizing the
<em>Hungarian loss</em>. The function is given by</p>

\[\begin{align}\text{L}_{\text{Hungarian}}(y, \hat{y}) = \sum_{i=1}^N \left[-\log  \hat{p}_{\hat{\sigma}(i)}(c_{i}) + \text{I}_{c_i\neq\emptyset} \text{L}_{\text{box}}\Big(b_{i}, \hat{b}_{\hat{\sigma}}(i)\Big)\right]\end{align}\]

<p>The difference to the matching loss is two-fold. First, we now penalize
wrong assignments to all classes including "no class" with the first
class-specific term. We do not include the "no class" instances to the
box loss \(\text{L}_{\text{box}}\) because they are not matched to a
bounding box anyway. Second, we scale the class importance compared to
the bounding boxes by taking the log of predicted class probabilities.
In practice, the class term is further reduced for "no class" objects
to take class imbalance into account. The last expression is the box
loss. It is the L1 loss of the bounding box vector \(b_i\). Note, however,
that the L1 loss penalizes larger boxes.
The following equations shows the expression:</p>

\[\begin{align}
    \text{L}_{\text{box}}(b_{i}, \hat{b}_{\sigma(i)}) =  \lambda_{\text{box}}||b_{i}- \hat{b}_{\sigma(i)}||_1,
\end{align}\]

<p>where \(\lambda_{\text{box}} \in \mathbb{R}\) is a hyperparameter. We
normalize the loss by the number of objects in each image.</p>

<h3 id="detrs-drawbacks">DETR’s drawbacks</h3>

<p>In spite of its intriguing design and commendable
performance, DETR encounters certain challenges. Firstly, it requires
significantly more training epochs to reach convergence compared to
existing object detectors. For instance, when evaluated on the COCO
benchmark <d-cite key="lin_microsoft_2014"></d-cite>, DETR necessitates 500 epochs for
convergence, making it approximately 10 to 20 times slower than Faster
R-CNN <d-cite key="ren_faster_2015"></d-cite>. Secondly, DETR exhibits relatively lower
proficiency in detecting small objects <d-cite key="zhu_deformable_2021"></d-cite>.
Contemporary object detectors typically utilize multi-scale features,
employing high-resolution feature maps for the detection of small
objects. However, employing high-resolution feature maps leads to
impractical complexities for DETR. These aforementioned issues primarily
stem from the deficiency of Transformer components in processing image
feature maps. During initialization, the attention modules distribute
nearly uniform attention weights to all pixels in the feature maps. It
requires many training epochs for the attention weights to learn to
concentrate on sparse meaningful locations. Additionally, the
computation of attention weights in the Transformer encoder is quadratic
in relation to the number of pixels. Consequently, processing
high-resolution feature maps becomes highly computationally and memory
intensive.<br /><br /></p>

<figure id="fig:object-queries">
<center><img src="/assets/img/dl-series/paper-trackformer.png" style="width:66.6%" /></center>
</figure>

<h3 id="2-trackformer-as-multi-object-tracker">2) Trackformer as Multi-Object Tracker</h3>

<p>Trackformer <d-cite key="meinhardt_trackformer_2022"></d-cite> extends DETR to multi-object
tracking. To be precise, it uses the variant Deformable DETR <d-cite key="zhu_deformable_2021"></d-cite>, presumably because the results were better.
Trackformer not only achieved state of the art results for online
tracking but also presented an end-to-end architecture that solves the
three sub-tasks of track initialization (detection), prediction of next
positions, and matching predictions with detections. Thereby, it
bypasses intermediate layers that are usually present in previous
pipeline designs, similar to how DETR facilitated object detection. The
main idea is depicted by
<a href="#fig:trackformer">3</a>. It is to re-use DETR’s <em>decoded</em> object
queries that have been matched to an actual object in one frame and use
them as additional object queries for the next frame as <em>autoregressive
track queries</em>. Accordingly, we dynamically adjust the transformer
decoder sequence length. The static object queries are responsible for
initializing new tracks and the taken over track queries allow tracking
objects across frames. In contrast do DETR, besides the bounding box
quadruple \(b\) and the object class \(c\), we additionally predict predict
the track ID across frames in an implicit way from by enumerating the
track queries.</p>

<p>With the described approach, we train the model to not only decode
learned object queries into representations that can detect objects but
also to use the decoded queries again as decoder input to detect the
same object when possible. If we match the decoder output from an object
query in frame \(t-1\), it is re-used as an additional track query as
decoder input for frame \(t\). If its output is matched again, we assume
that both detections belong to the same object with ID \(k\).</p>

<p><br /><br /></p>

<figure id="fig:trackformer">
<center><img src="/assets/img/dl-series/4-trackformer.png" style="width:100.0%" /></center>
</figure>
<p><b>Figure 3.Trackformer architecture.</b> Trackformer extends DETR to
tracking on video data by feeding the decoded object queries that are
matched to actual objects as additional <em>autoregressive tracking
queries</em> (colored detection squares) next to the object queries for
the next image into the transformer decoder (dark blue). The decoder
processes the set of <span class="math inline"><em>N</em><sub>track</sub> + <em>N</em><sub>object</sub></span>
queries to further track or remove existing tracks (light blue) and to
initialize new tracks (purple). Image source: <d-cite key="meinhardt_trackformer_2022"></d-cite>.</p>

<h3 id="set-prediction-loss">Set prediction loss</h3>

<p>We now want to formulate a loss that allows the
model to learn the bipartite matching \(j=\pi (i)\) between target objects
\(y_i\) to the set of both object and track query predictions \(\hat{y}_j\).
For this purpose, let us denote the subset of target track identities at
frame \(t\) with \(K_t \subset K\). This is different from DETR as \(K\)
contains all object identities for all images in the video sequence.
These object or track identities can be present in multiple frames,
i.e. they can intersect from frame to frame. Trackformer takes three
steps to associate queries with targets. The last step corresponds to
DETR’s method. The steps to obtain the mapping of predicted detections
to target detections \(\hat{\sigma}\) for one frame are the following:
first, we match \(K_{\text{track}} = K_{t-1} \cap K_t\) (target objects in
the current frame that were also present in the previous frame) by track
identity. This means, we associate these targets with the output from
the previous query. Second, we match
\(K_{\text{leaving}} = K_{t-1} \setminus K_t\) (objects leaving the scene
between two frames) with background class \(\emptyset\). And third, we
match \(K_{\text{init}} = K_{t} \setminus K_{t-1}\) (objects entering the
scene) with the \(N_{\text{object}}\) object queries by minimum cost
mapping based on object class and bounding box similarity the same ways
as DETR assigned its targets to object queries.</p>

<p>Output embeddings which were not matched, i.e. 1) proposals with worse
class and bounding box similarity than others or 2) track queries
without corresponding ground truth object, are assigned to background
class \(\emptyset\).</p>

<p>With this order, we prioritize matching track queries from the last
frame even if object queries from the current frame yield more similar
bounding boxes and classes. This is necessary because, in order to
assign the same object ID to detections in multiple frames, we have to
train the object queries to not only initialize a track after on pass
through the decoder but also to detect the same object after two passes
through the decoder with given the respective interactions from the
image encodings.</p>

<p>The final set prediction loss for one frame is computed over all
\(N=N_{\text{object}}+N_{\text{track}}\) model outputs. Because
\(K_{t-1} \setminus K_t\) (objects that left the scene) are not contained
in the current-frame permutation, we write the loss as</p>

<p>\(\begin{align}
\text{L}_{\text{MOT}}(y,\hat{y},\pi)=\sum_{i=1}^N \text{L}_{\text{query}}(y,\hat{y},\pi).
\end{align}\)
Further, we define the loss per query and differentiate two categories.
First, we have the object query loss \(L_0\) for outputs from unmatched
embeddings. And second, we have the track query loss \(L_1\) for outputs
from matched embeddings that will be overtaken to the next time period
as track queries. Formally, the query loss is given by</p>

\[\begin{align}\text{L}_{\text{query}}=
    \begin{cases}
        \phantom{.}L_0 &amp;= -\lambda_{\text{cls}} \log \hat{p}_i (\emptyset) \quad \phantom{..................}\text{if } i \notin \pi \\
        \phantom{.}L_1 &amp;= -\lambda_{\text{cls}} \log \hat{p}_i (c_{\pi=i}) + \text{L}_{\text{box}}(b_{\pi=i},\hat{b}_i) \quad \text{if } i \in \pi.
    \end{cases}\end{align}\]

<p>The expression captures two features: first, \(L_1\) rewards track queries
that find the right bounding box for objects that are still present on
the current frame and it rewards object queries with similar outputs to
new objects. Second, \(L_0\) not only rewards track queries that predict
the background class if their object has left the scene but also object
queries that predict the background class if their bounding box
prediction is off. The discussed details about track and object queries,
and the matching rules with examples for assigned bounding boxes and
losses are depicted in
<a href="#fig:trackformer_query_t0t1">4</a>.</p>

<p><br /><br /></p>

<figure id="fig:trackformer_query_t0t1">
<center><img src="/assets/img/dl-series/4_trackformer_t1t2.png" style="width:100.0%" /></center>
</figure>
<p><b>Figure 4. Training track and object queries.</b> The black boxes are
ground truth detections and the colorful, annotated boxes are
predictions from the respective output embedding. Embeddings that do not
spawn a box predict a class score smaller than the threshold. In t=0,
the most similar boxes (green and red) are matched with the two target
boxes according to matching step 2: <span class="math inline"><em>K</em><sub>init</sub></span> (symbolized by
“<span class="math inline">/</span>”). For these boxes, we compute <span class="math inline"><em>L</em><sub>1</sub></span> based on boxes and
class scores, and for the unmatched boxes, we compute <span class="math inline"><em>L</em><sub>0</sub></span> solely based on the
class scores. Then we update the model parameters accordingly. The
matched output embeddings are taken over as additional input embeddings,
carrying the object IDs from the objects on the previous image. They are
matched with priority according to matching step 1: <span class="math inline"><em>K</em><sub>track</sub></span> (symbolized by
“<span class="math inline">∩</span>”). Embedding 2’ is matched although
the bounding boxes from output embedding <span class="math inline"><em>a</em>′</span> is more similar to the target. We
feed embeddings <span class="math inline">1′</span> and <span class="math inline">2′</span> to the decoder in period <span class="math inline"><em>t</em> = 2</span> (see Figure 5).</p>

<h3 id="track-query-re-identification">Track query re-identification</h3>

<p>What happens if objects are occluded
or re-enter the scene? To deal with such cases, we keep feeding
previously removed track queries for a <em>patience window</em> of
\(T_{track-reid}\) frames into the decoder. During this window,
predictions from track ids are only considered if a classification score
higher than \(\sigma_{track-reid}\) is reached.</p>

<figure id="fig:trackformer_query_t2t3">
<center><img src="/assets/img/dl-series/4_trackformer_t3t4.png" style="width:100.0%" /></center>
</figure>
<p><b>Figure 5. Training track queries with re-identification. Make sure to
compare this Figure to Figure 4.</b>
From <span class="math inline"><em>t</em> = 1</span>, we additionally
feed output embeddings <span class="math inline">1′</span> and <span class="math inline">2′</span>, carrying the object identities for
pedestrian 1 (green) and pedestrian 2 (blue) to the encoder. However,
pedestrian 2 has left the scene between <span class="math inline"><em>t</em> = 1</span> and <span class="math inline"><em>t</em> = 2</span> and pedestrian 1 is occluded
by a news pedestrian with ID 3. Here, we depict the case where we have
no ground truth annotation for the occluded pedestrian. Note that the
prediction from embedding <span class="math inline">1′</span> is quite
reasonable given that pedestrian 1 is almost invisible. In contrast,
embedding <span class="math inline">2′</span> keeps predicting a
bounding box in the close to the upper right corner independent of the
image. Since there are no detections with IDs previously matched to 1 or
2, we cannot apply matching step 1: <span class="math inline"><em>K</em><sub>track</sub></span> (symbolized by
“<span class="math inline">∩</span>”) to output embeddings <span class="math inline">1″</span> and <span class="math inline">2″</span>.
Instead, we match both outputs with the background class according to
step 2 <span class="math inline"><em>K</em><sub>leaving</sub></span>
(symbolized by “<span class="math inline">∖</span>”). Assuming the green
pedestrian would be annotated, however, we would have apply step 1:
<span class="math inline"><em>K</em><sub>track</sub></span> (symbolized
by “<span class="math inline">∩</span>”) to embedding <span class="math inline">1″</span> instead. To track the new pedestrian with
ID 3, we apply matching step 3: <span class="math inline"><em>K</em><sub>init</sub></span> (symbolized by
“<span class="math inline">/</span>”) to embedding <span class="math inline">3</span> and update the model according to the
respective losses. Since we keep unmatched embeddings with patience, we
take over embeddings <span class="math inline">1″</span> and <span class="math inline">2″</span> to the next frame in addition to embedding 3. This allows the model to re-identify object 1 (green) in period <span class="math inline"><em>t</em> = 3</span>.</p>

<h3 id="results">Results</h3>

<p>Trackformer achieved state-of-the-art performance in
multi-object tracking on MOT17 and MOT20 datasets. It is important to pre-train the model on large tracking datasets, to use track augmentations, and probably also to use pre-trained detection weights for tracking-specific pre-training.<br /><br /></p>

<figure id="fig:object-queries">
<center><img src="/assets/img/dl-series/paper-deformable-detr.png" style="width:66.6%" /></center>
</figure>

<h3 id="3-extra-deformable-detr-as-more-efficient-detector">3) Extra. Deformable DETR as More Efficient Detector</h3>

<p>In order to tackle these challenges, <d-cite key="zhu_deformable_2021"></d-cite> introduces a
deformable attention module as replacement for the conventional
attention module in DETR’s transformer model. Drawing inspiration from
deformable convolution <d-cite key="dai_deformable_2017"></d-cite> <d-cite key="zhu_deformable_2021"></d-cite>, the
deformable attention module focuses its attention solely on a limited
set of key sampling points surrounding a reference point. By allocating
a fixed number of keys per query, <d-cite key="zhu_deformable_2021"></d-cite> can alleviate the
problems associated with convergence and feature spatial resolution by
decreasing the transformer’s complexity as a function of the image
dimension to a sub-quadratic level. Furthermore, <d-cite key="zhu_deformable_2021"></d-cite>
use this module to aggregate multiple feature maps of different
resolution taken from the CNN backbone in the encoder to "multi-scale"
feature maps and for the object queries to aggregate the relevant
information from these maps for the detection predictions. The design is
inspired by the finding that multi-scale feature maps are crucial for
teaching image transformers to effectively represent objects depicted at
strongly distinct scales <d-cite key="lin_feature_2017"></d-cite>.
Figure <a href="#fig:deformable-detr">6</a> depicts the complete Deformable DETR
architecture.<br /></p>

<figure id="fig:deformable-detr">
<center><img src="/assets/img/dl-series/4d-deformable-attention.png" style="width:100.0%" /></center>
</figure>
<p><b>Figure 6. High-level Deformable DETR architecture.</b> We extract three
feature maps at different resolution levels from the CNN backbone. In
the encoder, we aggregate the information from all multi-scale feature
maps with deformable self-attention, attending only to a learned sample
of important locations around a learned reference point from every
feature map for each feature map (deformable attention). This results in
three encoder feature maps of the same dimensions. In the decoder, the
learned object queries extract features from queries and values from
themselves with conventional self-attention and from the keys from the
encoder feature maps with deformable cross-attention. Again, for each
object query, the learned reference point and a set of learned offsets
is used to only query a set of keys from the encoder. With that, we
replace the self-attention modules in the encoder and the and
cross-attention modules in the decoder with 4 heads of the deformable
attention module and learn from feature maps at different resolutions.
This makes the transformer’s complexity as a function of pixel number
sub-quadratic and allows the model to better learn objects at strongly
distinct scales. Image source: <d-cite key="zhu_deformable_2021"></d-cite>.<br /><br /></p>

<h3 id="multi-head-attention-revisited">Multi-Head Attention Revisited</h3>

<p>When provided with a query element
(e.g., a target word in the output sentence) and a set of key elements
(e.g., source words in the input sentence), the multi-head attention
module aggregates the key contents based on attention weights, which
gauge the compatibility of query-key pairs. In order to enable the model
to focus on diverse representation subspaces and positions, the outputs
of various attention heads are combined linearly using adjustable
weights.</p>

<p>Let \(q \in \Omega_q\) denote an index for the query element represented
by feature \(z_q \in \mathbb{R}^C\), and let \(k \in \Omega_k\) denote an
index for the key element represented by feature \(x_k \in \mathbb{R}^C\)
with feature dimension \(C\) and set of query and key elements \(\Omega_q\)
and \(\Omega_k\), respectively. We compute the multi-head attention
feature for query index \(q\) with</p>

\[\begin{align}\text{MultiHeadAttn}(z_q, x) = \sum_{m=1}^M W_m [\sum_{k \in \Omega_k} A_{mqk} \cdot W_m^{T} x_k],\end{align}\]

<p>where \(m\) is index the attention head and
\(W_m \in \mathbb{R}^{C \times C_v}\) are learned weights with
\(C_v = C/M\). We normalize the attention weights
\(A_{mqk} \propto \text{exp}\{\frac{z_q^T U_m^T V_m x_k}{\sqrt{C_v}}\}\)
with learned weights \(U_m, V_m \in \mathbb{R}^{C \times C_v}\) to
\(\sum_{k \in \Omega_k} A_{mqk}=1\). In order to clarify distinct spatial
positions, the representation features, denoted as \(z_q\) and \(x_k\), are
formed by concatenating or summing the element contents with positional
embeddings.</p>

<h3 id="deformable-attention">Deformable Attention</h3>

<p>The main challenge in applying Transformer
attention to image feature maps is that it considers all potential
spatial locations. To overcome this limitation, <d-cite key="zhu_deformable_2021"></d-cite>
propose a deformable attention module. It selectively focuses on a small
number of key sampling points around a reference point, irrespective of
the spatial dimensions of the feature maps. By employing a small number
of keys per query, it can address the issues of convergence and feature
spatial resolution. In contrast to the previous notation, let
\(x \in \mathbb{R}^{C \times H \times W}\) denote an input feature map and
let \(q\) denote the index for a query element with feature \(z_q\) and a
2-d reference point \(p_q\). We calculate the deformable attention feature
with</p>

\[\begin{align}\text{DeformAttn}(z_q, p_q, x) = \sum_{m=1}^M W_m [\sum_{k \in \Omega_k} A_{mqk} \cdot W_m^{T} x_k (p_q + \Delta p_{mqk})],\end{align}\]

<p>where \(k\) indexed the sampled keys and \(K   \ll  HW\) is the total number
of sampled keys. Futher, \(\Delta p_{mqk} \in \mathbb{R}^2\) denotes the
sampling offset and \(A_{mkq}\) the attention weights of the
\(k^{\text{th}}\) sampling point in the \(m^{\text{th}}\) attention head,
respectively. The attention weights are normalized to own over the
sample keys. We feed query feature \(z_q\) to a \(3MK\)-channel linear
projection operator. The inital \(2MK\) channels encode the sampling
offsets \(Delta p_{mkq}\), and the latter \(MK\) channels are used as input
to the softmax function to compute the attention weights \(A_mqk\).</p>

<h3 id="multi-scale-deformable-attention">Multi-scale Deformable Attention</h3>

<p>We extend the deformable attention
module to multiple differently-scaled feature maps with a few small
changes. Let \(\{x^l\}_{l=1}^L\) denote the set of differently scaled
feature maps, where \(x^l \in \mathbb{R}^{C \times H_l \times W_l}\) and
let \(\hat{p}_q \in [0,1]^2\) denote the normalized reference points for
each query element element \(q\). Then, we calculate the multi-scale
deformable attention with</p>

\[\begin{gathered}
\text{MSDeformAttn}(z_q, \hat{p}_q, \{x^l\}_{l=1}^L) = \\
\sum_{m=1}^M W_m [\sum_{l=1}^L \sum_{k \in \Omega_k} A_{mlqk} \cdot W_m^{T} x^l (\phi_l (\hat{p}_q ) + \Delta p_{mlqk})],
\end{gathered}\]

<p>where \(l\) indexes the input feature and we expand the normalized
attention weights and the reference point by this dimension.
\(\hat{p}_q  \in [0,1]^2\) are also normalized coordinates with \((0,0)\)
and \((1,1)\) as top-left and bottom-right coordinates, respectively.
Function \(\phi_l(\hat{p}_q )\) is the inverse of the normalization
function and maps \(\hat{p}_q\) back to the respective feature map
coordinates. In constrast to the singlescale deformable attention
module, we sample \(LK\) feature map points instead of \(K\) points and
interact the different feature maps with each other.</p>

<h3 id="results-1">Results</h3>

<p>On the COCO benchmark for object detection, Deformable DETR
outperforms DETR, particularly in detecting small objects, while
requiring only about one-tenth of the training epochs.</p>]]></content><author><name>Tobias Stenzel</name></author><category term="applications" /><category term="transformer" /><category term="MOT" /><category term="multiple-object-tracking" /><summary type="html"><![CDATA[A lower-level explanation of Meinhardt et al. (2022)'s paper about applying the transformer to multi-object tracking (MOT)]]></summary></entry><entry><title type="html">7. Recurrent Neural Networks</title><link href="https://www.tobiasstenzel.com/blog/2023/dl-rnn/" rel="alternate" type="text/html" title="7. Recurrent Neural Networks" /><published>2023-03-07T00:00:00+00:00</published><updated>2023-03-07T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/dl-rnn</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/dl-rnn/"><![CDATA[<h2 id="recurrent-neural-networks">Recurrent Neural Networks</h2>

<p>Many tasks require input or output spaces that contain sequences. For
instance, in translation programs we oftentimes encode words as
sequences of one-hot vectors. These vectors are index vectors with a one
at the position of an integer that maps to a word in a fixed vocabulary.
A simple recurrent neural network (RNN) processes a sequence of vectors
\(\{x_1, ..., x_T\}\) with a recurrence formula
\(h_t = f_\theta(h_{t-1},x_t)\). The function \(f\) that we will describe in
more detail below takes the same parameters \(\theta\) at every time step
to process an arbitrary number of vectors (cf. parameter sharing and
inductive bias). An interpretation of the hidden vector \(h\) is that of a
summary of all previous \(x\) vectors. A common initialization is
\(h_0=\vec{0}\). We can define \(f\) according to three criteria. The first
criterion is the input and output space. We would model \(f\) differently
for spaces from one to many, many to one or many to many vectors. The
second criterion is the order in which we process input vectors and
predict output vectors. This depends on the nature of the data. For
example, we may either want to predict \(y_t\) directly after processing
\(x_t\), or predict the complete output sequence \(\{y_1, ..., y_T\}\) after
we have processed the complete input sequence \(\{x_i\}_{i=1}^T\). Another
option is to not only compute a summary of the past but also a summary
of the future and use both vectors in our predictions. The third
criterion is how we want to improve upon drawbacks of the model
formulation for longer sequences. We will come to this point later in
this section. To illustrate how RNNs work, we will look at two examples.</p>

<h3 id="vanilla-recurrent-neural-networks">Vanilla Recurrent Neural Networks</h3>

<p>Vanilla Recurrent Neural Networks use a recurrence defined by</p>

<p>\(\)\begin{align}
h_t &amp;= \phi \big ( W\begin{bmatrix}
       x_{t} <br />
       h_{t-1} 
     \end{bmatrix}
     \big ).
\label{eq:vanilla-rnn}
\end{align}\(\)</p>

<p>Here, we concatenate the current input and the previous hidden states,
transform both linearly and pass it to a non-linear activation function.
This vector notation is equivalent to
\(h_t = \phi (W_{xh}x_t + W_{hh}h_{t-1})\). The two matrices \(W_{xh}\) and
\(W_{hh}\) are concatenated horizontally to \(W\). If the input vectors
\(x_t\) have dimension \(1 \times D\) and the hidden vectors dimension
\(1 \times H\), then \(W_{xh} \in \mathbb{R}^{H \times D}\),
\(W_{hh} \in \mathbb{R}^{H \times H}\), and weight matrix \(W\) is a matrix
with dimensions \([H \times (D+H)]\). Vanilla RNN models the current
hidden states \(h_t\) at each time step as a linear function of the
elements in the previous hidden states \(h_{t-1}\) and the current input
\(x_t\), transformed by a non-linearity. In a classification task, e.g.
where we want to predict the next written letter in a prompt by the
previously types letters, we would apply the Softmax function to a
linear transformation of the hidden state at each time step,
\(o_t = W_{ho} h_t\), in order to predict the next character’s one-hot
encoding. This is illustrated by Figure
<a href="#fig:vanilla-rnn">5</a>.</p>

<figure id="fig:vanilla-rnn">
<center><img src="/assets/img/dl-series/2f-vanilla-rnn.png" style="width:95%" /></center>
</figure>
<p><b>Figure 5. Vanilla RNN as character-level language model.</b> The left side
shows the <em>unrolled RNN</em>. The vocabulary has four characters and
the training sequence is “hello”. Each letter is represented by a 1-hot
encoding (yellow) and the RNN predicts the encodings of the next letter
(green) at each time step. The RNN has a hidden state with three
dimensions (red). The output has four dimensions. The dimensions are the
logits for the next character. They are the softmax of a linear
transformation of the hidden states. During supervised learning, the
model will be trained to increase (decrease) the logits of the correct
(false) characters. The right side shows the <em>rolled-up RNN</em>. The
graph has a cycle that shows that the same hidden states are shared
across time and that the architecture is the same for each
step.</p>

<h3 id="encoder-decoder-rnns">Encoder-Decoder RNNs</h3>

<p>Encoder-Decoder RNNs use the complete input history
\(\{x_i\}_1^{T_x}\) to predict the first output \(y_1\). Then, it
additionally uses the complete prediction history
\(\{\hat{y}_i\}_1^{t-1}\) to predict the next \(y_t\) for \(t=2,...,T_y\). The
model is able to generate sequences of arbitrary length that can be
unequal to the length of the input sequence. An examplary task is
translating sentences from English to German. Here, we work with one-hot
encodings of words from a fixed vocabulary instead of letters from an
alphabet. To build this RNN, we use the same recursion from the previous
example as an encoder RNN. However, do not classify the output at each
timestep directly. Instead, we use the last hidden state from the
encoder \(h_T\) as a context vector \(c_0\). Intuitively, the context vector
is an abstract representation of the entire input sentence. Then, we use
another RNN, the decoder RNN to process the information from the context
and the output from the previous period to generate the hidden states
\(s_t\) for the current period:</p>

<p>\(\)\begin{align}
s_t = \phi (W_{os}o_{t-1} + W_{ss}s_{t-1})
\end{align}\(\)</p>

<p>with \(s_0 = c_0\) and, for instance \(y_0 = \vec{0}\). We then predict the
next word’s one-hot encoding from the hidden states like in the last
example with the softmax of \(o_t=W_{so}s_t\). To model the fact that
input and output sequences can have different length, we require special
start of sentence, \(&lt;\)sos\(&gt;\), and end of sentence, \(&lt;\)eos\(&gt;\), tokens.</p>

<p>The encoder uses the \(&lt;\)eos\(&gt;\) as \(x_{1}\). The decoder takes \(&lt;\)sos\(&gt;\)
as \(y_1\) and stops the recursion when it returns \(&lt;\)eos\(&gt;\).</p>

<figure id="fig:encoder-decoder-rnn">
<center><img src="/assets/img/dl-series/2g-encoder-decoder-rnn.png" style="width:95%" /></center>
</figure>
<p><b>Figure 6. Encoder-Decoder RNN as word-level language model.</b> The input
language has two and the output language has three words. Every word,
the start and the end of a sentence are represented by a one-hot
encoding (yellow) and the RNN predicts the encodings of the translation
of the input sentence. The encoder RNN has a hidden state (green) that
is updated by one word step-by-step to produce the final context vector
(purple). The decoder RNN takes linear functions of the final encoding
and the start embedding to compute the hidden states for the next output
embedding (blue). The one-hot encoding of the output words (red) are a
softmax of a linar function of these hidden states. The decoder RNN
iterates the prediction until it returns the end token. With this
architecture, we can predict sequences of arbitrary length using the
embedded information of a whole input sequence.<br />
<br /></p>

<p>The simplicity of the RNN’s formulation has two drawbacks. First, the
connections between inputs and hidden states through linear layers and
element-wise non-linearities is not flexible enough for some tasks.
Second, the recurrent graph structure leads to problematic dynamics
during the backward pass.</p>

<h3 id="exploding-and-vanishing-gradient-problem">Exploding and Vanishing Gradient Problem</h3>

<p>We now explore the second
problem more formally. The vanilla RNN’s loss with respect to the weight
matrix in Equation
<a href="#eq:vanilla-rnn">1</a> is given by:</p>

<p>\(\)\begin{align}\frac{\partial L}{\partial W} = \sum_{t=1}^T \sum_{k=1}^{t+1} \frac{\partial L_{t+1}}{\partial o_{t+1}} \frac{\partial o_{t+1}}{\partial h_{t+1}} \frac{\partial h_{t+1}}{\partial h_{k}} \frac{\partial h_{k}}{\partial W}.
\label{eq:rnn-derivative}\end{align}\(\) The crucial part is the derivative of the
hidden layer in the next period with respect to some past layer \(k\). It
is given by the recursive product \(\)\begin{align}\label{eq:rnn-derivative-recursion}
\frac{\partial h_{t+1}}{\partial h_k} = \prod_{j=k}^t\frac{\partial h_{t+1}}{\partial h_{k}} = \frac{\partial h_{t+1}}{\partial h_{t}} \frac{\partial h_{t}}{\partial h_{t-1}} … \frac{\partial h_{k+1}}{\partial h_{k}} = \prod_{j=k}^t \text{diag} \big(W_{hh}\phi’ (W  [x_{j+1};h_j])\big).\end{align}\(\)
Equation
<a href="#eq:rnn-derivative-recursion">4</a> shows that, in order to compute
the gradients for \(W_{xh}\) and \(W_{hh}\), we have to multiply a large
number of Jacobians depending on the size of the input sequences. The
reason is that the derivative of a hidden layer \(h_{t+1}\) with respect
to some previous layer \(h_k\) equals the product of the derivatives from
\(t+1\) to \(k+1\) with respect to their previous layers. For simplicity,
let us assume that these derivatives are constant. Further, let us
compute the eigendecomposition of the fixed Jacobian matrix
\(\frac{\partial h_{t+1}}{\partial h_t}\) to analyze the problem more
formally. We obtain the eigenvalues
\(\lambda_1, \lambda_2,...,\lambda_n\), with
\(|\lambda_1|&gt; |\lambda_2|&gt;...&gt;|\lambda_n|\), and the respective
eigenvectors, \(v_1, v_2, ..., v_n\). With these components, we can write
the constant update of the hidden state in the direction of an
eigenvector \(v_i\) as \(\lambda_i \Delta\). During the backward pass from
\(t+1\) to \(k\), this change becomes \(\lambda_i^{t-k} \Delta h\). As a
result, if the largest eigenvalue \(\lambda_1\) is smaller than one, the
gradient will vanish, and if it is larger than one, the gradient
explodes. In practice, the gradients oftentimes rather vanishes due the
contribution of the activation derivative which cannot exceed one. In
this case, the earlier hidden states are barely updated. The effect is
that the front parts of the input do not impact the prediction. In other
words, our model has a weak long-term memory.</p>

<p>There are many approaches to alleviate this problem. Examples are
regularization, careful weight initializations, using ReLU activations
only, and gated recurrent networks like Long Sort-Term Memory (LSTM) or
Gated Recurrent Unit (GRU). We can view these gating mechanisms as much
more sophisticated extensions of the skip connections from Chapter
<a href="https://www.tobiasstenzel.com/blog/2023/dl-cnn/#skip-connections">5</a>. Today, the
most common approach is the transformer architecture. It is not only
less vulnerable to vanishing gradients but also provides a more
expressive coupling of current inputs and previous states compared to
vanilla RNNs.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = &quot;Deep Learning Series&quot;,
  author  = &quot;Stenzel, Tobias&quot;,
  year    = &quot;2023&quot;,
  url     = &quot;https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>]]></content><author><name>Tobias Stenzel</name></author><category term="dl-fundamentals" /><category term="sneural-net-archetype" /><category term="backprop" /><category term="exploding-vanishing-gradients" /><summary type="html"><![CDATA[🧱 The encoder-decoder RNN is quite useful for understanding the transformer.]]></summary></entry><entry><title type="html">6. Convolutional Neural Networks</title><link href="https://www.tobiasstenzel.com/blog/2023/dl-cnn/" rel="alternate" type="text/html" title="6. Convolutional Neural Networks" /><published>2023-03-06T00:00:00+00:00</published><updated>2023-03-06T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/dl-cnn</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/dl-cnn/"><![CDATA[<h2 id="sec:cnn">Convolutional Neural Networks</h2>

<p>A Convolutional Neural Networks (CNN) <d-cite key="lecun_gradient-based_1998"></d-cite> is
type of neural network architecture that is specifically designed for
input data where the tabular arrangement includes a spatial meaning.
Moreover, the features of one example are not stored in a vector of
different property values but \(x\) is a multi-dimensional array (i.e. a
<em>tensor</em>). For instance, the data for a color image could have
dimensions \(32 \times 32 \times 3\). The first two dimensions represent
the height and the width of the pixels and the third dimension stands
for three color channels red, green and blue. We can find these spatial
relationships in many data, not only in images. Further examples are
sound spectrogram or sentence data. The main motivation for CNNs is to
reduce the complexity with parameter sharing by focusing on relations
between spatially close data points.</p>

<h3 id="cross-correlation">Cross-correlation</h3>
<p>The core building block of a CNN is the
<em>Convolutional Layer</em> (or CONV layer). The main operation in this layer
is the cross-product between tensors. We denote the cross-product
between two tensors \(I\) and \(W\) as \(I \star W = O\). \(I\) is the input and
\(W\) is the kernel or the filter. If the two matrices are three
dimensional tensors, then the third dimension of both tensors must have
equal length. Let \(i_1, i_2\) and \(f_1, f_2\) be the width and the height
of the input and the filter, respectively. The output, activation map O,
has dimension \((i_1 - f_1 + 1)  \times (i_2 - f_2 + 1)\). Accordingly,
the cross-product for one output element is given by the following
formula :</p>

\[\begin{align}O_{i,j} = (I \star W)_{i,j} = \sum_{m=1}^{k_1} \sum_{n=1}^{k_2}
W_{m,n} \cdot I_{i+m-1,j+n-1} + b.
\label{eq:cross-correlation}\end{align}\]

<p>Figure <a href="#fig:cross-correlation">3</a> illustrates Equation
<a href="#eq:cross-correlation">1</a> without adding the filter-specific
bias \(b\). We fill the activation map row-wise starting from the first
element in the top-left corner. To compute this element, we place the
filter on top of the input so that the feature and the top-left element
of the filter overlap. Then, we calculate the dot product between the
elements at the same position, i.e. we obtain \(O_{1,1}\) by
\(5 \cdot 1+6 \cdot 2+3 \cdot 8+4 \cdot 9=94\). The cross-correlation
operation is equivalent to the convolution operation with horizontally
and vertically flipped filter.</p>

<figure id="fig:cross-correlation">
<center><img src="/assets/img/dl-series/cross-correlation.png" style="width:80%" /></center>
</figure>
<p><b>Figure 3. Cross-correlation between two matrices.</b> Input matrix <span class="math inline"><em>I</em></span> has shape 4 <span class="math inline">×</span> 4 and kernel matrix W has 3 <span class="math inline">×</span> 3. The colored areas in I show the
receptive field for each output in <span class="math inline"><em>O</em></span>. The matrix elements are numbered
by their matrix indices.</p>

<h3 id="concrete-example">Concrete example</h3>
<p>Let us continue with another example in three
dimensions. Our input I is a \(32 \times 32 \times 3\) tensor that
represents an image with red, green and blue channels. Our filter W is a
\(5 \times 5 \times 3\) tensor. We have one filter channel for each color
channel. Now we <em>convolve</em> this filter by sliding it across the whole
image. With that, we interact the color channels dimensions because we
compute the dot product over three dimensions. The result is an
<em>activation map</em> with dimensions \(28 \times 28\) because we can only
place a \(5 \times 5\) filter only 28 times over a \(32 \times 32\) tensor.
It is common to pad the input with a frame of zeros to control the first
two dimension lengths of the output. For instance, a frame of zeros with
thickness 2 maintains the first two dimension lengths of the input.
Another option is to slide the filter with some stride to reduce the
impact of the relations between close pixels on the filter weights. For
instance, convolving the \(5 \times 5 \times 3\) filter over the
\(32 \times 32 \times 3\) image with no padding and stride 2 results in an
activation map of size \(16 \times 16 \times 3\) instead. Lastly, the CONV
layer does not only use a single filter but a set of filters. E.g., with
a set of seven filter, we obtain the same number of \(16 \times 16\)
activation maps. We stack these activation maps along the third
dimension of the resulting output tensor. Thus, we have transformed a
\(32 \times 32 \times 3\) into a \(16 \times 16 \times 7\) stack of
activation maps. Intuitively, each single filter has the capacity to
detect specific local features in the input tensor that may be of
importance to later layers. The weights in this filter tensor are
parameters that we train with backpropagation.</p>

<h3 id="general-definition">General definition</h3>
<p>A convolutional layer for images that are
represented by a three dimensional input tensor is given by the
following five components:</p>

<ul>
  <li>
    <p><strong>Input:</strong> a tensor \(I\) of size \(W_1 \times H_1 \times D_1\)</p>
  </li>
  <li>
    <p><strong>Hyperparameters:</strong> the number of filters \(K\), the filter’s width
or height \(F\) (assuming both are equal), the stride \(S\) , and the
amount of zero padding, \(P\).</p>
  </li>
  <li>
    <p><strong>Output:</strong> \(D_2\) different activation maps stored in a volume of
size \(W_2 \times H_2 \times D_2\), where \(W_2= (W_1 - F+2P)/S+1\),
\(H_2=(H_1-F+2P)S+1\), and \(D_2=K\).</p>
  </li>
  <li>
    <p><strong>Complexity:</strong> the number of parameters in each filter is
\(F \times F \times D_1\). This gives a total of
\(K \times (F \times F \times D_1) + K\) parameters for the whole
layer. Note that the filter depth always equals the input depth.
Moreover, the last \(K\) represents the bias terms that we add to the
respective filter after each dot product computation with the data.</p>
  </li>
  <li>
    <p><strong>Operation:</strong> Each d-th slice of the output tensor (of size
\(W_2 \times H2\)) is the result of computing the cross-correlation
between the d-th filter over the input tensor with a stride of S and
offsetting the result by d-th bias afterwards.</p>
  </li>
</ul>

<h3 id="parameter-sharing">Parameter sharing</h3>
<p>Cross-correlation slides each filter over the
input with the same weights at every position. As a consequence, the
size of the receptive field, i.e. the set of inputs that impact one
output, is much smaller compared to fully-connected layers (cf. Figure
<a href="https://www.tobiasstenzel.com/blog/2023/dl-fnn/#fig:vanilla_neural_net">2</a>. In particular, a convolutional
layer is a special case of a fully connected layer, where many neurons
have the same (re-arranged) set of weights and where most weights are
set to zero except of a small neighborhood. Hence, the convolutional
layer has much less parameters and is less prone to overfitting. To
illustrate this point, let us consider the example of a
\(128 \times 128 \times 3\) input image that is taken by a convolutional
layer with 32 \(5 \times 5 \times 3\) filters, padding of 2 and a stride
of 1. The output is a \(128 \times 128 \times 32\) volume consisting of
\(524,288\) elements. We compute this volume with only
\(32*5*5*3+32=2,432\) total parameters. In contrast, if this was a fully
connect layer that computes every output element based on its own
specific weights, we would use
\(524,288 * (128 \times 128 \times 3) = 25,769,803,776\) parameters. This
number is not only gigantic but it would also be difficult not to
overfit the data even if we could store the parameters and compute the
result.</p>

<h3 id="pooling-layers">Pooling layers</h3>
<p>Another building block of CNNs are pooling layers.
These layers are used to further reduce overfitting by downsampling the
convolutions output with a fixed scheme and without any parameters.
Specifically, these pooling operations are applied to each activation
map separately and preserve the depth of the output volumes but not
their height and width. As with cross-correlation, we slide the pooling
filter over its input. However, we have to do this for each input
channel separately because the pooling operation has no depth. A common
setting is a \(2 \times 2\) filter with stride 2 where the filter
represents a max operation over four numbers. This filter gives us an
output tensor that is downsampled by \(2 \times 2\) along the first two
dimensions.</p>

<h3 id="backward-pass">Backward pass</h3>
<p>The Jacobian of a convolution layer \(O = I \star W\)
is given by \(I \star J^O\), where \(J^O\) is the Jacobian that contains the
upstream gradients \(\delta^o\) with respect to the activation map
parameters. This is illustrated by Figure <a href="#fig:gradient-cross-correlation">4</a>. In comparison to the
Jacobian for a fully-connected linear layer, the smaller, shared
downstream gradient is only multiplied with the activations of its
adjacent elements from the previous layer. The first derivatives of the
pooling operations average and max are simple. The derivative of the
average with respect to one element is 1 divided by the number of
elements. The derivative of the max is the indicator function of maximum
element’s index.</p>

<figure id="fig:gradient-cross-correlation">
<center><img src="/assets/img/dl-series/gradient-cross-correlation.png" style="width:80%" /></center>
</figure>
<p><b>Figure 4. Backward pass through cross-correlation.</b> The Jacobian <span class="math inline"><em>J</em><sup><em>W</em></sup></span> for the
weight parameters of the cross-correlation <span class="math inline"><em>O</em> = <em>I</em> ⋆ <em>W</em></span> is given
by the cross correlation between the Jacobian <span class="math inline"><em>J</em><sup><em>O</em></sup></span> that contains
the downstream gradients <span class="math inline"><em>δ</em><sup><em>o</em></sup></span> with respect
to the activation map parameters. The shaded area in <span class="math inline"><em>I</em></span> shows the elements that are used
in the dot product with <span class="math inline"><em>J</em><sup><em>O</em></sup></span> to compute
the shaded element in <span class="math inline"><em>J</em><sup><em>W</em></sup></span>.</p>

<h3 id="cnn-architectures">CNN architectures</h3>
<p>We build complete CNNs by stacking convolutional
and pooling layers. A classical architecture is LeNet-5 for digit
classification from black &amp; white images of hand-written digits <d-cite key="lecun_gradient-based_1998"></d-cite>. A slightly simplified version has the form
INPUT, [[CONV, POOL] \(\times\) 2], CONV, FC, FC, SOFTMAX. In this
notation, INPUT stands for a tensor of a batch of images
(\([100 \times 32 \times 32 \times 1]\) for a batch of 100 32 \(\times\) 32
black &amp; white images), CONV denotes six, sixteen, and 120 \(5 \times 5\)
filters with stride 1 and tanh activation, POOL denotes an average
pooling layer with a 2 \(\times\) 2 filter and a stride of 2, and FC
represents fully-connected layers. The first layer has tanh activations
and the last layer calculates the softmax probabilities for ten
different digits. The FC layers are used to extract features not only
locally but globally and because this type of layer is cheaper after
multiple rounds of downsampling. A receptive field of a (hidden) feature
is the set if inputs that influence this feature. In a fully connected
layer, the receptive field of every hidden feature is always the
complete input vector or tensor. By stacking multiple convolutional
layers, we can achieve the same receptive field with much less
parameters. The outputs from higher layers have larger receptive fields
and thus represent higher-level features. One example for these type of
features could be far-reaching edges.</p>

<h3 id="lengthy-network-paths">Lengthy network paths</h3>
<p>In the last paragraph, we learned that
classic CNN architectures are essentially a stack of functions. In
Chapter <a href="https://www.tobiasstenzel.com/blog/2023/dl-backprop/#backpropagation">4</a>, however, we saw that a sequence of function
applications results in a long and linear backpropagation graph given by
a multiplication sequence of partial derivatives. If a number of these
partial derivatives are either very small or very large, their
multiplicative effect can cause either too small or too large gradient
updates during optimization. Especially layers with sigmoid activations
(e.g. logistic, tanh) with derivatives that are flat or extremely steep
for large parts of the domain are problematic. If parameters have once
reached these parts, learning oftentimes stops for larger chunks of the
network for two reasons. First, for these parameters, it requires a
number of unusually large steps to leave these extreme areas. And
second, other parameters with gradients that include multiplications
with the extreme gradients are set to zero or infinity, too.</p>

<h3 id="skip-connections">Skip connections</h3>
<p>We can alleviate the problem by connecting earlier
(or bottom) layers, \(h^{(i)}\), with later (or top) layers, \(h^{(i+k)}\),
via the duplication operation followed by the "+" operator. With that,
we open up a new path past the majority of the stacked functions. We
call this type of link a <em>skip connection</em>. The effect is that, in the
backward pass, \(l^{(i)}\) receives another downstream Jacobian
\(J^{(i+k)}\cdot ... \cdot J^{(o)}\) that we add to the more complex
Jacobian
\(J^{(i+1)} \cdot J^{(i+2)} \cdot ... \cdot J^{(i+k)} \cdot... \cdot J^{(o)}\).
Intuitively, the updates from the more complex Jacobian are used to
learn the difference between the bottom layer and the top layer. As a
result, we can learn a simple representation of the model without
exposing the gradient to further multiplicative transformations and, in
addition, we can learn another representation for more complex relations
between the input features. An illustrative toy model similar to Figure
<a href="https://www.tobiasstenzel.com/blog/2023/dl-backprop/#toy-example">1</a> is \(C(\theta)= \tanh (\theta)^n\) where
\(n \in \mathbb{N}^+\) represents the number of subsequent tanh
operations. The model with skip connection is
\(C_{res}(\theta)= C(\theta) + \theta\). We can observe the described
technical aspects by comparing \(\partial C / \partial \theta\) with
\(\partial C_{res} / \partial \theta\) and the respective computational
graphs. An early implementation of this idea is Microsoft’s ResNet <d-cite key="he_deep_2016"></d-cite>. This architecture uses skip connections that only skip
one layer at a time. We will return to the problem of vanishing and
exploding gradients from lenthy network parths in our discussion of
recurrent neural networks.</p>

<h3 id="inductive-bias">Inductive bias</h3>
<p>In the previous paragraph, we have seen how we can
design neural network architectures to form sensible predictions on a
domain-specific type of input data. We have also learned how to exploit
the peculiar spatial relations in this data in order to save parameters
and training time compared to fully-connected FNNs. The assumptions that
we pose on the relations in the data in our architecture design is
called the <em>inductive bias</em>. In summary, there are three inductive
biases in the convolution layers of CNNs:</p>

<ul>
  <li>
    <p><strong>Translation invariance:</strong> the convolution operation is translation
invariant, i.e. \(f(x) = f(T(x))\) with \(x, a \in \mathbb{R}^n\) and
\(T:\mathbb{R}:x \rightarrow \mathbb{R}^n\), where \(f\) denotes the
convolution and \(T\) a transformation of the input. For images,
\(n=2\). The motivation is that we want to identify an object
independent of changes to its position. For other transformations,
such as rotations and change in color, however, we need to train on
additional augmented images.</p>
  </li>
  <li>
    <p><strong>Locality of features:</strong> the filter sizes are much smaller than the
image because we assume that local relations between the pixels are
more important than global relations.</p>
  </li>
  <li>
    <p><strong>Universality of feature extractors:</strong> we can reuse the same filter
for all regions of the input because we assume that the hidden
features which we extract are similarly important at each position.</p>
  </li>
</ul>

<p>In order to improve our results, we soften the inductive bias regarding
the locality of features with two additional layers: at the beginning of
the network, we include cheap pooling layers and towards the end we add
fully connected layers.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = &quot;Deep Learning Series&quot;,
  author  = &quot;Stenzel, Tobias&quot;,
  year    = &quot;2023&quot;,
  url     = &quot;https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>]]></content><author><name>Tobias Stenzel</name></author><category term="dl-fundamentals" /><category term="neural-net-archetype" /><category term="backprop" /><category term="cross-correlation" /><category term="inductive-bias" /><category term="receptive-field" /><category term="compute-graph" /><summary type="html"><![CDATA[🔑️ Key sections about parameter sharing, inductive biases, skip connections, and cross-correlation.]]></summary></entry><entry><title type="html">5. Feedforward Neural Networks</title><link href="https://www.tobiasstenzel.com/blog/2023/dl-fnn/" rel="alternate" type="text/html" title="5. Feedforward Neural Networks" /><published>2023-03-05T00:00:00+00:00</published><updated>2023-03-05T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/dl-fnn</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/dl-fnn/"><![CDATA[<h2 id="feedforward-neural-networks">Feedforward Neural Networks</h2>

<p>In the last sections we learned that we can compute any differentiable
loss function between an arbitrary differentiable function \(f\) that
takes input \(x\) and outputs predictions \(\hat{y}\) and the data \((x,y)\),
and optimize the model \(f\) with respect to its parameters \(\theta\) with
stochastic gradient descent. In this section, we look at how to
construct \(f\) as a neural network.</p>

<h3 id="vanilla-neural-networks">Vanilla Neural Networks</h3>

<p>In the two examples from Chapter <a href="https://www.tobiasstenzel.com/blog/2023/dl-supervised-learning/#supervised-learning">2</a>, neural network regression and
neural network classification, we have already discovered one main idea
of vanilla neural networks: combining matrix multiplications and
element-wise non-linearities. The other idea is that we can repeat, or
layer, these two transformations multiple times. For instance,
abstracting from the concrete structure of the input and output data, we
would write a neural network with two fully connected layers as
\(f(x)=W_1 \phi (W_1 x)\), where \(\phi\) represents an an element-wise
non-linearity like tanh and \(W_1\), \(W_2\) are matrices that interact,
scale, and shift the inputs. A 3-layer neural networks would be
implemented as \(f(x)=W_3 \phi (W_1 \phi (W_1 x))\), and so forth. The
outputs from the intermediate functions are called hidden layers, and
one output a hidden unit. We can think of hidden units as feature
abstractions from the previous layer, or latent features for the next
layer. During training, the neural network learns which feature
abstractions are useful to the next layer. A network is called deep if
it has more than one hidden layer. Common choices for non-linearity
\(\phi\) are tanh, the rectified linear unit (ReLU) \(\max(0,x)\), and the
logistic function \(1/(1+e^{-x})\). Usually, we add an additional element
\(x_0 = 1\) to the input vector. The corresponding weight, or bias,
\(b:=w_0\) shifts the output. The choice of the last layer depends on the
type of output data \(y\). For instance, we could select the logistic
function for binary classification, the softmax function for multi-class
classification, and a linear layer to predict natural numbers. Similar
to our toy example in Figure <a href="https://www.tobiasstenzel.com/blog/2023/dl-backprop/#toy-example">1</a>
reference=”fig:toy_graph”}, we can depict a vanilla neural network as a
directed acyclical graph and compute its gradient via backpropagation in
a supervised learning setting. Figure
<a href="#fig:vanilla_neural_net">2</a> depicts a two-layer example
architecture for a binary classification task. Note the similarity
between this network and the second example from Chapter <a href="https://www.tobiasstenzel.com/blog/2023/dl-supervised-learning/#supervised-learning">2</a>. Figure
<a href="#fig:vanilla_neural_net">2</a> clarifies how we aggregate dot
product operations on the neuron level to matrix operations on the layer
level. In the last decade, the neural network approach has led to
state-of-the-art results in areas with large amounts of high-quality
data such as computer vision, natural language processing, speech
recognition and others. A theoretical reason for this success is that
neural networks can achieve universal approximation. This means that
they can approximate any continuous function, either via sufficient
depth (number of layers; e.g. <d-cite key="cybenko_approximation_1989"></d-cite> or width
(number of columns in weight matrices; e.g. <d-cite key="hanin_approximating_2017"></d-cite>. A practical reason is that we can optimize
these functions very efficiently with many parallel computations on
modern hardware.</p>

<figure id="fig:vanilla_neural_net">
<center><img src="/assets/img/dl-series/vanilla-neural-net.png" style="width:100%" /></center>
</figure>
<p><b>Figure 2. Vanilla neural network with two fully-connected hidden
layers for binary classification.</b> The first hidden layer, <span class="math inline"><em>h</em><sup>(1)</sup></span>, is composed of
three neurons. Each neuron takes the input vector <span class="math inline"><em>x</em></span> and compute the dot product with
its weight vector from its respective column of the layer’s weight
matrix <span class="math inline"><em>W</em><sup>(1)</sup></span>.
Moreover, <span class="math inline"><em>h</em><sup>(1)</sup></span>
introduces non-linearity via an elementwise non-linear operation <span class="math inline"><em>ϕ</em></span>. The second layer, <span class="math inline"><em>h</em><sup>(2)</sup></span>, repeats the same
process with the previous hidden layer as its input but reduces the
number of hidden features. The output layer o has the same size as the
last hidden layer and computes the Softmax probabilities for classes one
and two.</p>

<h3 id="backward-pass">Backward pass</h3>

<p>To improve our understanding about backpropagation of
neural networks, we look at the partial derivatives of typical layers.
Note that we usually consider the derivatives with respect to the loss
instead of the cost function because the regularization terms are not
complex and simply add up to the more complicated loss derivative at the
end of each update computation.</p>

<ul>
  <li>
    <p><strong>ReLU:</strong> the derivative of the element-wise ReLU is given by
\(\frac{\partial ReLU}{\partial z} = 0\) if \(x \leq\) 0 and 1 if x
\(&gt;\) 0. As a consequence, we propagate only the
gradient for the neurons with positive activations back to the
previous layer.</p>
  </li>
  <li>
    <p><strong>Linear layer:</strong> Let \(z = W x\) be a linear layer with one input
channel and \(n\) elements, \(x \in \mathbb{R}^{n \times 1}\), with
\(W \in \mathbb{R}^{m \times n}\) and let \(z, \delta 
 := \frac{\partial L}{\partial z} \in  \mathbb{R}^{m \times 1}\).
Then
\(\frac{\partial L}{\partial W} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial W} = \delta x^T\).
The same result, \(\delta X^T\), holds for linear layers with \(k\)
features or input channels, i.e. with
\(X \in \mathbb{R}^{n \times k}\) and
\(Z, \delta \in \mathbb{R}^{m \times k}\). Hence, <em>each</em> input
receives the input-specic respective weighted sum of the upstream
gradient of <em>all</em> neurons from the next layer.</p>
  </li>
  <li>
    <p><strong>Softmax:</strong> Let \(\hat{p}=\text{softmax}(z)\) denote the softmax
probabiltities with \(\hat{p}, z \in \mathbb{R}^{n \times m}\) and let
\(L(\hat{p},y)\) with \(y \in \mathbb{R}^{n \times 1}\) be the scalar
cross-entropy loss. We get
\(\frac{\partial L}{\partial z} = \frac{1}{n}(\hat{p} - y \otimes \vec{1}^m
)\), where \(\otimes\) denotes the outer product between two vectors.
If we add another logit layer \(l\in \mathbb{R}^{n \times m}\) below
the softmax, we obtain the following simple result:</p>
  </li>
</ul>

\[\frac{\partial L}{\partial l_{i,j}} =
\begin{cases}
   \hat{p}_{i,j} &amp; \text{if i $\neq$ j} \\
   \hat{p}_{i,j} - 1 &amp; \text{if i = j}.
\end{cases}\]

<ul>
  <li>Intuitively, for each example \(i \in 1,...,n\), the gradients for
each parameter that flow from each predicted probability is
increased by the amount that the predicted probability differs from
the actual label (times \(1/n\)). Therefore, gradient descent in
particular updates parameters \(\theta\) towards the direction of the
gradients from the bad predictions.</li>
</ul>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = &quot;Deep Learning Series&quot;,
  author  = &quot;Stenzel, Tobias&quot;,
  year    = &quot;2023&quot;,
  url     = &quot;https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>]]></content><author><name>Tobias Stenzel</name></author><category term="dl-fundamentals" /><category term="neural-net-archetype" /><category term="backprop" /><summary type="html"><![CDATA[Feedforward Neural Networks]]></summary></entry><entry><title type="html">4. Backpropagation</title><link href="https://www.tobiasstenzel.com/blog/2023/dl-backprop/" rel="alternate" type="text/html" title="4. Backpropagation" /><published>2023-03-04T00:00:00+00:00</published><updated>2023-03-04T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/dl-backprop</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/dl-backprop/"><![CDATA[<h2 id="backpropagation">Backpropagation</h2>

<p>We learned that we can find a mapping \(f \in \mathcal{F}\) that maps
features X to outcome Y consistent with the data by minimizing the cost
function with repeated gradient evaluations using a gradient descent
optimizer.</p>

<p>We compute the gradient with backpropagation. This algorithm allows us
to efficiently compute gradients of functions that 1.) are
scalar-valued, 2.) have many input parameters, and 3.) can be decomposed
into simpler intermediate functions that are differentiable. The
mathematical formula of backpropagation is inspired by the third
property. It is the gradient computation via recursive applications of
the chain rule from calculus. The algorithmic order is inspired by the
first two properties. The idea is to efficiently compute the resulting
derivative starting from the intermediate functions on the parameter
instead of the cost function side.</p>

<h3 id="toy-example">Toy example</h3>

<p>Recall that we would like to compute the gradient of
cost function \(C(\theta)\), which takes not only parameters \(\theta\) but
also multiple examples \((x_i, y_i)\) as input. Specifically, our
first-order optimizer requires the gradient of the cost function with
respect to the model parameters \(\nabla_{\theta}C(\theta)\) in order to
update the parameters \(\theta\). Let us consider the following example.
Let \(C(\theta_1, \theta_2)=\theta_1 \theta_2 + \tanh (\theta_1)\) be the
cost function of a neural network with parameters \(\theta_1\) and
\(\theta_2\). Figure <a href="#fig:toy_graph">1</a> shows that we can view this equation as a
computational graph with cost function \(C\) as root and parameters as
leaf nodes. We introduce intermediate variables to write \(C\) as a
sequence of the intermediate functions \(z_3=\tanh (\theta_1)\),
\(z_4=\theta_1 \theta_2\), \(z_5=z_3+z_4\), and \(C(\theta)=z_5\). For ease of
notation, we will later also write \(\theta_1,\theta_2\) and \(C(\theta)\)
as \(z_1, z_2\) and \(z_6\). We can write the gradient of the cost function
with respect to each parameter as a combination of the gradients of its
parent nodes using the chain rule from calculus and the fact that
multiple occurrences of a term add up in its derivative. This is shown
in Equation <a href="#eq:dtheta_1">1</a> and
<a href="#eq:dtheta_2">2</a>:</p>

\[\begin{align}
\frac{\partial C}{\partial \theta_1}=\frac{\partial C}{\partial z_5} \bigg( \frac{\partial z_5}{\partial z_3} \frac{\partial z_3}{\partial \theta_1} + \frac{\partial z_5}{\partial z_4} \frac{\partial z_4}{\partial \theta_1} \bigg) = 1 - \tanh ( \theta_1)^2 + \theta_2,
\label{eq:dtheta_1}\end{align}\]

\[\begin{align}
\label{eq:dtheta_2}
\quad \frac{\partial C}{\partial \theta_2}=\frac{\partial C}{\partial z_5}\frac{\partial z_5}{\partial z_4}\frac{\partial z_4}{\partial \theta_2}   =\theta_1.\end{align}\]

<figure id="fig:toy_graph">
<center><img src="/assets/img/dl-series/compute-graph.png" style="width:75%" /></center>
</figure>
<p><b>Figure 1. Computational graph for a toy example of a neural network’s
forward pass without vector-valued intermediate functions, data,
regularization and loss.</b> The function given by the graph is <span class="math inline"><em>C</em>(<em>θ</em><sub>1</sub>,<em>θ</em><sub>2</sub>) = <em>θ</em><sub>1</sub><em>θ</em><sub>2</sub> + tanh (<em>θ</em><sub>1</sub>)</span>.
Intermediate functions that are relevant for deriving the gradient with
the chain rule are denoted by <span class="math inline"><em>z</em><sub><em>i</em></sub></span> with <span class="math inline"><em>i</em> ∈ {1, …., 6}</span>. The edges along
which the intermediate evaluations move forth during the forward pass
and along which the gradients move back during the backward pass are
denoted by <span class="math inline"><em>e</em><sub><em>i</em>, <em>j</em></sub></span>
with <span class="math inline"><em>i</em>, <em>j</em> ∈ {1, …, 6}</span>. We can
view two edges leaving one node as a a shortcut depiction for applying
the <span class="math inline"><code>duplicate</code></span> or fork
operation along both edges.</p>

<h3 id="vector-valued-intermediate-functions">Vector-valued intermediate functions</h3>
<p>The toy example deviates from
a realistic neural network application in a few aspects. One of these
aspects is that we usually consider vectors of large parameter groups.
Another aspect is that the intermediate functions that transform the
parameter vector \(\theta\) step-by-step are usually vector-valued (unlike
the final cost function). Apart of confluence operators like "+" and
"*" that usually combine different parameter groups, the final
gradients can be written as a sequence of "local" gradient
computations. Due to vector-valued functions and the number of
parameters, these computations are written as matrix multiplications. We
look at this extension more formally. To this end, let \(z_0\) be the
input vector which we transform through a series of functions be
\(z_i = f_i(z_{i-1})\) where \(i=1,...,k\) and only the last \(z_i\) can be
scalar. Assuming that the functions \(f_i\) are once differentiable, we
can compute the Jacobian matrix \(\frac{\partial z_i}{\partial z_{i-1}}\)
for all intermediate functions. This will give us the values of the
first derivative of every output dimension of \(z_{i}\) depending on each
single input dimension of \(z_{i-1}\). From the multivariable chain rule,
we obtain the result that the gradient of our final function with
respect to input vector equals the product of all intermediate
Jacobians:
\(\frac{\partial z_k}{\partial z_{0}} = \prod_{i=1}^k \frac{\partial z_i}{\partial z_{i-1}}\).</p>

<h3 id="reverse-accumulation">Reverse accumulation</h3>
<p>We can compute
\(\frac{\partial C}{\partial \theta}\) in two different ways. In this
section we learn that one approach is much more efficient. The reason is
the the structure of our problem: we minimize a scalar-valued function
with a large number of parameters. One approach is to compute the
gradient from parameters to output:
\(\frac{\partial C}{\partial \theta} = \frac{\partial z_k}{\partial z_{k-1}} \cdot ... \cdot \frac{\partial z_2}{\partial z_1}\).
This is called forward accumulation. The other approach, reverse
accumulation, is to compute the gradients from output to parameters:
\(\frac{\partial C}{\partial \theta} = \frac{\partial z_2}{\partial z_1}\cdot   ... \cdot \frac{\partial z_k}{\partial z_{k-1}}\).
The first approach is less efficient but more intuitive. It is more
intuitive because the derivatives can be computed in sync with the
evaluation steps. This makes it easier to think about how confluence
operations like "\(+\)", "\(*\)", the fork operation <code class="language-plaintext highlighter-rouge">duplicate</code>, or
filter operations like <code class="language-plaintext highlighter-rouge">max</code> or <code class="language-plaintext highlighter-rouge">average</code> transform the gradients. We
will later see that it is crucial to use these operations in a careful
way in the architecture design of neural networks because they can have
large effects on the model performance. In reverse accumulation, we fix
the dependent variable to be differentiated and compute the derivative
with respect to each intermediate function recursively. Table
<a href="#tab:reverse">1</a> shows
how we can calculate the two gradients of our toy example step-by-step
with this method. There are two important things to note. First,
computing the backward operations corresponding to the forward
operations is prone to error. This is one reason why this should be done
automatically by a graph-based computer program. Second, we can re-use
our intermediate computations \(\bar{z}_3\) to \(\bar{z}_5\) for both
gradients based only on one evaluation of the cost function
\(C(\theta) = z_6\). In realistic neural network applications, we are able
to re-use a large number of intermediate results based on only one
forward evaluation for an even larger number of input parameters. In
contrast, with forward-mode accumulation, computing the gradient
requires to evaluate each intermediate function with the whole parameter
vector. Although re-using the intermediate gradients requires more
storage, the reverse-mode accumulation strategy is much more efficient
for functions like neural networks where the number of output values is
much smaller than the number of input values.</p>

<figure id="tab:reverse">
<center><img src="/assets/img/dl-series/table-backprop.png" style="width:80%" /></center>
</figure>

<p><b>Table 1. Gradient computations in reverse accumulation mode for toy
example <span class="math inline"><em>C</em>(<em>θ</em><sub>1</sub>,<em>θ</em><sub>2</sub>) = <em>θ</em><sub>1</sub><em>θ</em><sub>2</sub> + tanh <em>θ</em><sub>1</sub></span>.</b>
The left column shows the evaluation and the right column depicts the
derivation steps. <span class="math inline"><em>z̄</em></span> denotes
the derivative of the cost function with respect to an intermediate
expression, i.e. <span class="math inline">\(\frac{\partial C}{\partial
z}\)</span>. In the backward pass, we first evaluate the scalar-cost
function and use its derivative with respect to <span class="math inline"><em>z</em><sub>5</sub></span> at the top of the
graph. Here we assume <span class="math inline"><em>z̄</em><sub>5</sub></span> equals <span class="math inline">1</span>. In the next step, we combine the chained
gradients of the intermediate expressions according to the respective
backward functions of the <code>duplicate</code> and the confluence
operations “<span class="math inline">+</span>” and “<span class="math inline">*</span>” from top to bottom. Addition distributes
the upstream gradient down to all its inputs. Multiplication passes the
upstream gradient multiplied with the <em>other</em> input back. I.e.,
<span class="math inline">\(\frac{\partial (z_3 + z_4)}{\partial z_{4}} =
1, \frac{\partial (z_1 * z_2)}{\partial z_{2}} = z_1\)</span>, and the
backward function of <span class="math inline">(<em>z</em><sub>1, 3</sub>,<em>z</em><sub>1, 4</sub>) = <code>duplicate</code>(<em>z</em><sub>1</sub>)</span>
equals <span class="math inline">\(\frac{\partial z_3}{\partial z_{1}} +
\frac{\partial z_4}{\partial z_{1}}\)</span>, where <span class="math inline"><em>z</em><sub>1, 3</sub></span> and <span class="math inline"><em>z</em><sub>1, 4</sub></span> denote abbreviated
nodes from the implicit duplication operation in the third and fourth
evaluation step.</p>

<h3 id="implementation">Implementation</h3>
<p>In our toy example, we have already discovered that
it is easier to think of the evaluation and differentiation of a neural
<em>network</em> in terms of a directed graph of operations instead of a linear
sequence of function applications. In this graph, the nodes stand for
differentiable operations that take a number of vectors from its
incoming edges, transforms and potentially interrelates them, and sends
the result to the next nodes along its outgoing edges. The graph
abstraction translates to most implementations of the backpropagation
algorithm. A <em>Graph</em> object keeps the connections (e.g. <code class="language-plaintext highlighter-rouge">duplicate</code> or
\(+\) ) between the nodes and the collection of operations (e.g. \(\tanh\)).
Both Node and Graph objects implement a <code class="language-plaintext highlighter-rouge">forward()</code> and a <code class="language-plaintext highlighter-rouge">backward()</code>
method. The Graph’s <code class="language-plaintext highlighter-rouge">forward()</code> calls the Nodes’ <code class="language-plaintext highlighter-rouge">forward()</code> methods in
their topological order. With that, every Node computes its operation on
its input, and the Graph sends it to the next Node. The Graph’s
<code class="language-plaintext highlighter-rouge">backward()</code> iterates over the nodes in reverse topological order and
calls their <code class="language-plaintext highlighter-rouge">backward()</code> methods. In the backward pass, each Node is
given the gradient of the cost function with respect to its output and
it returns the "chained" gradients with respect to all its inputs.
"Chained" means that the Node multiplies the Jacobian that it received
from its parent nodes with its own local Jacobian. The Graph sends the
resulting product to the Nodes’ children and the process repeats until
the recursion ends at the last computation which includes the data and
the current parameter values. At last, the optimizer updates the
parameter vector based on the final gradient (Step 3 in Algorithm <a href="https://www.tobiasstenzel.com/blog/2023/dl-optimization/#first-order-methods">1</a>. Note
that the implemented compute graph for Figure
<a href="#fig:toy_graph">1</a> would in practice be larger and rudimentary because we can decompose
many operations, like divide or subtract in tanh, into more basic
operations.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = &quot;Deep Learning Series&quot;,
  author  = &quot;Stenzel, Tobias&quot;,
  year    = &quot;2023&quot;,
  url     = &quot;https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>]]></content><author><name>Tobias Stenzel</name></author><category term="dl-fundamentals" /><category term="backprop" /><category term="reverse-accumulation" /><category term="compute-graph" /><summary type="html"><![CDATA[💡 Includes a nice detail about why we don't 'frontpropagate'. ;-)]]></summary></entry><entry><title type="html">3. Optimization</title><link href="https://www.tobiasstenzel.com/blog/2023/dl-optimization/" rel="alternate" type="text/html" title="3. Optimization" /><published>2023-03-03T00:00:00+00:00</published><updated>2023-03-03T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/dl-optimization</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/dl-optimization/"><![CDATA[<h2 id="optimization">Optimization</h2>

<p>In the previous section we formalized supervised learning of a
predictive model as solving the optimization problem
\(\theta^* = \arg \min_{\theta \in \Theta} C(\theta)\), where \(\theta\)
denotes the model parameters and cost function \(C\) represents the
average loss of all examples plus a regularization penalty. In realistic
settings, there is no closed form solution to this problem. Therefore,
we have to rely on schemes that iteratively proposes new parameters
given the previous choice or the initial guess. We want to choose a
method that proposes new candidates with a high likelihood of reducing
the loss compared to the current parameters, so we can replace them.</p>

<h3 id="first-order-methods">First order methods</h3>
<p>In practice, a neural network is a composition
of many small functions that are easy to differentiate. Therefore, we
can compute the gradient \(\nabla_\theta C\) from our network via the
backpropagation technique. We will discuss this method in detail in the
next section. The gradient of our cost function is a vector of
first-order partial derivatives. It contains the direction of its
fastest increase and its magnitude is the rate of increase in that
direction. The gradient at a minimum of the loss function equals zero.
Therefore, we can use the negative gradient as search direction for
selecting the next proposal \(\theta_{i+1}\) from current candidate
\(\theta_i\). This is the basic idea of the gradient descent algorithm.
The method alternates between two steps: 1.) Compute the gradient
\(\nabla_\theta C(\theta)\). 2.) Update \(\theta\) by subtracting a small
multiple of the gradient. Many applications use very large datasets. For
instance, the number of training images in ImageNet is about 1 million.
Therefore, it is handy to approximate the loss gradient from a small
minibatch of examples (e.g. 100). With that, we can update \(\theta\) many
times for every epoch. An epoch is one iteration over the complete
training set. In practice, a large number of approximate updates works
better than a small number of exact updates. This algorithm is called
Stochastic Gradient Descent (SGD). It is summarized in Algorithm 1.</p>

<figure id="fig:supervised-learning">
<center><img src="/assets/img/dl-series/sgd.png" style="width:90%" /></center>
</figure>

<p>A crucial parameter for gradient descent algorithms is the learning
rate, or step size, \(\eta\). If we set it too small, the optimization
requires too many steps. If we set it too large, the algorithm may not
converge or even diverge. As an illustration, consider the following toy
example with convex objective function: Let \(f=x^2\) with gradient
\(\frac{df}{dx}=2x\) and initial parameter value \(x = 1\). With \(\eta&lt;1\)
the algorithm finds \(x^*=0\). However, with \(\eta=1\), it oscillates
between \(x=-1\) and \(x=1\), and with \(\eta&gt;1\) it diverges to \(x=\infty\).
Generally, finding a useful learning rate depends on the objective
function.</p>

<h3 id="advanced-first-order-methods">Advanced first order methods</h3>

<p>Oftentimes, we can achieve faster
convergence speed with modified formulations of the update direction
\(\Delta \theta\) from Step 3 in Algorithm 1. The
two main ideas are to use weighted moving averages of all gradients so
far and to compute parameter-specific learning rates. Common variants
are Momentum <d-cite key="sutskever_importance_2013"></d-cite>, Adagrad <d-cite key="duchi_adaptive_2011"></d-cite>, RMSProp <d-cite key="hinton_lecture_2012"></d-cite> and Adam <d-cite key="kingma_adam_2014"></d-cite>. The <strong>Momentum</strong> update is inspired by the physics
notion of momentum. The current update direction is not determined by
the current gradient but also by the previous gradients. The impact of
the previous gradients, however, decays exponentially for every previous
iteration. Let \(\alpha\) be the decay parameter and
\(g := \nabla_{\theta}C(\theta)\). We then replace Step 3 by two steps:
First, we compute the "velocity" \(v= \alpha v + g\) (initialized at
zero), and second, we compute the parameter update with
\(\Delta \theta = -\eta v\). <strong>Adagrad</strong> introduces element-wise learning
rates for the gradient. I.e. we weigh every partial derivative with the
moving average of its squared sum. The motivation is two-fold: first the
shape of the objective function can vary between different dimensions.
Therefore, dimension-specific learning rates may improve the algorithm.
Second, we can achieve a more equal exploration of each dimension by
equipping dimensions with a history of small gradients with larger step
sizes and vice versa. Formally, Adagrad first introduces the
intermediate variable \(r = r + g \odot g\) where \(\odot\) denotes
elementwise multiplication. Second, it computes the gradient update with
\(\Delta \theta = -\frac{\epsilon}{\delta + \sqrt{r}} \odot g\). In the
last expression, \(\delta\) is a small number to avoid division by zero.
<strong>RMSProp</strong> introduces an additional hyperparameter to weigh Adagrad’s
running average, i.e. \(r s= \rho r + (1-\rho)g \odot g\). Lastly, the
<strong>Adam</strong> update combines gradient momentum and RMSProp’s
dimension-specific learning rates.</p>

<h3 id="hyperparamter-optimization">Hyperparamter optimization</h3>

<p>In the discussion of supervised learning
and optimization, we encountered many configurations that we did not
specify. Examples are the number of hidden units \(H\) in our neural
network, the regularization strength \(\lambda\), or the learning rate
\(\eta\) in gradient descent. These parameters either change the
composition of the model parameters \(\lambda\) or impact their selection
during training. Therefore, they have to be set before a model is
trained. To this end, we have to select our hyperparameter space
\(\mathcal{H}\). Due to restrictions of time and computational power, this
choice is often based on experiences with similar models from previous
research. Principled approaches range from model-free methods like
random search to global optimization frameworks like Bayesian
optimization. Model-free methods are simple and do not use the
evaluation history whereas global optimization techniques additionally
consider the uncertain trade-off between exploring new values and
exploiting good values that have already been found. I recommend the
textbook chapter by <d-cite key="feurer_hyperparameter_nodate"></d-cite> for more explanations
and concrete examples.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = &quot;Deep Learning Series&quot;,
  author  = &quot;Stenzel, Tobias&quot;,
  year    = &quot;2023&quot;,
  url     = &quot;https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>]]></content><author><name>Tobias Stenzel</name></author><category term="dl-fundamentals" /><category term="optimization" /><category term="first-order-methods" /><summary type="html"><![CDATA[Optimization]]></summary></entry><entry><title type="html">2. Supervised Learning</title><link href="https://www.tobiasstenzel.com/blog/2023/dl-supervised-learning/" rel="alternate" type="text/html" title="2. Supervised Learning" /><published>2023-03-02T00:00:00+00:00</published><updated>2023-03-02T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/dl-supervised-learning</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/dl-supervised-learning/"><![CDATA[<h2 id="supervised-learning">Supervised Learning</h2>

<p>Prediction rules based on some specific set of information can be
written as a mapping \(f:X \rightarrow Y\), where \(X\) is an input space
and \(Y\) is an output space. To recognize a dog on a photo, for example,
\(X\) would be the space of images and \(Y\) would be a probability interval
\([0,1]\) for the presence of a dog. However, it is oftentimes very
difficult to find an explicit function \(f\) from theoretical
considerations about the problem. For problems where it is easy to find
many examples \((x,y) \in X \times Y\), the supervised learning approach
is usually well-suited. In our example, the requirement would be a
dataset that consists of a large number of images which are annotated
with presence or absence of a dog.</p>

<h3 id="loss-function">Loss function</h3>

<p>To be concrete, our dataset
\(\{(x_1,y_1),...(x_n,y_n)\}\) includes \(n\) examples. Our theoretical
assumption about the data is that these examples are drawn from a data
generating distribution \(D\) with independent and identically distributed
(i.i.d.) random variables, i.e.
\((x_i, y_i) \overset{\mathrm{iid}}{\sim} D\). In the supervised learning
context, learning the mapping \(f:X \rightarrow Y\) means selecting the
function \(f\) from a set of candidate functions \(\mathcal{F}\) so that \(f\)
yields the best predictions for \(y\) given any \(x\). I.e., we want to find
the best approximation for \(f(Y|X)\). We achieve this by selecting a
scalar-valued loss function \(L(\hat{y},y)\) which measures the difference
between prediction \(\hat{y}_i\) and actual outcome \(y_i\). In theory, our
objective is to find the function with the lowest expected loss for all
examples from the data generating distribution \(D\). In practice,
however, we have to rely on our dataset. Therefore, we approximate our
theoretical objective using the assumption that our data is drawn from
an i.i.d. distribution. We search for function \(f^*\) with the lowest
average loss over the data:</p>

\[\begin{align}
\label{eq:loss}
f^* \approx \arg \min_{f \in \mathcal{F}} \frac{1}{n} \sum^n_{i=1} L \big  ( \ f(x_i), y_i \big  ).
\end{align}\]

<h3 id="regularization">Regularization</h3>

<p>Oftentimes, the i.i.d. assumption is too strong. In
this case, we usually do not achieve the best result for predicting
unseen outcomes with a function that is optimized solely with regards to
our dataset. In short, this function does not necessarily generalize
well. A further issue is how to choose between multiple functions with
the same minimal loss. The approach that addresses both problems at once
is regularization. The idea is to add a regularization penalty to our
objective function from which we obtain our predictor. The penalization
criterion \(R(f)\) is function complexity. We thus search for the function
that fits the data best <em>and</em> has a low complexity:</p>

<p>\(\begin{align}
\label{eq:cost}
f^* \approx \arg \min_{f \in \mathcal{F}} \frac{1}{n} \sum^n_{i=1} L \big  (f(x_i), y_i \big  ) + R(f),
\end{align}\)
where \(R(f)\) is a scalar-valued function. With regularization, we can
achieve a better generalization and choose between functions that
achieve a similar loss.</p>

<p>Frequently, we have already selected our model but not its parameters.
Hence, we take function \(f\) as given but we want to learn the best
parameters \(\theta\) for this function. In this situation, the concept of
loss and regularization directly translates from finding the optimal
function to finding the optimal parameters:</p>

\[\begin{align}
\label{eq:params}
\theta^* \approx \arg \min_{\theta \in \Theta} \frac{1}{n} \sum^n_{i=1} L \big (f(x_i;\theta), y_i \big  ) + R(\theta),
\end{align}\]

<p>where \(\Theta\) denotes the parameter space. Common examples for
\(R(\theta)\) are multiples of vector norms. In this setting, choices like
the model, the loss or the regularization are called hyperparameters.
These are parameters that are set before the actual model parameters are
learned. Hyperparameter choices are oftentimes critical to the quality
of the learned model. There are several other ways to prevent
overfitting the model besides regularization. Examples are choosing
simpler models, stopping the optimization process early, changing or
disabling some model units during training (dropout), and dataset
augmentations.</p>

<h3 id="model-validation">Model validation</h3>

<p>How do we test whether our model generalizes well
to unseen data? The usual machine learning approach is as follows: at
first, we split our examples in three groups: a large training, and
smaller validation and test sets. Secondly, we learn the model
parameters with the training data. The third step is to compute
evaluation metrics for this model from the unseen test data. In general,
we want to minimize the distance between the prediction and the target
vector. For classification tasks, we can use evaluation metrics based on
the confusion matrix. This enables us to identify single classes which
are more difficult to predict for the model. Optionally, we can repeat
the third step multiple times for different hyperparameter
configurations and compare the generalization errors. Then, we evaluate
the best model once again on the validation data and report its
performance. We will briefly discuss hyperparameter selection in the end
of the optimization section.</p>

<h3 id="example--linear-regression">Example — Linear regression</h3>

<p>Assume we have a dataset with 100
observations \((n=100)\) of two features each \((m=2, X=\mathbb{R}^2)\) and
a scalar annotation \((Y=\mathbb{R})\). According to the input and output
space, we choose to restrict the candidate class \(\mathcal{F}\) to the
family of linear functions, i.e.
\(\mathcal{F}=\{w^Tx + b | w \in \mathbb{R}^2, b\in \mathbb{R}\). With
this choice, we set our hypothesis space from a class of functions to
the set of three parameters \(\theta=\{w_1, w_2, b\}\) with
\(w=[w_1, w_2]\). Next, I list two common hyperparameter choices. First,
the squared difference between predicted value and target,
\(L(\hat{y}, y)=(\hat{y} - y)^2\), is a common loss function. Second, the
L2 norm of the weights multiplied with importance parameter \(\lambda\) is
a typical regularizer choice, i.e. \(R(w,b)=\lambda(w_1^2 + w_2^2)\). The
L2 norm counteracts extreme weights and an excessive effect of one
single weight on the prediction \(\hat{y}\). Taken together, our objective
is</p>

\[\begin{align}
\theta^* =  \arg \min_{w,b} \underbrace{
    \Bigg [ \frac{1}{n} \sum^n_{i=1} (w^T x_i + b - y_i)^2 \Bigg ]}_\text{data fitting}
    + \underbrace{\Bigg [ \lambda (w_1^2 + w_2^2) \Bigg ]}_\text{regularization}.
\end{align}\]

<h3 id="example--neural-network-regression">Example — Neural network regression</h3>

<p>Perhaps the relationship between
features \(x_1, x_2\) and the scalar target \(y\) is not liner and there are
interactions between the two features. A model class that is well-known
for its theoretical capabilities of approximating continuous functions
are feedforward neural networks (FNN). As a preview, we can extend the
previous example to a specific FNN called neural network regression. It
has the form \(f(x;\theta)= w_2 \text{ tanh} (W_1^T x + b_1) + b_2\) with
parameters \(\theta=\{W_1, b_2, w_1, b_1\}\). \(W_1\) is matrix with
dimension \(H \times 2\), \(b_1\) and \(w_2\) are both vectors of length \(H\),
and \(b_2\) is a scalar. \(H\) is an integer-type hyperparameter.
\(W_1^T x + b_1\) is often called a hidden layer. Its elements, or
neurons, are outputs of multiplicative interactions between elements
from previous layers, in this case the two inputs \(x_1\) and \(x_2\). tanh
denotes the hyperbolic tangent that squashes elements from the
real-valued domain to the interval [-1,1]. It is applied element-wise
and introduces non-linearity to the model. The objective is given by:</p>

\[\begin{align}
    \theta^* =  \arg \min_{W_1, b_2, w_1, b_1} \underbrace{
    \Bigg [ \frac{1}{n} \sum^n_{i=1} \big (w_2 \text{ tanh}  ( W_1^T x_i + b_1 ) + b_2 - y_i \big)^2 \Bigg ]}_\text{data fitting}
    + \underbrace{\Bigg [ \lambda \big (||W_1||^2_2 + ||w_2||_2^2 \big ) \Bigg ]}_\text{regularization}.
\end{align}\]

<h3 id="example--neural-network-classification">Example — Neural network classification</h3>

<p>Oftentimes, we do not want
to predict a real number but we want to predict whether an input
corresponds to an output of a specific class \(k\) of \(K\) possible
classes. For instance, we may want to predict whether a dog, a, cat, or
a budgie is shown in a picture. To formalize this problem, we encode our
classes as non-negative integers starting at 0. We also encode our
output as a vector of \(|K|\) zeros, where only the \(k^*\)-th element
representing the actual class encoded as \(k^*\) is set to one. Using the
previous order, a picture that shows a dog is encoded as \(y=[1, 0, 0]\).
We can achieve a model that predicts a vector of this form with two
simple adjustments of the neural network regression model. The first
adjustment is replacing vector \(w_2\) by matrix \(W_2\) with dimensions
\(K \times H\). This gives us a real-valued vector \(z\) with one
real-valued element for every class. The second adjustment is that we
compute class probabilities by applying the softmax function to \(z\),
i.e. \(\hat{p}_k = e^{z_k} / \sum_{i=1}^K e^{z_i}\). Handy properties of
the softmax function in the probability context are, first, that every
element is mapped to \([0,1]\), and second, that the sum of all elements
is normalized to one. Hence, our class prediction would be the class
that is represented by the largest element in vector \(\hat{p}\). For
instance, with \(K=3\) and \(\hat{p}=[0.1, 0.7, 0.2]\), our prediction
\(\hat{y}\) equals \([0, 1, 0]\). The most common loss function for
classification is the cross-entropy loss. It takes the predicted class
probabilities \(\hat{p}\) instead of the predicted class vector \(\hat{y}\).
We denote both options by output \(o\):</p>

<p>\(\begin{align}
L(o,y) = L(\hat{p},y) = - \sum_{k=1}^K y_k \log \hat{p}_k = -\log  \hat{p}_{k=k^*}.
\end{align}\)
The first equality is the cross-entropy definition for two distribution,
i.e \(H(q,r) = - \sum_x q(x) \log r(x)\). The second equality shows that
the loss equals the negative log probability from vector \(p\) at the
position of the actual class \(k^*\). It follows from the fact that target
distribution is degenerate. This means that only the true class has
probability one. Additional motivation for choosing the cross-entropy
for classification problems is that minimizing this loss is equivalent
to maximizing the likelihood of observing model parameters \(\theta\)
conditional on predicting the true class label.</p>

<h2 id="summary">Summary</h2>

<p>Supervised learning requires a dataset of \(n\) examples
\(\{(x_1,y_1)\), ..., \((x_n,y_n)\}\), where \((x_i,y_i) \in X \times Y\).
\(y_i\) represents the annotation that we want to predict based on
features \(x_i\). Finding a mapping \(f:X \rightarrow Y\) is a two-step
process. First, we have to formalize the problem by choosing</p>

<ol>
  <li>
    <p>The search space of functions \(\mathcal{F}\) with
\(f \in \mathcal{F}\).</p>
  </li>
  <li>
    <p>The scalar-valued loss function \(L(\hat{y}, y)\) that measures the
difference between the network’s predictions
\(\hat{y} = f_{\theta}(x)\) and the target \(y\).</p>
  </li>
  <li>
    <p>The scalar term \(R(f)\) that penalizes overly complex functions.</p>
  </li>
</ol>

<p>In deep learning without architecture search, the space of functions is
one neural network \(f_\theta\) with some parameters \(\theta \in \Theta\).
Putting these parts together, the second step is to find these
parameters from the optimization problem
\(\theta^* = \arg \min_{\theta \in \Theta} C(\theta)\) with
\(C(\theta) = \frac{1}{n} \sum^n_{i=1} L \big (f_\theta), y_i \big  ) + R(f_{\theta})\).
We call \(C(\theta)\) the cost function.</p>

<figure id="fig:supervised-learning">
    <center><img src="/assets/img/dl-series/supervised-learning.png" style="width:100%" /></center>
</figure>
<p><b>Figure 1: Computational graph for a general supervised learning
approach.</b> Examples <span class="math inline">{<em>x</em><sub><em>i</em></sub>}<sub><em>i</em> = 1</sub><sup><em>n</em></sup></span>
and parameters <span class="math inline"><em>θ</em></span> are taken by
model f to predict the targets by <span class="math inline">{<em>ŷ</em><sub><em>i</em></sub>}<sub><em>i</em> = 1</sub><sup><em>n</em></sup></span>.
Data loss L computes the difference between the predictions and the
targets <span class="math inline">{<em>y</em><sub><em>i</em></sub>}<sub><em>i</em> = 1</sub><sup><em>n</em></sup></span>.
Regularization loss R penalizes extreme parameters. The sum of both
penalties is given by cost C. Oftentimes we use predicted class
probabilities <span class="math inline"><em>p̂</em></span> instead of
(rounded) predictions.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = &quot;Deep Learning Series&quot;,
  author  = &quot;Stenzel, Tobias&quot;,
  year    = &quot;2023&quot;,
  url     = &quot;https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>]]></content><author><name>Tobias Stenzel</name></author><category term="dl-fundamentals" /><category term="supervised-learning" /><summary type="html"><![CDATA[Supervised Learning]]></summary></entry><entry><title type="html">1. Overview</title><link href="https://www.tobiasstenzel.com/blog/2023/dl-overview/" rel="alternate" type="text/html" title="1. Overview" /><published>2023-03-01T00:00:00+00:00</published><updated>2023-03-01T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/dl-overview</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/dl-overview/"><![CDATA[<h2 id="overview">Overview</h2>

<p>I wrote most of the content for the Background section of my Master’s Thesis
“Multi-camera Multi-object Tracking with Transformers”. During this time, I discovered some new aspects and thought about
how to best introduce some concepts like backpropagation or the transformer. The purpose of this series is to share
this content about the fundamentals of deep learning with you.</p>

<p>This is the structure:</p>
<ol>
  <li><a href="https://www.tobiasstenzel.com/blog/2023/dl-overview/">Overview</a></li>
  <li><a href="https://www.tobiasstenzel.com/blog/2023/dl-supervised-learning/">Supervised Learning</a></li>
  <li><a href="https://www.tobiasstenzel.com/blog/2023/dl-optimization/">Optimization</a></li>
  <li><a href="https://www.tobiasstenzel.com/blog/2023/dl-backprop/">Backpropgation</a></li>
  <li><a href="https://www.tobiasstenzel.com/blog/2023/dl-fnn/">Feedforward Neural Networks</a></li>
  <li><a href="https://www.tobiasstenzel.com/blog/2023/dl-cnn/">Convolutional Neural Networks</a></li>
  <li><a href="https://www.tobiasstenzel.com/blog/2023/dl-rnn/">Recurrent Neural Networks</a></li>
  <li><a href="https://www.tobiasstenzel.com/blog/2023/dl-transformer/">Transformer</a></li>
</ol>

<h2 id="highlights">Highlights</h2>

<p>Some highlights are:</p>

<ol>
  <li>Preparing the introduction of the transformer carefully via self-made, detailed figures of <a href="https://www.tobiasstenzel.com/blog/2023/dl-rnn/#fig:vanilla-rnn">vanilla RNN</a>, <a href="https://www.tobiasstenzel.com/blog/2023/dl-rnn/#fig:encoder-decoder-rnn">encoder-decoder RNN</a> and <a href="https://www.tobiasstenzel.com/blog/2023/dl-transformer/#fig:attention/">encoder-decoder RNN with attention</a> ✨</li>
  <li>Explicitly spelling out <a href="https://www.tobiasstenzel.com/blog/2023/dl-transformer/#the-complete-transformer-architecture">what makes the transformer great</a> ✨</li>
  <li>Explaining convolution (or rather cross-correlation) with the <a href="https://www.tobiasstenzel.com/blog/2023/dl-cnn/#cross-correlation">right figure and equation</a> ⚖</li>
  <li>Explaining backpropagation with a very simple <a href="https://www.tobiasstenzel.com/blog/2023/dl-backprop/#toy-example">toy example</a> 🪀</li>
  <li>An explanation of <a href="https://www.tobiasstenzel.com/blog/2023/dl-backprop/#reverse-accumulation">why we do not “frontpropagate”</a> although the multiplication in the chain rule is commutative 🤯</li>
  <li>Showing backward passes of important layers. Did you know that the <a href="https://www.tobiasstenzel.com/blog/2023/dl-cnn/#fig:gradient-cross-correlation">backward pass of a convolution</a> is also a convolution? 🤯</li>
</ol>

<p>In my view, many deep learning concepts are usually introduced way too quickly or there is not enough reflection about the properties of some architectures. An example of the first observation is showing the pictures of the architecture from the original transformer paper without clarifying the encoder-decoder
beforehand in greater detail. Examples for the second observation are that it is frequently not stated what makes
the transformer great, what the inductive bias of a convolutional layer is, or why we even propagate gradients back. I took
the time to think about and research these things.</p>

<h2 id="target-group">Target group</h2>

<p>In this series, the content is very dense, although I go into some detail regarding the main concepts. Ideally, there would be much more pictures, too. As a consequence, these posts are not for beginners. Instead, I recommend this series to three groups of people:</p>

<ol>
  <li>as complementary material for students who are taking a deep learning class right now</li>
  <li>people who want to refresh some already present knowledge</li>
  <li>DL professionals who can perhaps fill some small gaps.</li>
</ol>

<h2 id="credits">Credits</h2>

<p>Of course, this series is essentially a recompilation of material from other people.
These are my main references: I recommend the Deep Learning book by Goodfellow <d-cite key="goodfellow_deep_2016"></d-cite>
, the CS231n lecture notes by Fei-Fei Li <d-cite key="li_cs231n_2018"></d-cite>, and Andrej Karpathy’s dissertation <d-cite key="karpathy_connecting_2016"></d-cite> Other references are the Wikipedia article about automatic differentiation <d-cite key="noauthor_automatic_nodate"></d-cite>
, these blogposts about convolutional neural nets <d-cite key="kafunah_backpropagation_2016"></d-cite>, recursive neural nets <d-cite key="arat_backpropagation_2019"></d-cite>, and these posts about transformers <d-cite key="weng_attention_2018"></d-cite>, <d-cite key="karpathy_transformer_2022"></d-cite>, <d-cite key="vaswani_transformers_2021"></d-cite>.</p>

<p>I also want to thank Prof. Rainer Gemulla for his excellent lectures, especially his Deep Learning class, at Mannheim University.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = &quot;Deep Learning Series&quot;,
  author  = &quot;Stenzel, Tobias&quot;,
  year    = &quot;2023&quot;,
  url     = &quot;https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>]]></content><author><name>Tobias Stenzel</name></author><category term="dl-fundamentals" /><category term="highlights" /><category term="motivation" /><category term="target-group" /><category term="credits" /><summary type="html"><![CDATA[🎬 Read this post before any other post.]]></summary></entry></feed>