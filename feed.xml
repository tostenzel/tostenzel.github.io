<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://www.tobiasstenzel.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.tobiasstenzel.com/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-03-29T17:56:19+00:00</updated><id>https://www.tobiasstenzel.com/feed.xml</id><title type="html">blank</title><subtitle>Tobias&apos; homepage.
</subtitle><entry><title type="html">8. Transformer</title><link href="https://www.tobiasstenzel.com/blog/2023/dl-transformer/" rel="alternate" type="text/html" title="8. Transformer" /><published>2023-03-08T00:00:00+00:00</published><updated>2023-03-08T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/dl-transformer</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/dl-transformer/"><![CDATA[<h2 id="transformer">Transformer</h2>

<p>In the last section we learned that RNNs have problems with learning
relations between the first parts of an input sequence with the output
and later parts of the input sequence. An architecture without this
structural problem is the transformer. The architecture was published by <d-cite key="vaswani_attention_2017"></d-cite> and applied to machine translation. We will
develop this model step-by-step, starting with its core component,
attention.</p>

<h3 id="attention">Attention</h3>

<p>Let us introduce this concept with an example. Figure
<a href="#fig:attention">7</a> shows an encoder-decoder network with attention similar to the previous
encoder-decoder RNN (Figure
<a href="https://www.tobiasstenzel.com/blog/2023/dl-rnn/#fig:encoder-decoder-rnn">6</a>). The core idea of attention is
defining the hidden state of the decoder-RNN as a function of every
hidden state from the encoder-RNN for every time period without
recursion. The result of the attention function is the context vector.
We will use this vector for every output element. In the specific
network in Figure <a href="#fig:attention">7</a>, the context vector is a function of both the
decoder states \(s\) and the encoder states \(h\). Further, it is
additionally concatenated with \(s\) to predict the output layer.</p>

<figure id="fig:attention">
<center><img src="/assets/img/dl-series/2h-attention.png" style="width:50%" /></center>
</figure>
<p><b>Figure 7. Encoder-Decoder with attention.</b> In contrast to the encoder-decoder RNN, the output layer is a function of the concatenation of the hidden states and a time-dependent context vector (black boxes). The main idea is that the context vector \(c_t\) is a function of all hidden states \(\{h_t\}_{t=1}^{T}\) instead of only the last one (and the previous state \(s_{t-1}\)). This function is called <em>attention</em> (red). The black box represents vector concatenation. \(c_1\) is initialized with $h_4$, \(s_1\) with arbitrary values, and \(o_1\) is discarded.
<br /></p>

<p>The attention function that returns the context vector for output \(y_t\)
wit attention scores for each input is given by the following
expression:</p>

\[\begin{align}c_t = \sum_{i=1}^n \alpha_{t,i} h_i\end{align}\]

<p>\(\alpha_{t,i}\) is a softmax function of another function <em>score</em> that
measures how well output \(y_t\) and input \(x_i\) are aligned through the
encoder state \(h_i\):</p>

\[\begin{align}\alpha_{t,i} = \text{align}(y_t,x_i) = \frac{\exp \text{score}(s_{t-1}, h_i)}{\sum_{j=1}^n\exp\text{score} (s_{t-1}, h_j)}.\end{align}\]

<p>There are many different scoring functions <d-cite key="weng_attention_2018"></d-cite>. A
common choice is the scaled dot-product score
\((s_t, h_i)=\frac{s_t^T h_i}{\sqrt{d}}\), where \(d\) is the hidden state
dimension of both encoder and decoder states. Here, the alignment score
for one sequence element is given by a relative score of the dot-product
between the respective encoder hidden state and the current decoder
hidden state. We scale down the dot product to prevent vanishing
gradients from a pass to a softmax layer. After training the model, we
can analyze how much each output element depends on, or <em>attends</em> to,
each input. We do this by assembling a table with outputs as columns and
the output-specific alignment scores for each input as rows.</p>

<p>Another option for an encoder-decoder with attention is using a
self-attention mechanism to compute the context vector, for example,
with score \((h_j, h_i)=\frac{h_j^T h_i}{\sqrt{d}}\). We can use the
scores, for instance, in machine translation, to model how important the
previous words are for translating the current word in a sentence. Note
that we can execute many of these operations in parallel for the whole
input and output sequences using matrix operations.</p>

<p>Next, we will discuss an expansion of the attention mechanism and how it
is applied to the transformer’s encoder and decoder, separately.
Finally, we will look at the complete model, and how it replaces the
positional information from the encoder RNN in a simple way without any
recurrence.</p>

<h3 id="key-value-and-query">Key, Value and Query</h3>

<p>As we do not use recurrence of single sequence
elements anymore, let us denote the whole sequence of input embeddings
by \(X \in \mathbb{R}^{L \times D^{(x)}}\). \(L\) can either be the complete
input length \(T_x\) or later only a fraction of it. \(D^{(x)}\) is the
input embedding’s length. Let us denote the sequence of output
embeddings by \(Y \in \mathbb{R}^{M \times D^{(y)}}\). The transformer
uses an extension of the attention mechanism, the multi-head attention,
as its core building block. The first step is that, instead of using the
softmax of the scaled dot-product between encoder states \(h\) and decoder
states \(s\) directly as in the last section, it uses the scaled
dot-product with two different input encodings,
\(K=XW^k \in \mathbb{R}^{L \times D_k}\) and
\(V=XW^v \in \mathbb{R}^{L \times D_v}\), and an output encoding
\(Q=YW^q \in \mathbb{R}^{M \times D_k}\), with
\(W^k \in \mathbb{R}^{D^{(x)} \times D_k}, W^q \in \mathbb{R}^{D^{(y)} \times D_k}\)
and \(W^v \in \mathbb{R}^{D^{(x)} \times D_v}\). Note that source and
target embeddings are projected into a shared or common vector space with consistent dimensions. With that, we can compute the dot product between input and output sequences of different fixes maximal length and obtain \(D_v\) embeddings of dimension \(M\). We compute
attention with</p>

\[\begin{align}c(Q,K,V)=\text{Softmax}\big(\frac{QK^T}{\sqrt{n}}\big)V.\end{align}\]

<p>In this equation, we refer to \((K, V)\) as key-value pairs, and \(Q\) as the query (see Appendix for an explanation of the names). By interpreting the dot product as a similarity measure, the context matrix \(c(Q,K,V) \in \mathbb{R}^{M \times D_v}\) illustrates the similarity between the input and a representation of the input, which is weighted by its similarity to the output (in the context of cross-attention).</p>

<p>Let’s summarize the computation in simpler terms: The context matrix comprises \(M\) rows, one for each output element, and \(D_v\) columns, corresponding to the dimensions of the value vectors. To delve deeper, we consider each row in the resulting context matrix \(c(Q, K, V)\). For a specific output element, its row is computed by taking a weighted sum of the value vectors from matrix \(V\). These weights are determined by the similarity between the query vector for that output element (from matrix \(Q\)) and the key vectors for all elements in the input sequence (from matrix \(K\)). The Softmax function is applied to normalize these weights for each output element.</p>

<p>Importantly, we apply masking to the embeddings for unseen target elements at each time step, ensuring that only relevant input elements contribute to the computation. The context matrix serves as the foundation for subsequent computations in the attention mechanism, allowing the model to determine how much attention to allocate to each part of the input sequence when generating the output.</p>

<p>In the transformer encoder, there is an important module where the queries are also source representations and in the decoder, there is a module where keys and values are also target representations (self-attention).</p>

<h3 id="multi-head-attention">Multi-Head Attention</h3>

<p>Instead of computing the attention once, the
multi-head approach splits the three input matrices into smaller parts
and then computes the scaled dot-product attention for each part in
parallel. The independent attention outputs are then concatenated and
linearly transformed into the next layer’s input dimension. This allows
us to learn from different representations of the current information
simultaneously with high efficiency. In the <a href="https://www.tobiasstenzel.com/blog/2023/dl-cnn/">CNN post</a>, we have already introduced the principle of applying the same operation multiple times with different learned sets of parameters: remember that CNNs contain stacks of multiple filters to provide the model with multiple feature maps, each covering different aspects of the input image. Multi-head attenttion is defined by</p>

\[\begin{align}\text{MultiHead}(X_q, X_k, X_v)= [ \text{head}_1;...;\text{head}_h ] W^o,\end{align}\]

<p>where \(\text{head}_i=\)Attention\((X_q W^q_i, X_k W^k_i, X_v W^v_i)\) and \(W_i^q  \in \mathbb{R}^{D^{(y)} \times D_v /H}\), \(W_i^k \in \mathbb{R}^{D^{(x)} \times D_k / H}\), \(W_i^v \in \mathbb{R}^{D^{(x)} \times D_v /H}\)
are matrices to map input embeddings of chunk size \(L \times D\) into
query, key and value matrices. \(W^o \in \mathbb{R}^{D_v \times D}\) is
the linear transformation in the output dimensions. These four weight
matrices are learned during training. Target self-attention and cross
attention layers compute outputs in \(\mathbb{R}^{M \times D}\), and
source self-attention calculates outputs in \(\mathbb{R}^{L \times D}\).</p>

<h3 id="transformer-encoder">Transformer Encoder</h3>

<p>The Transformer architecture, as introduced by Vaswani et al. in 2017 <d-cite key="vaswani_attention_2017"></d-cite>, represents a groundbreaking advancement in deep learning, particularly in the fields of natural language processing and sequence modeling. However, nowadays it is the go-to model for many other areas, especially when a lot of resources are available. The first major part of the Transformer is its encoder, which plays a pivotal role in generating rich input representations using self-attention mechanisms.</p>

<figure id="fig:transformer-encoder">
<center><img src="/assets/img/dl-series/2i-transformer-encoder.png" style="width:40%" /></center>
</figure>

<p><strong>Figure 8. Transformer encoder.</strong> The Transformer Encoder consists of a stack of identical layers, including a multi-head self-attention layer (Red) for capturing contextual information and a position-wise fully-connected feed-forward network for introducing non-linearity. These two components work synergistically to process input sequences effectively and extract meaningful representations, with attention parameters focusing on dependencies and position-wise FNN parameters capturing position-specific patterns. Image source: <d-cite key="lin_survey_2022"></d-cite>.</p>

<p>In its original form, the encoder consists of a stack of N = 6 identical layers, each with unique parameters. These layers consist of two critical components:</p>

<ol>
  <li>
    <p><strong>Multi-head self-attention layer (red):</strong> This layer is the cornerstone of the Transformer’s ability to capture contextual information from input sequences. It allows the model to focus on different parts of the input sequence simultaneously. Self-attention is a mechanism where each input element contributes to the representation of every other element, making it robust to capturing long-range dependencies and essential for tasks such as machine translation <d-cite key="vaswani_attention_2017"></d-cite>.</p>
  </li>
  <li>
    <p><strong>Position-wise fully-connected feed-forward network (blue):</strong> This layer introduces non-linearity into the network, allowing the model to capture complex patterns and relationships within the data.</p>
  </li>
</ol>

<p>These two components, self-attention and position-wise FNN, are fundamental in processing input sequences of varying length effectively and extracting meaningful representations. It is crucial to understand the clear distinction between the parameters used in the attention mechanism and those employed in the position-wise fully-connected feed-forward network (FNN) in order to understand the modules specific tasks, mechanisms and how they can process sequences of varying length:</p>

<ul>
  <li>
    <p><strong>Attention parameters:</strong> The attention mechanism, which consists of the attention parameters, is responsible for capturing dependencies and relationships between elements within the input sequence. It does this by assigning different attention weights to each element in the sequence based on its relevance to other elements. This allows the model to focus more on important elements and less on irrelevant ones. Importantly, the attention mechanism can adapt to input sequences of different lengths because the attention weights are calculated dynamically for each position in the sequence. Longer sequences may receive different attention distributions compared to shorter sequences.</p>
  </li>
  <li>
    <p><strong>Position-wise FNN parameters:</strong> The position-wise FNN introduces non-linearity and complexity into the model. While the same position-wise FNN is applied to each position within the input sequence independently, the transformations performed by this network can capture position-specific patterns and relationships. This means that even though the same FNN parameters are shared across positions, the content of the positions can lead to different activations and outputs, allowing the model to handle sequences of varying lengths. The majority of the model’s parameters are part of this module.</p>
  </li>
</ul>

<p>The combination of attention and position-wise FNN parameters enables the transformer to process input sequences of different lengths by dynamically adjusting the attention weights and capturing position-specific information. The attention mechanism provides a mechanism for the model to focus on relevant parts of the sequence, while the position-wise FNN allows for nonlinear transformations that can adapt to different content in the sequence. This flexibility is one of the key strengths of the Transformer architecture and makes it well-suited for a wide range of natural language processing tasks where input sequences may vary in length.</p>

<h3 id="transformer-decoder">Transformer Decoder</h3>

<p>The decoder network, illustrated in Figure <a href="#fig:transformer-decoder">9</a>, is equally crucial in the Transformer architecture, as it enables the generation of sequences autoregressively based on the encoded source representation.</p>

<figure id="fig:transformer-decoder">
<center><img src="/assets/img/dl-series/2j-transformer-decoder.png" style="width:66%" /></center>
</figure>

<p><strong>Figure 9. Transformer decoder.</strong> The Transformer decoder plays a crucial role in autoregressively generating sequences based on encoded source representations. It includes a masked multi-head self-attention layer to capture dependencies within the target sequence, a multi-head cross-attention layer for accessing relevant source information, and a fully-connected feed-forward network to model complex relationships, all followed by normalized residual layers for stability. Image source <d-cite key="lin_survey_2022"></d-cite>.</p>

<p>Similar to the encoder, the decoder comprises N = 6 identical layers. These layers consist of:</p>

<ol>
  <li>
    <p><strong>Masked Multi-Head Self-Attention Layer (Red):</strong> In the decoder, this layer is masked to prevent attending to future positions in the output sequence. It focuses on capturing dependencies among elements within the target sequence that have already been generated. This masking is crucial for ensuring the autoregressive nature of the decoder, allowing it to generate sequences one element at a time <d-cite key="vaswani_attention_2017"></d-cite>. Specifically, the masked multi-head self-attention layer in the decoder prevents the model from attending to future positions in the (Shifted) Outputs sequence. This mechanism ensures that each position in the (Shifted) Outputs sequence is generated based on the previously generated elements, adhering to the autoregressive nature of sequence generation.</p>
  </li>
  <li>
    <p><strong>Multi-Head Cross-Attention Layer (Red):</strong> The decoder generates queries from the previously generated target representations and uses them to attend to the encoded source representations. This mechanism ensures that the decoder accesses relevant information from the source to generate the next part of the target sequence.</p>
  </li>
  <li>
    <p><strong>Fully-Connected Feed-Forward Network (Blue):</strong> Similar to the encoder, this layer introduces non-linearity, enabling the decoder to model complex relationships.</p>
  </li>
</ol>

<p>Each of these layers is followed by a normalized residual layer (yellow), contributing to the stability and effectiveness of the Transformer’s decoding process.</p>

<h3 id="the-complete-transformer-architecture">The Complete Transformer Architecture</h3>

<p>Figure <a href="#fig:transformer-complete">10</a> presents the complete Transformer architecture:</p>

<figure id="fig:transformer-complete">
<center><img src="/assets/img/dl-series/2k-transformer-complete.png" style="width:100%" /></center>
</figure>

<p><strong>Figure 10. The complete transformer.</strong> In its original form, the Transformer processes both source and target sequences through embedding layers (light blue), creating representations of dimension D = 512 for each element. Adding sinusoidal positional encoding vectors to the embeddings pallows the model to distinguish between different positions and capture positional dependencies. The Transformer’s ability to handle input and output sequences of different lengths is primarily achieved through its masking mechanism. Padding masks ensure that the model does not consider padded elements when calculating attention scores, and the self-attention mechanism can adapt to variable-length sequences while maintaining coherence. The term “(Shifted) Outputs” emphasizes the autoregressive nature of the decoding process, where each output position depends on previously generated positions. Image source <d-cite key="lin_survey_2022"></d-cite>.</p>

<p>The Transformer possesses several critical properties:</p>

<ul>
  <li>
    <p><strong>Inductive Bias for Self-Similarity:</strong> The self-attention mechanism empowers the model to identify recurring patterns or themes within the data, irrespective of their positions. This inductive bias is invaluable in real-world domains where recurrence is a prevalent pattern <d-cite key="vaswani_attention_2017"></d-cite>.</p>
  </li>
  <li>
    <p><strong>Expressive Forward Pass:</strong> The Transformer establishes direct and intricate connections between all input and output elements, facilitating the learning of complex algorithms in a few steps.</p>
  </li>
  <li>
    <p><strong>Wide and Shallow Compute Graph:</strong> Thanks to residual layers and matrix products in attention layers, the compute graph remains wide and shallow. This promotes fast forward and backward passes on parallel hardware while mitigating issues like vanishing or exploding gradients through techniques such as layer normalization and dot product scaling.</p>
  </li>
</ul>

<h3 id="complexity-comparison">Complexity Comparison</h3>

<p>To conclude this chapter, let’s compare the complexities of the three main architectures in deep learning, as shown in Table <a href="#fig:comparison">2</a>, based on the survey by Lin et al. in 2022 <d-cite key="lin_survey_2022"></d-cite>:</p>

<figure id="fig:comparison">
<center><img src="/assets/img/dl-series/complexity-comparison.png" style="width:55%" /></center>
</figure>

<p><strong>Table 2. Comparison of computation complexity between models.</strong> The “length” refers to the number of elements in the input sequence, and “dim” denotes the embedding depth for each element. It’s worth noting that attention mechanisms, as used in Transformers, excel when the input sequence length is much smaller than the embedding depth. This advantage is evident in tasks like machine translation or question-answering, where capturing long-range dependencies is crucial. However, for direct application to image data, which inherently has a larger input length (e.g., CIFAR images with 3072 elements), preprocessing steps are necessary to reduce the input length effectively.</p>

<p>In summary, the Transformer architecture’s success lies in its ability to capture complex dependencies, handle sequences of varying lengths, and process them efficiently through its innovative components, making it a fundamental building block in modern deep learning.</p>

<h2 id="appendix--naming-convention-of-key-value-and-query">Appendix — Naming Convention of Key, Value and Query</h2>

<p>The naming convention of “key,” “query,” and “value” in the context of attention mechanisms is rooted in their respective roles and functions within the mechanism. These names help clarify how each component contributes to the computation of the context vector in attention.</p>

<p><strong>Key (K):</strong> The key is essentially a set of representations derived from the input sequence (or encoder states in the case of sequence-to-sequence models like the Transformer). These representations are crucial for determining how much attention should be given to each element in the input sequence when generating an output. Keys are used to match against the queries.</p>

<p><em>Reasoning:</em> Think of the “key” as a guide or a set of pointers that specify which parts of the input sequence are relevant for generating the output. It helps the model identify what to focus on.</p>

<p><strong>Query (Q):</strong> The query represents what the model is currently trying to generate or pay attention to within the output sequence. In other words, it’s a representation of the current target or decoder state.</p>

<p><em>Reasoning:</em> The “query” represents the current “question” or “target” that the model is interested in. It’s used to determine how well each element in the input sequence (represented by keys) aligns or matches with the current target.</p>

<p><strong>Value (V):</strong> The value component contains the information that will be used to produce the context vector. Like keys, values are derived from the input sequence. However, they represent the content or information associated with each element in the input sequence.</p>

<p><em>Reasoning:</em> The “value” carries the actual information that the model will use when generating the output. It’s like the “answer” or “content” associated with each part of the input sequence.</p>

<p>To put it all together, the attention mechanism calculates a weighted sum of values based on the similarity between the query and keys. This weighted sum, known as the context vector, is used to determine how much each part of the input sequence should contribute to the current output. The keys help identify which parts are relevant, the query specifies what is being looked for, and the values provide the information to be used.</p>

<p>This naming convention, while abstract, makes the roles and relationships of these components intuitive, facilitating a clearer understanding of how attention mechanisms work in neural networks.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = &quot;Deep Learning Series&quot;,
  author  = &quot;Stenzel, Tobias&quot;,
  year    = &quot;2023&quot;,
  url     = &quot;https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>]]></content><author><name>Tobias Stenzel</name></author><category term="dl-fundamentals" /><category term="neural-net-archetype" /><category term="attention" /><category term="transformer" /><category term="generative-models" /><summary type="html"><![CDATA[⏮️ I recommend reading the RNN post first for the encoder-decoder architecture.]]></summary></entry><entry><title type="html">Trackformer — Multi-Object Tracking with Transformers</title><link href="https://www.tobiasstenzel.com/blog/2023/trackformer/" rel="alternate" type="text/html" title="Trackformer — Multi-Object Tracking with Transformers" /><published>2023-03-08T00:00:00+00:00</published><updated>2023-03-08T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/trackformer</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/trackformer/"><![CDATA[<h2 id="1-introduction">1.) Introduction</h2>

<p>This blog post provides a detailed explanation of the transformer-based tracking model Trackformer <d-cite key="meinhardt_trackformer_2022"></d-cite>. Trackformer not only achieved state-of-the art results but has also a comparably simple architecture that facilitates implementation and maintenance for practitioners. The coolest thing about this blog post is the inclusion of two self-made images that depict Trackformer’s training process for different scenarios where objects are (re-)entering, leaving, or moving in a camera-recorded, dynamic scene.<br /></p>

<p>Multi-object tracking (MOT) is an important task in the Computer Vision domain. MOT is closely related to object detection because tracking models utilize the detections in each frame of a video with the aim of assigning the same ID to the same objects across different frames over time. Trackformer uses the transformer-based object detector DEtection TRansformer (DETR) <d-cite key="carion_end--end_2020"></d-cite>. The primary concept behind DETR is to learn a larger number of transformer-decoder embeddings that generate bounding boxes (by cross-attending to the image features), with a set of these embeddings successfully matched to the actual objects in an image. The fundamental idea behind Trackformer is to feed the output embeddings from successfully matched embeddings to the transformer for the next image in a video sequence and train these embeddings to match the same object again. In this configuration, the intertemporally matched embeddings carry the identity of an object over to the next image.<br /></p>

<p>We will delve into both models in more detail separately. First, we will explore DETR <d-cite key="carion_end--end_2020"></d-cite>. I will explain the complete model during inference time, then during training time, including the loss function, and subsequently, we will discuss advantages, disadvantages, and results. Second, we will repeat the same process for Trackformer <d-cite key="meinhardt_trackformer_2022"></d-cite>. Finally, we will learn about Deformable DETR <d-cite key="zhu_deformable_2021"></d-cite>, which is a more efficient version of DETR. Deformable DETR employs deformable attention instead of regular attention. Trackformer actually utilizes Deformable DETR for its detections. The Deformable DETR paper is quite technical, so feel free to skip this part!<br /><br /></p>

<figure id="fig:object-queries">
<center><img src="/assets/img/dl-series/paper-detr.png" style="width:66.6%" /></center>
</figure>

<h2 id="2-detr-as-object-detector">2.) DETR as Object Detector</h2>

<p>Previous object detection systems incorporate various manually designed
elements, such as anchor generation, rule-based training target
assignment, and non-maximum suppression (NMS) post-processing <d-cite key="liu_deep_2018"></d-cite>. These components do not constitute fully end-to-end
solutions. In a recent study, <d-cite key="carion_end--end_2020"></d-cite> introduced DETR,
an object detection approach that eliminates the need for such manual
components. DETR represents the first fully end-to-end object detector
and achieved highly competitive results on the COCO 2017 detection
dataset <d-cite key="lin_microsoft_2014"></d-cite>. The two main components are a set-based
global loss that enforces a subset of unique class predictions via
bipartite matching, and a transformer encoder-decoder architecture with
learned embeddings called <em>object queries</em>. In contrast to <d-cite key="vaswani_attention_2017"></d-cite>, these queries are learned encodings of
potential objects on the image instead of rule-based encoded
representations of the target sequence. This section explains first the
transformer at inference time, and second the bipartite matching loss
with which we train the model.</p>

<h3 id="complete-transformer-based-detr-architecture">Complete transformer-based DETR architecture</h3>

<p>The overall model has
three components as depicted by Figure
<a href="#fig:object-queries">1</a>. The first component is a conventional
CNN backbone that generates a lower-resolution activation map many
channels. The transformation from large embedding length (image height
\(\cdot\) width) to large embedding depth is crucial for the efficiency of
the attention modules (see
Table <a href="https://www.tobiasstenzel.com/blog/2023/dl-transformer/#complexity-comparison">2</a>. The second component is the transformer
encoder-decoder. In contrast to the original decoder by <d-cite key="vaswani_attention_2017"></d-cite>, DETR decodes \(N\) objects at once at each
decoder layer instead of predicting the output sequence in an
autoregressive manner by masking later elements. Because the decoder is
permutation-invariant like the encoder, we also require \(N\) different
decoder input embeddings to generate different results. We achieve this
by feeding learned embeddings with dimension \(d\) that we call <em>object
queries</em> to the decoder. In contrast to that, the original transformer
in <d-cite key="vaswani_attention_2017"></d-cite> takes the decoder outputs from the previous
iteration as decoder inputs. The decoder transforms the \(N\) object
queries into \(N\) output embeddings. The third component is a two-headed
prediction network that is shared for all output embeddings. It is
defined as an FFN, i.e. a a 3-layer vanilla neural net with ReLU
activations for the normalized bounding box values and hidden dimension
\(d\). We normalize the input to the prediction FFNs with a shared
layer-norm. The one head is a linear projection layer to predict the
bounding box values.
The other head is a softmax layer that predicts the
class labels.<br /></p>

<figure id="fig:object-queries">
<center><img src="/assets/img/dl-series/4b-detr-object-queries.png" style="width:100.0%" /></center>
</figure>
<p><b>Figure 1. High-level DETR architecture with CNN backbone,
transformer-encoder decoder and FFN prediction heads.</b> In contrast to the
autoregressive sequence generation in <d-cite key="vaswani_attention_2017"></d-cite>, the outputs are computed at
once by feeding different learned positional encodings, called
<em>object queries</em>, into the transformer decoder. Image source: <d-cite key="carion_end--end_2020"></d-cite>.<br /><br /></p>

<p>The transformer allows the model to use self- and cross-attention over
the object queries to include the information about all potential
objects with pair-wise relations in their prediction of only one
potential object, while using the whole image as context.</p>

<h3 id="object-detection-set-prediction-loss">Object Detection Set Prediction Loss</h3>

<p>During training, we want to
learn predictions that have the right number of no classes and the right
number of classes with the right class label and bounding box. Formally,
we achieve this the following way: From the transformer decoder
prediction head, DETR infers \(N\) predictions for the tuples of bounding
box coordinates and the object class. \(N\) has to be at least as large as
the maximal number of objects on one image. The first tuple element are
the bounding box coordinates, denoted by \(b \in [0,1]^4\). These are four
values: the image-size normalized center coordinates, and the normalized
height and width of the box w.r.t. the input image’s borders. Using
center coordinates and normalization help us to deal with images of
different sizes. Examples for the second tuple element, class \(c\), are
"person A", for person detection, or "car" for object detection. The
remaining class is the "no object" class, denoted by \(\emptyset\). For
every image, we pad the target class vector \(c\) of length \(N\) with
\(\emptyset\) if the image contains less than \(N\) objects. To score
predicted tuples
\(\hat{y}=\{\hat{y}_i\}_{i=1}^N=\{(\hat{c}_i,\hat{b}_i)\}_{i=1}^N\) with
respect to the targets \(y\), <d-cite key="carion_end--end_2020"></d-cite> apply a loss
function that we apply to a class permutation based on an <em>optimal</em>
bipartite matching between the predicted and target tuples. With this
loss function, we jointly maximize the log likelihood of the class
permutation and minimize the bounding box losses.
Figure <a href="#fig:bipartite-matching">2</a> depicts an example of bipartite
matching between predictions and ground truth for a picture of two
seagulls at a shore during training time. We observe that the matching
procedure selects a unique permutation that directly maps exactly one
prediction to one target. Thus, with that approach we do not have to
handle mappings of many similar predicted bounding boxes and classes to
one target, for instance, to the same seagull.</p>

<p><br /></p>

<figure id="fig:bipartite-matching">
<center><img src="/assets/img/dl-series/4a-detr-bipartite-matching.png" style="width:100.0%" /></center>
</figure>
<p><b>Figure 2. DETR training pipeline with bipartite matching loss.</b> The
loss function generates an optimal one-to-one mapping between
predictions and targets according to bounding box and object class
similarity (colors). In this unique permutation of <span class="math inline"><em>N</em></span> classes, class predictions with no
match among the class targets are regarded as “no object” predictions,
too (green). Image source: <d-cite key="carion_end--end_2020"></d-cite>.</p>

<p><br /><br /></p>

<p>The next section describes the loss function required for the bipartite
matching between predicted and target detections that is optimal in
terms of bounding box and class similarity. We find the optimal
permutation of predicted detections
\(\hat{\sigma}\in\mathcal{\mathfrak{S}}_N\) of \(N\) elements from:</p>

\[\begin{align}
    \hat{\sigma} = arg\,min_{\sigma\in\mathcal{\mathfrak{S}}_N} \sum_{i}^{N} \text{L}_{\text{match}}(y_i, \hat{y}_{\sigma(i)}),
\end{align}\]

<p>where \(\text{L}_{\text{match}}\) is a pair-wise matching cost between
target \(y_i\) and prediction \(\hat{y}\) with index \(\sigma(i)\). We compute
the assignment efficiently with the Hungarian matching algorithm <d-cite key="kuhn_hungarian_1955"></d-cite> instead of brute force.</p>

<p>We want to assign predictions and targets that are close in terms of
class and bounding box. Thus, the matching considers both the class
score for every target and the similarity of predicted and target box
coordinates. We reward a high class score for the target class and a
small bounding box discrepancy. To this end, let us denote the index function by \(\text{I}\), the
probability of predicting class \(c_i\) for detection with permutation
index \(\sigma(i)\) by \(\hat{p}_{\sigma(i)}(c_i)\) and the predicted box
by \(\hat{b}_{\sigma(i)}\). With that, we define the pair-wise matching
loss as</p>

\[\begin{align}
\text{L}_{\text{match}}(y_i, \hat{y}_{\sigma(i)}) = -\text{I}_{c_i\neq\emptyset} \hat{p}_{\sigma (i)}(c_i) + \text{I}_{c_i\neq\emptyset} \text{L}_{\text{box}}(b_{i}, \hat{b}_{\sigma(i)}).
\end{align}\]

<p>Here, our objective is to find the best matching for the "real"
classes \(c \neq \emptyset\) by ignoring class predictions that are
directly mapped to "no class" by the prediction. However, the model
still has to learn not to predict too many real classes. Given our
optimal matching \(\hat{\sigma}\), we achieve this by minimizing the
<em>Hungarian loss</em>. The function is given by</p>

\[\begin{align}\text{L}_{\text{Hungarian}}(y, \hat{y}) = \sum_{i=1}^N \left[-\log  \hat{p}_{\hat{\sigma}(i)}(c_{i}) + \text{I}_{c_i\neq\emptyset} \text{L}_{\text{box}}\Big(b_{i}, \hat{b}_{\hat{\sigma}}(i)\Big)\right]\end{align}\]

<p>The difference to the matching loss is two-fold. First, we now penalize
wrong assignments to all classes including "no class" with the first
class-specific term. We do not include the "no class" instances to the
box loss \(\text{L}_{\text{box}}\) because they are not matched to a
bounding box anyway. Second, we scale the class importance compared to
the bounding boxes by taking the log of predicted class probabilities.
In practice, the class term is further reduced for "no class" objects
to take class imbalance into account. The last expression is the box
loss. It is the L1 loss of the bounding box vector \(b_i\). Note, however,
that the L1 loss penalizes larger boxes.
The following equations shows the expression:</p>

\[\begin{align}
    \text{L}_{\text{box}}(b_{i}, \hat{b}_{\sigma(i)}) =  \lambda_{\text{box}}||b_{i}- \hat{b}_{\sigma(i)}||_1,
\end{align}\]

<p>where \(\lambda_{\text{box}} \in \mathbb{R}\) is a hyperparameter. We
normalize the loss by the number of objects in each image.</p>

<h3 id="detrs-drawbacks">DETR’s drawbacks</h3>

<p>In spite of its intriguing design and commendable
performance, DETR encounters certain challenges. Firstly, it requires
significantly more training epochs to reach convergence compared to
existing object detectors. For instance, when evaluated on the COCO
benchmark <d-cite key="lin_microsoft_2014"></d-cite>, DETR necessitates 500 epochs for
convergence, making it approximately 10 to 20 times slower than Faster
R-CNN <d-cite key="ren_faster_2015"></d-cite>. Secondly, DETR exhibits relatively lower
proficiency in detecting small objects <d-cite key="zhu_deformable_2021"></d-cite>.
Contemporary object detectors typically utilize multi-scale features,
employing high-resolution feature maps for the detection of small
objects. However, employing high-resolution feature maps leads to
impractical complexities for DETR. These aforementioned issues primarily
stem from the deficiency of Transformer components in processing image
feature maps. During initialization, the attention modules distribute
nearly uniform attention weights to all pixels in the feature maps. It
requires many training epochs for the attention weights to learn to
concentrate on sparse meaningful locations. Additionally, the
computation of attention weights in the Transformer encoder is quadratic
in relation to the number of pixels. Consequently, processing
high-resolution feature maps becomes highly computationally and memory
intensive.<br /><br /></p>

<figure id="fig:object-queries">
<center><img src="/assets/img/dl-series/paper-trackformer.png" style="width:66.6%" /></center>
</figure>

<h2 id="3-trackformer-as-multi-object-tracker">3.) Trackformer as Multi-Object Tracker</h2>

<p>Trackformer <d-cite key="meinhardt_trackformer_2022"></d-cite> extends DETR to multi-object
tracking. To be precise, it uses the variant Deformable DETR <d-cite key="zhu_deformable_2021"></d-cite>, presumably because the results were better.
Trackformer not only achieved state of the art results for online
tracking but also presented an end-to-end architecture that solves the
three sub-tasks of track initialization (detection), prediction of next
positions, and matching predictions with detections. Thereby, it
bypasses intermediate layers that are usually present in previous
pipeline designs, similar to how DETR facilitated object detection. The
main idea is depicted by Figure
<a href="#fig:trackformer">3</a>. It is to re-use DETR’s <em>decoded</em> object
queries that have been matched to an actual object in one frame and use
them as additional object queries for the next frame as <em>autoregressive
track queries</em>. Accordingly, we dynamically adjust the transformer
decoder sequence length. The static object queries are responsible for
initializing new tracks and the taken over track queries allow tracking
objects across frames. In contrast do DETR, besides the bounding box
quadruple \(b\) and the object class \(c\), we additionally predict predict
the track ID across frames in an implicit way from by enumerating the
track queries.</p>

<p>With the described approach, we train the model to not only decode
learned object queries into representations that can detect objects but
also to use the decoded queries again as decoder input to detect the
same object when possible. If we match the decoder output from an object
query in frame \(t-1\), it is re-used as an additional track query as
decoder input for frame \(t\). If its output is matched again, we assume
that both detections belong to the same object with ID \(k\).</p>

<p><br /><br /></p>

<figure id="fig:trackformer">
<center><img src="/assets/img/dl-series/4-trackformer.png" style="width:100.0%" /></center>
</figure>
<p><b>Figure 3.Trackformer architecture.</b> Trackformer extends DETR to
tracking on video data by feeding the decoded object queries that are
matched to actual objects as additional <em>autoregressive tracking
queries</em> (colored detection squares) next to the object queries for
the next image into the transformer decoder (dark blue). The decoder
processes the set of <span class="math inline"><em>N</em><sub>track</sub> + <em>N</em><sub>object</sub></span>
queries to further track or remove existing tracks (light blue) and to
initialize new tracks (purple). Image source: <d-cite key="meinhardt_trackformer_2022"></d-cite>.</p>

<h3 id="set-prediction-loss">Set prediction loss</h3>

<p>We now want to formulate a loss that allows the
model to learn the bipartite matching \(j=\pi (i)\) between target objects
\(y_i\) to the set of both object and track query predictions \(\hat{y}_j\).
For this purpose, let us denote the subset of target track identities at
frame \(t\) with \(K_t \subset K\). This is different from DETR as \(K\)
contains all object identities for all images in the video sequence.
These object or track identities can be present in multiple frames,
i.e. they can intersect from frame to frame. Trackformer takes three
steps to associate queries with targets. The last step corresponds to
DETR’s method. The steps to obtain the mapping of predicted detections
to target detections \(\hat{\sigma}\) for one frame are the following:
first, we match \(K_{\text{track}} = K_{t-1} \cap K_t\) (target objects in
the current frame that were also present in the previous frame) by track
identity. This means, we associate these targets with the output from
the previous query. Second, we match
\(K_{\text{leaving}} = K_{t-1} \setminus K_t\) (objects leaving the scene
between two frames) with background class \(\emptyset\). And third, we
match \(K_{\text{init}} = K_{t} \setminus K_{t-1}\) (objects entering the
scene) with the \(N_{\text{object}}\) object queries by minimum cost
mapping based on object class and bounding box similarity the same ways
as DETR assigned its targets to object queries.</p>

<p>Output embeddings which were not matched, i.e. 1) proposals with worse
class and bounding box similarity than others or 2) track queries
without corresponding ground truth object, are assigned to background
class \(\emptyset\).</p>

<p>With this order, we prioritize matching track queries from the last
frame even if object queries from the current frame yield more similar
bounding boxes and classes. This is necessary because, in order to
assign the same object ID to detections in multiple frames, we have to
train the object queries to not only initialize a track after on pass
through the decoder but also to detect the same object after two passes
through the decoder with given the respective interactions from the
image encodings.</p>

<p>The final set prediction loss for one frame is computed over all
\(N=N_{\text{object}}+N_{\text{track}}\) model outputs. Because
\(K_{t-1} \setminus K_t\) (objects that left the scene) are not contained
in the current-frame permutation, we write the loss as</p>

<p>\(\begin{align}
\text{L}_{\text{MOT}}(y,\hat{y},\pi)=\sum_{i=1}^N \text{L}_{\text{query}}(y,\hat{y},\pi).
\end{align}\)
Further, we define the loss per query and differentiate two categories.
First, we have the object query loss \(L_0\) for outputs from unmatched
embeddings. And second, we have the track query loss \(L_1\) for outputs
from matched embeddings that will be overtaken to the next time period
as track queries. Formally, the query loss is given by</p>

\[\begin{align}\text{L}_{\text{query}}=
    \begin{cases}
        \phantom{.}L_0 &amp;= -\lambda_{\text{cls}} \log \hat{p}_i (\emptyset) \quad \phantom{..................}\text{if } i \notin \pi \\
        \phantom{.}L_1 &amp;= -\lambda_{\text{cls}} \log \hat{p}_i (c_{\pi=i}) + \text{L}_{\text{box}}(b_{\pi=i},\hat{b}_i) \quad \text{if } i \in \pi.
    \end{cases}\end{align}\]

<p>The expression captures two features: first, \(L_1\) rewards track queries
that find the right bounding box for objects that are still present on
the current frame and it rewards object queries with similar outputs to
new objects. Second, \(L_0\) not only rewards track queries that predict
the background class if their object has left the scene but also object
queries that predict the background class if their bounding box
prediction is off. The discussed details about track and object queries,
and the matching rules with examples for assigned bounding boxes and
losses are depicted in
<a href="#fig:trackformer_query_t0t1">4</a>.</p>

<p><br /><br /></p>

<figure id="fig:trackformer_query_t0t1">
<center><img src="/assets/img/dl-series/4_trackformer_t1t2.png" style="width:100.0%" /></center>
</figure>
<p><b>Figure 4. Training track and object queries.</b> The black boxes are
ground truth detections and the colorful, annotated boxes are
predictions from the respective output embedding. Embeddings that do not
spawn a box predict a class score smaller than the threshold. In t=0,
the most similar boxes (green and red) are matched with the two target
boxes according to matching step 2: <span class="math inline"><em>K</em><sub>init</sub></span> (symbolized by
“<span class="math inline">/</span>”). For these boxes, we compute <span class="math inline"><em>L</em><sub>1</sub></span> based on boxes and
class scores, and for the unmatched boxes, we compute <span class="math inline"><em>L</em><sub>0</sub></span> solely based on the
class scores. Then we update the model parameters accordingly. The
matched output embeddings are taken over as additional input embeddings,
carrying the object IDs from the objects on the previous image. They are
matched with priority according to matching step 1: <span class="math inline"><em>K</em><sub>track</sub></span> (symbolized by
“<span class="math inline">∩</span>”). Embedding 2’ is matched although
the bounding boxes from output embedding <span class="math inline"><em>a</em>′</span> is more similar to the target. We
feed embeddings <span class="math inline">1′</span> and <span class="math inline">2′</span> to the decoder in period <span class="math inline"><em>t</em> = 2</span> (see Figure 5).</p>

<h3 id="track-query-re-identification">Track query re-identification</h3>

<p>What happens if objects are occluded
or re-enter the scene? To deal with such cases, we keep feeding
previously removed track queries for a <em>patience window</em> of
\(T_{track-reid}\) frames into the decoder. During this window,
predictions from track ids are only considered if a classification score
higher than \(\sigma_{track-reid}\) is reached.</p>

<figure id="fig:trackformer_query_t2t3">
<center><img src="/assets/img/dl-series/4_trackformer_t3t4.png" style="width:100.0%" /></center>
</figure>
<p><b>Figure 5. Training track queries with re-identification. Make sure to
compare this Figure to Figure 4.</b>
From <span class="math inline"><em>t</em> = 1</span>, we additionally
feed output embeddings <span class="math inline">1′</span> and <span class="math inline">2′</span>, carrying the object identities for
pedestrian 1 (green) and pedestrian 2 (blue) to the encoder. However,
pedestrian 2 has left the scene between <span class="math inline"><em>t</em> = 1</span> and <span class="math inline"><em>t</em> = 2</span> and pedestrian 1 is occluded
by a news pedestrian with ID 3. Here, we depict the case where we have
no ground truth annotation for the occluded pedestrian. Note that the
prediction from embedding <span class="math inline">1′</span> is quite
reasonable given that pedestrian 1 is almost invisible. In contrast,
embedding <span class="math inline">2′</span> keeps predicting a
bounding box in the close to the upper right corner independent of the
image. Since there are no detections with IDs previously matched to 1 or
2, we cannot apply matching step 1: <span class="math inline"><em>K</em><sub>track</sub></span> (symbolized by
“<span class="math inline">∩</span>”) to output embeddings <span class="math inline">1″</span> and <span class="math inline">2″</span>.
Instead, we match both outputs with the background class according to
step 2 <span class="math inline"><em>K</em><sub>leaving</sub></span>
(symbolized by “<span class="math inline">∖</span>”). Assuming the green
pedestrian would be annotated, however, we would have apply step 1:
<span class="math inline"><em>K</em><sub>track</sub></span> (symbolized
by “<span class="math inline">∩</span>”) to embedding <span class="math inline">1″</span> instead. To track the new pedestrian with
ID 3, we apply matching step 3: <span class="math inline"><em>K</em><sub>init</sub></span> (symbolized by
“<span class="math inline">/</span>”) to embedding <span class="math inline">3</span> and update the model according to the
respective losses. Since we keep unmatched embeddings with patience, we
take over embeddings <span class="math inline">1″</span> and <span class="math inline">2″</span> to the next frame in addition to embedding 3. This allows the model to re-identify object 1 (green) in period <span class="math inline"><em>t</em> = 3</span>.</p>

<h3 id="results">Results</h3>

<p>Trackformer achieved state-of-the-art performance in
multi-object tracking on MOT17 and MOT20 datasets. It is important to pre-train the model on large tracking datasets, to use track augmentations, and probably also to use pre-trained detection weights for tracking-specific pre-training.<br /><br /></p>

<figure id="fig:object-queries">
<center><img src="/assets/img/dl-series/paper-deformable-detr.png" style="width:66.6%" /></center>
</figure>

<h2 id="4-extra-deformable-detr-as-more-efficient-detector">4.) Extra. Deformable DETR as More Efficient Detector</h2>

<p>In order to address DETR’s disadvantages, <d-cite key="zhu_deformable_2021"></d-cite> introduces a
deformable attention module as replacement for the conventional
attention module in DETR’s transformer model. Drawing inspiration from
deformable convolution <d-cite key="dai_deformable_2017"></d-cite> <d-cite key="zhu_deformable_2021"></d-cite>, the
deformable attention module focuses its attention solely on a limited
set of key sampling points surrounding a reference point. By allocating
a fixed number of keys per query, <d-cite key="zhu_deformable_2021"></d-cite> can alleviate the
problems associated with convergence and feature spatial resolution by
decreasing the transformer’s complexity as a function of the image
dimension to a sub-quadratic level. Furthermore, <d-cite key="zhu_deformable_2021"></d-cite>
use this module to aggregate multiple feature maps of different
resolution taken from the CNN backbone in the encoder to "multi-scale"
feature maps and for the object queries to aggregate the relevant
information from these maps for the detection predictions. The design is
inspired by the finding that multi-scale feature maps are crucial for
teaching image transformers to effectively represent objects depicted at
strongly distinct scales <d-cite key="lin_feature_2017"></d-cite>.
Figure <a href="#fig:deformable-detr">6</a> depicts the complete Deformable DETR
architecture.<br /></p>

<figure id="fig:deformable-detr">
<center><img src="/assets/img/dl-series/4d-deformable-attention.png" style="width:100.0%" /></center>
</figure>
<p><b>Figure 6. High-level Deformable DETR architecture.</b> We extract three
feature maps at different resolution levels from the CNN backbone. In
the encoder, we aggregate the information from all multi-scale feature
maps with deformable self-attention, attending only to a learned sample
of important locations around a learned reference point from every
feature map for each feature map (deformable attention). This results in
three encoder feature maps of the same dimensions. In the decoder, the
learned object queries extract features from queries and values from
themselves with conventional self-attention and from the keys from the
encoder feature maps with deformable cross-attention. Again, for each
object query, the learned reference point and a set of learned offsets
is used to only query a set of keys from the encoder. With that, we
replace the self-attention modules in the encoder and the and
cross-attention modules in the decoder with 4 heads of the deformable
attention module and learn from feature maps at different resolutions.
This makes the transformer’s complexity as a function of pixel number
sub-quadratic and allows the model to better learn objects at strongly
distinct scales. Image source: <d-cite key="zhu_deformable_2021"></d-cite>.<br /><br /></p>

<h3 id="multi-head-attention-revisited">Multi-Head Attention Revisited</h3>

<p>When provided with a query element
(e.g., a target word in the output sentence) and a set of key elements
(e.g., source words in the input sentence), the multi-head attention
module aggregates the key contents based on attention weights, which
gauge the compatibility of query-key pairs. In order to enable the model
to focus on diverse representation subspaces and positions, the outputs
of various attention heads are combined linearly using adjustable
weights.</p>

<p>Let \(q \in \Omega_q\) denote an index for the query element represented
by feature \(z_q \in \mathbb{R}^C\), and let \(k \in \Omega_k\) denote an
index for the key element represented by feature \(x_k \in \mathbb{R}^C\)
with feature dimension \(C\) and set of query and key elements \(\Omega_q\)
and \(\Omega_k\), respectively. We compute the multi-head attention
feature for query index \(q\) with</p>

\[\begin{align}\text{MultiHeadAttn}(z_q, x) = \sum_{m=1}^M W_m [\sum_{k \in \Omega_k} A_{mqk} \cdot W_m^{T} x_k],\end{align}\]

<p>where \(m\) is index the attention head and
\(W_m \in \mathbb{R}^{C \times C_v}\) are learned weights with
\(C_v = C/M\). We normalize the attention weights
\(A_{mqk} \propto \text{exp}\{\frac{z_q^T U_m^T V_m x_k}{\sqrt{C_v}}\}\)
with learned weights \(U_m, V_m \in \mathbb{R}^{C \times C_v}\) to
\(\sum_{k \in \Omega_k} A_{mqk}=1\). In order to clarify distinct spatial
positions, the representation features, denoted as \(z_q\) and \(x_k\), are
formed by concatenating or summing the element contents with positional
embeddings.</p>

<h3 id="deformable-attention">Deformable Attention</h3>

<p>The main challenge in applying Transformer
attention to image feature maps is that it considers all potential
spatial locations. To overcome this limitation, <d-cite key="zhu_deformable_2021"></d-cite>
propose a deformable attention module. It selectively focuses on a small
number of key sampling points around a reference point, irrespective of
the spatial dimensions of the feature maps. By employing a small number
of keys per query, it can address the issues of convergence and feature
spatial resolution. In contrast to the previous notation, let
\(x \in \mathbb{R}^{C \times H \times W}\) denote an input feature map and
let \(q\) denote the index for a query element with feature \(z_q\) and a
2-d reference point \(p_q\). We calculate the deformable attention feature
with</p>

\[\begin{align}\text{DeformAttn}(z_q, p_q, x) = \sum_{m=1}^M W_m [\sum_{k \in \Omega_k} A_{mqk} \cdot W_m^{T} x_k (p_q + \Delta p_{mqk})],\end{align}\]

<p>where \(k\) indexed the sampled keys and \(K   \ll  HW\) is the total number
of sampled keys. Futher, \(\Delta p_{mqk} \in \mathbb{R}^2\) denotes the
sampling offset and \(A_{mkq}\) the attention weights of the
\(k^{\text{th}}\) sampling point in the \(m^{\text{th}}\) attention head,
respectively. The attention weights are normalized to own over the
sample keys. We feed query feature \(z_q\) to a \(3MK\)-channel linear
projection operator. The inital \(2MK\) channels encode the sampling
offsets \(Delta p_{mkq}\), and the latter \(MK\) channels are used as input
to the softmax function to compute the attention weights \(A_mqk\).</p>

<h3 id="multi-scale-deformable-attention">Multi-scale Deformable Attention</h3>

<p>We extend the deformable attention
module to multiple differently-scaled feature maps with a few small
changes. Let \(\{x^l\}_{l=1}^L\) denote the set of differently scaled
feature maps, where \(x^l \in \mathbb{R}^{C \times H_l \times W_l}\) and
let \(\hat{p}_q \in [0,1]^2\) denote the normalized reference points for
each query element element \(q\). Then, we calculate the multi-scale
deformable attention with</p>

\[\begin{gathered}
\text{MSDeformAttn}(z_q, \hat{p}_q, \{x^l\}_{l=1}^L) = \\
\sum_{m=1}^M W_m [\sum_{l=1}^L \sum_{k \in \Omega_k} A_{mlqk} \cdot W_m^{T} x^l (\phi_l (\hat{p}_q ) + \Delta p_{mlqk})],
\end{gathered}\]

<p>where \(l\) indexes the input feature and we expand the normalized
attention weights and the reference point by this dimension.
\(\hat{p}_q  \in [0,1]^2\) are also normalized coordinates with \((0,0)\)
and \((1,1)\) as top-left and bottom-right coordinates, respectively.
Function \(\phi_l(\hat{p}_q )\) is the inverse of the normalization
function and maps \(\hat{p}_q\) back to the respective feature map
coordinates. In constrast to the singlescale deformable attention
module, we sample \(LK\) feature map points instead of \(K\) points and
interact the different feature maps with each other.</p>

<h3 id="results-1">Results</h3>

<p>On the COCO benchmark for object detection, Deformable DETR
outperforms DETR, particularly in detecting small objects, while
requiring only about one-tenth of the training epochs.</p>]]></content><author><name>Tobias Stenzel</name></author><category term="applications" /><category term="transformer" /><category term="MOT" /><category term="multiple-object-tracking" /><summary type="html"><![CDATA[A lower-level explanation of the paper Multi-Object Tracking with Transformers (Meinhardt et al., 2022) including DETR (Carion et al., 2020) and Deformable DETR (Zhu et al., 2020). 🎥]]></summary></entry><entry><title type="html">7. Recurrent Neural Networks</title><link href="https://www.tobiasstenzel.com/blog/2023/dl-rnn/" rel="alternate" type="text/html" title="7. Recurrent Neural Networks" /><published>2023-03-07T00:00:00+00:00</published><updated>2023-03-07T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/dl-rnn</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/dl-rnn/"><![CDATA[<h1 id="recurrent-neural-networks">Recurrent Neural Networks</h1>

<p>Many tasks require input or output spaces that contain sequences. For example, in translation programs, words are often encoded as sequences of one-hot vectors. Each one-hot vector has a 1 at the position of an integer mapped to a word in a fixed vocabulary.
A simple recurrent neural network (RNN) processes a sequence of vectors \(\{x_1, \dots, x_T\}\) using the recurrence formula:</p>

\[\begin{align}
h_t = f_\theta(h_{t-1}, x_t).
\end{align}\]

<ul>
  <li>\(f\) is a function (detailed below) that uses the same parameters \(\theta\) at every time step. This implies parameter sharing and introduces an inductive bias for modeling sequences.</li>
  <li>We can interpret the hidden vector \(h_t\) as a summary of all previous \(x\)-vectors.</li>
  <li>A common initialization is \(h_0 = \vec{0}\).</li>
</ul>

<p>We can define \(f\) according to <strong>three criteria</strong>:</p>

<ul>
  <li>
    <p><strong>1. Input–Output Space</strong><br />
  Depending on the task, we may want to handle “one-to-many,” “many-to-one,” or “many-to-many” mappings between the inputs and outputs.</p>
  </li>
  <li><strong>2. Order of Processing</strong><br />
  The nature of the data may require:
    <ul>
      <li>Predicting \(y_t\) immediately after seeing \(x_t\).</li>
      <li>Predicting the entire output sequence \(\{y_1, \dots, y_T\}\) only after reading the full input sequence \(\{x_1, \dots, x_T\}\) in one shot.</li>
      <li>Predicting the output sequence bit by bit after reading the complete input in order to also include the output elements that where generated so far as information for the next prediction.</li>
    </ul>
  </li>
  <li><strong>3. Handling Long Sequences</strong><br />
  As sequences grow longer, we encounter <strong>drawbacks</strong> (e.g., vanishing/exploding gradients).  We will address this point later in the section.</li>
</ul>

<p>To illustrate how RNNs work, let us look at two specific examples. The first example is a vanilla RNN for predicting the next character in a text prompt. The second example is a token level encoder-decoder RNN for translation that can also work well with longer text. We will prepare this example with a longer explanation of the encoder-decoder architecture in general. Understanding this concept is crucial for understanding modern transformer based models. Finally, we look at the backward pass of the vanilla RNN to understand fundamental problems of RNNs in a deeper way.</p>

<blockquote>
  <p><strong><em>Tokenization</em></strong></p>

  <p>The examples here use sequences of characters or words. However, in realistic applications text is mapped to a sequence of elements that are instances of a fixed vocabulary. This mapping is achieved by neural networks called tokenizers. Tokens can be anything from words, subword segments, or individual characters.</p>
</blockquote>

<h2 id="vanilla-recurrent-neural-networks">Vanilla Recurrent Neural Networks</h2>

<p>Vanilla RNNs use a simple recurrence defined by:</p>

\[\begin{align}
h_t &amp;= \phi \big ( W\begin{bmatrix}
       x_{t} \\
       h_{t-1} 
     \end{bmatrix}
     \big ).
\label{eq:vanilla-rnn}
\end{align}\]

<ol>
  <li>We concatenate the current input \(x_t\) and the previous hidden state \(h_{t-1}\).</li>
  <li>We transform them linearly (by multiplying with \(W\)).</li>
  <li>We apply a non-linear activation \(\phi\).</li>
</ol>

<p>In vector form, this is equivalent to:</p>

<p>\(\)\begin{align}
h_t = \phi \big ( W_{xh}x_t + W_{hh}h_{t-1} \big ).
\(\)\end{align}</p>

<ul>
  <li>\(W_{xh}\) and \(W_{hh}\) are concatenated horizontally to form \(W\).</li>
  <li>If \(x_t \in \mathbb{R}^D\) and \(h_t \in \mathbb{R}^H\), then<br />
\(\,W_{xh} \in \mathbb{R}^{H \times D}\)<br />
\(\,W_{hh} \in \mathbb{R}^{H \times H}\)<br />
\(\,W \in \mathbb{R}^{\,H \times (D+H)}.\)</li>
</ul>

<p>Effectively, a vanilla RNN models the new hidden state \(h_t\) at each time step as a linear function of the previous hidden state \(h_{t-1}\), and the current input \(x_t\). Both inputs are passed through a non-linearity \(\phi\).</p>

<p><strong>Example: Character-level Language Model</strong></p>

<p>In a classification task—e.g., predicting the next character in a text prompt—one can apply a Softmax to a linear transformation of the hidden state at every time step:</p>

<p>\(\)\begin{align}
o_t = W_{ho}\,h_t,
\quad
\hat{y}_t = \text{Softmax}(o_t),
\end{align}\(\)</p>

<p>to predict the next character’s one-hot encoding. The linear transformation projects the hidden state into the alphabet space and the softmax function converts the real vector into the unit space. Then, we can use the character with the highest number as prediction. This is illustrated by Figure
<a href="#fig:vanilla-rnn">5</a>.</p>

<figure id="fig:vanilla-rnn">
<center><img src="/assets/img/dl-series/2f-vanilla-rnn.png" style="width:95%" /></center>
</figure>
<p><b>Figure 5. Vanilla RNN as character-level language model.</b> The left side
shows the <em>unrolled RNN</em>. The vocabulary has four characters and
the training sequence is “hello”. Each letter is represented by a 1-hot
encoding (yellow) and the RNN predicts the encodings of the next letter
(green) at each time step. The RNN has a hidden state with three
dimensions (red). The output has four dimensions. The dimensions are the
logits for the next character. They are the softmax of a linear
transformation of the hidden states. During supervised learning, the
model will be trained to increase (decrease) the logits of the correct
(false) characters. The right side shows the <em>rolled-up RNN</em>. The
graph has a cycle that shows that the same hidden states are shared
across time and that the architecture is the same for each
step.</p>

<h2 id="the-encoderdecoder-architecture">The Encoder–Decoder Architecture</h2>

<p>An encoder–decoder model is build for applications that involve generating a sequence of potentially different length from another sequence. One such application is translation or question answering. In its most general case, the model transforms an input sequence
\(\begin{align}
  X = \{x_1, x_2, \dots, x_{T_x}\}
\end{align}\)
into an output sequence
\(\begin{align}
  \hat{Y} = \{\hat{y}_1, \hat{y}_2, \dots, \hat{y}_{T_y}\}.
\end{align}\)</p>

<p>It is structured into two main components:</p>

<p><strong>Encoder</strong><br />
Produces an internal representation \(R\) of \(X\). Formally:</p>

<p>\(\begin{align}
  R = \mathrm{Encoder}(X).
\end{align}\)</p>
<ul>
  <li>In some architectures, \(R\) is a <em>single vector</em> (often called a context).</li>
  <li>In others (e.g., with attention), \(R\) is a <em>sequence of states</em> \(\{h_1,\dots,h_{T_x}\}\).</li>
  <li>Or it may be multiple stacked layers of representations (e.g., in a Transformer).</li>
</ul>

<p><strong>Decoder</strong><br />
Generates each output element \(\hat{y}_t\) (for \(t=1,\dots,T_y\)) one at a time, with an autoregressive dependence on its own partial output and on the encoder representation \(R\). Concretely, we can think of the decoder in three submodules:</p>

<ul>
  <li>
    <p><strong>(a) Self-encoding of partial outputs</strong></p>

    <p>The decoder keeps a hidden state \(s_t\) that “remembers” what it has generated so far:</p>

\[\begin{align}
    s_t^{(\mathrm{self})} = \mathrm{SelfEnc}\bigl(s_{t-1}^{(\mathrm{self})},\, \hat{y}_{t-1}\bigr).
  \end{align}\]

    <p>This submodule accumulates the history \(\{\hat{y}_1, \dots, \hat{y}_{t-1}\}\) into \(s_t^{(\mathrm{self})}\).</p>
  </li>
  <li><strong>(b) Cross-encoding (referencing the encoder)</strong>
The decoder also needs to incorporate the <strong>encoder’s representation</strong> \(R\). We denote this step by:
\(\begin{align}
    s_t^{(\mathrm{cross})} = \mathrm{CrossEnc}\bigl(s_t^{(\mathrm{self})},\, R\bigr).
  \end{align}\)
    <ul>
      <li>In a simple setting, \(\mathrm{CrossEnc}\) might just copy or concatenate \(s_t^{(\mathrm{self})}\) with \(R\).</li>
      <li>In an attention-based setting, \(\mathrm{CrossEnc}\) might compute a new context \(\tilde{c}_t\) from \(\{h_1,\dots,h_{T_x}\}\) and combine that with \(s_t^{(\mathrm{self})}\).</li>
    </ul>
  </li>
  <li><strong>(c) Output module</strong>
Finally, the decoder uses a function \(\mathrm{Output}\) to map \(s_t^{(\mathrm{cross})}\) to a distribution over possible next tokens:
\(\begin{align}
    \hat{y}_t = \mathrm{Output}\bigl(s_t^{(\mathrm{cross})}\bigr).
  \end{align}\)
  Typically, \(\mathrm{Output}\) is a linear layer plus a softmax.</li>
</ul>

<p>Altogether, the generic decoder step might look like:</p>

\[\begin{align}
  \begin{aligned}
    s_t^{(\mathrm{self})} &amp;= \mathrm{SelfEnc}\bigl(s_{t-1}^{(\mathrm{self})},\, \hat{y}_{t-1}\bigr),\\
    s_t^{(\mathrm{cross})} &amp;= \mathrm{CrossEnc}\bigl(s_t^{(\mathrm{self})},\, R\bigr),\\
    \hat{y}_t &amp;= \mathrm{Output}\bigl(s_t^{(\mathrm{cross})}\bigr).
  \end{aligned}
\end{align}\]

<p>where \(\hat{y}_{t-1}\) is the decoder’s previous prediction.</p>

<p>In many actual implementations, submodules (a) and (b) may be fused into a single recurrent or feed-forward block. The above separation is conceptual, highlighting that the decoder is simultaneously <em>encoding the partial outputs</em> and <em>referencing the encoder representation</em>.</p>

<blockquote>
  <p><strong>Dealing with Sequences of Different Lengths</strong><br />
To allow different input and output lengths, we introduce two special tokens:</p>
  <ul>
    <li>Start-of-sentence: \(\langle sos \rangle\)</li>
    <li>End-of-sentence: \(\langle eos \rangle\)</li>
  </ul>

  <p>The encoder stops reading the input when it encounters \(\langle eos \rangle\). The decoder starts generating with \(\langle sos \rangle\) as \(y_1\) and stops when it generates \(\langle eos \rangle\).</p>
</blockquote>

<h2 id="encoder-decoder-rnns">Encoder-Decoder RNNs</h2>

<p>We now specialize this generic view to an RNN-based encoder–decoder. We will (i) specify a fairly general RNN recurrence form, and then (ii) show how it can reduce to simple linear transformations with a standard nonlinearity.</p>

<h3 id="general-rnn-recurrence-for-encoder-and-decoder">General RNN Recurrence for Encoder and Decoder</h3>

<ul>
  <li>
    <p><strong>Encoder RNN</strong>:<br />
\(\begin{align}
  h_t = f_{\mathrm{enc}}(h_{t-1},\, x_t), 
  \quad
  t=1,\dots,T_x,
\end{align}\)
with some initialization \(h_0\). We take the final state \(h_{T_x}\) as
\(\begin{align}
  R = h_{T_x},
\end{align}\)
a single “context vector.”</p>
  </li>
  <li>
    <p><strong>Decoder RNN</strong>: merges <strong>self-encoding</strong> and <strong>cross-encoding</strong> into a single update:
\(\begin{align}
  s_t = f_{\mathrm{dec}}\bigl(s_{t-1},\, \hat{y}_{t-1},\, R\bigr),
  \quad
  t=1,\dots,T_y,
\end{align}\)
and then
\(\begin{align}
  \hat{y}_t = \mathrm{Output}\bigl(s_t\bigr).
\end{align}\)
This single function \(f_{\mathrm{dec}}\) effectively does:</p>
    <ol>
      <li>Encode the partial outputs via \(\hat{y}_{t-1}\) and \(s_{t-1}\).</li>
      <li>Cross-encode the encoder’s context \(R\).</li>
    </ol>
  </li>
</ul>

<h3 id="vanilla-encoder-decoder-rnn">Vanilla Encoder-Decoder RNN</h3>

<p>In the simplest “vanilla” RNN, the encoder might be:</p>

<p>\(\begin{align}
  h_t = \phi\Bigl(
    W_{\mathrm{enc}}^{(hh)}\,h_{t-1}
    \;+\;
    W_{\mathrm{enc}}^{(xh)}\,x_t
  \Bigr),
\end{align}\)
where \(\phi(\cdot)\) is a nonlinearity (e.g. \(\tanh\)). Then we set</p>

\[\begin{align}
  R = h_{T_x}.
\end{align}\]

<p>For the decoder, we can define:</p>

<ol>
  <li>
    <p><strong>Initialization</strong>: 
\(\begin{align}
  s_0 = R = h_{T_x}.
\end{align}\)</p>
  </li>
  <li>
    <p><strong>Recurrent update</strong>:
\(\begin{align}
  s_t = \phi\Bigl(
    W_{\mathrm{dec}}^{(ss)}\,s_{t-1}
    \;+\;
    W_{\mathrm{dec}}^{(ys)}\,\hat{y}_{t-1}
    \;+\;
    W_{\mathrm{dec}}^{(cs)}\,R
  \Bigr).
\end{align}\)
This merges the partial output (via \(\hat{y}_{t-1}\)) with the context \(R\).</p>
  </li>
  <li>
    <p><strong>Output module</strong>:
\(\begin{align}
  \hat{y}_t = \mathrm{Softmax}\Bigl(
    W_{\mathrm{dec}}^{(so)}\,s_t
  \Bigr).
\end{align}\)</p>
  </li>
</ol>

<p>In this concrete instantiation, submodules (a) and (b) from the generic description appear in a single formula for \(s_t\). The <strong>output module</strong> is simply a linear (via \(W_{\mathrm{dec}}^{(so)}\)) plus softmax layer.</p>

<figure id="fig:encoder-decoder-rnn">
<center><img src="/assets/img/dl-series/2g-encoder-decoder-rnn.png" style="width:95%" /></center>
</figure>
<p><b>Figure 6. Encoder-Decoder RNN as word-level language model.</b> The input
language has two and the output language has three words. Every word,
the start and the end of a sentence are represented by a one-hot
encoding (yellow) and the RNN predicts the encodings of the translation
of the input sentence. The encoder RNN has a hidden state (green) that
is updated by one word step-by-step to produce the final context vector
(purple). The decoder RNN takes linear functions of the final encoding
and the start embedding to compute the hidden states for the next output
embedding (blue). The one-hot encoding of the output words (red) are a
softmax of a linar function of these hidden states. The decoder RNN
iterates the prediction until it returns the end token. With this
architecture, we can predict sequences of arbitrary length using the
embedded information of a whole input sequence.<br />
<br /></p>

<p>The simplicity of the RNN’s formulation has two drawbacks.</p>

<ol>
  <li>The connections between inputs and hidden states through linear layers and
  element-wise non-linearities is not flexible enough for some tasks.</li>
  <li>The recurrent graph structure leads to problematic dynamics
  during the backward pass.</li>
</ol>

<h2 id="the-exploding-and-vanishing-gradient-problem">The Exploding and Vanishing Gradient Problem</h2>

<p>We now explore the second problem—vanishing and exploding gradients—more formally. Consider the vanilla RNN’s loss with respect to the weight matrix in Equation \(\ref{eq:vanilla-rnn}\). Its derivative can be written as:</p>

<p>\(\)
\begin{align}
  \frac{\partial L}{\partial W}
  = \sum_{t=1}^T \sum_{k=1}^{t+1}
    \frac{\partial L_{t+1}}{\partial o_{t+1}}
    \frac{\partial o_{t+1}}{\partial h_{t+1}}
    \frac{\partial h_{t+1}}{\partial h_{k}}
    \frac{\partial h_{k}}{\partial W}.
\label{eq:rnn-derivative}
\end{align}
\(\)</p>

<p>A key factor here is the derivative of the hidden state at time \(t+1\) with respect to a <em>past</em> hidden state \(h_k\). From the chain rule, it follows a recursive product:</p>

<p>\(\)
\begin{align}
\label{eq:rnn-derivative-recursion}
  \frac{\partial h_{t+1}}{\partial h_k}
  = \prod_{j=k}^t
    \frac{\partial h_{j+1}}{\partial h_j}
  = \frac{\partial h_{t+1}}{\partial h_t}
    \cdot
    \frac{\partial h_t}{\partial h_{t-1}}
    \cdots
    \frac{\partial h_{k+1}}{\partial h_k}
  = \prod_{j=k}^t \mathrm{diag} \bigl(
      W_{hh} \,\phi’\bigl(W[x_{j+1} ;\,h_j]\bigr)
    \bigr).
\end{align}
\(\)</p>

<p>In essence, \(\frac{\partial h_{t+1}}{\partial h_k}\) is the product of Jacobians from time \(t\) down to \(k\). Because an RNN processes sequences, this product can become very large if \((t-k)\) is large (i.e., if the sequence is long).</p>

<h3 id="vanishing-and-exploding-effects">Vanishing and Exploding Effects</h3>

<p>To see why gradients may vanish or explode, assume for simplicity that each local Jacobian \(\tfrac{\partial h_{t+1}}{\partial h_t}\) is constant over time. Let us denote this constant Jacobian by \(J\). Suppose we decompose \(J\) via its eigendecomposition, yielding eigenvalues \(\lambda_1, \lambda_2, \dots, \lambda_n\) and corresponding eigenvectors \(v_1, v_2, \dots, v_n\), with</p>

<p>\(\)
\begin{align}
|\lambda_1| \geq |\lambda_2| \geq \cdots \geq |\lambda_n|.
\end{align}
\(\)</p>

<ul>
  <li><strong>Exploding Gradients</strong>: If \(\lvert\lambda_1\rvert &gt; 1\), then repeated multiplications by \(J\) cause gradients to grow exponentially, leading to exploding gradients.</li>
  <li><strong>Vanishing Gradients</strong>: Conversely, if \(\lvert\lambda_1\rvert &lt; 1\), the gradient norms shrink exponentially as they propagate back through time, eventually becoming negligibly small (vanishing).</li>
</ul>

<p>In practice, vanishing gradients are especially common when activation derivatives \(\phi'\) stay below 1 in magnitude (as with sigmoid or tanh). As a result, the earliest hidden states in the sequence receive almost no gradient information, hurting the model’s ability to capture long-range dependencies and effectively limiting its “long-term memory.”</p>

<h3 id="mitigation-strategies">Mitigation Strategies</h3>

<p>Various methods aim to alleviate exploding or vanishing gradients. Examples include:</p>

<ul>
  <li>Regularization (e.g., gradient clipping)</li>
  <li>Careful weight initialization</li>
  <li>ReLU activations</li>
  <li>Gated RNNs (LSTM or GRU), which introduce gating mechanisms akin to more sophisticated skip connections from Chapter
<a href="https://www.tobiasstenzel.com/blog/2023/dl-cnn/#skip-connections">5</a></li>
  <li>Transformer architectures, which rely on attention instead of purely recurrent connections and thus suffer less from vanishing gradients while providing more expressive long-range interactions.</li>
</ul>

<p>Today, the most widely used approach in many sequence tasks is the <strong>transformer</strong>, which not only avoids many vanishing-gradient issues but also offers more flexible and powerful modeling of contextual information.</p>

<hr />

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = &quot;Deep Learning Series&quot;,
  author  = &quot;Stenzel, Tobias&quot;,
  year    = &quot;2023&quot;,
  url     = &quot;https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>]]></content><author><name>Tobias Stenzel</name></author><category term="dl-fundamentals" /><category term="sneural-net-archetype" /><category term="backprop" /><category term="exploding-vanishing-gradients" /><summary type="html"><![CDATA[🧱 The encoder-decoder RNN is quite useful for understanding the transformer.]]></summary></entry><entry><title type="html">6. Convolutional Neural Networks</title><link href="https://www.tobiasstenzel.com/blog/2023/dl-cnn/" rel="alternate" type="text/html" title="6. Convolutional Neural Networks" /><published>2023-03-06T00:00:00+00:00</published><updated>2023-03-06T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/dl-cnn</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/dl-cnn/"><![CDATA[<h2 id="sec:cnn">Convolutional Neural Networks</h2>

<p>A Convolutional Neural Networks (CNN) <d-cite key="lecun_gradient-based_1998"></d-cite> is
type of neural network architecture that is specifically designed for
input data where the tabular arrangement includes a spatial meaning.
Moreover, the features of one example are not stored in a vector of
different property values but \(x\) is a multi-dimensional array (i.e. a
<em>tensor</em>). For instance, the data for a color image could have
dimensions \(32 \times 32 \times 3\). The first two dimensions represent
the height and the width of the pixels and the third dimension stands
for three color channels red, green and blue. We can find these spatial
relationships in many data, not only in images. Further examples are
sound spectrogram or sentence data. The main motivation for CNNs is to
reduce the complexity with parameter sharing by focusing on relations
between spatially close data points.</p>

<h3 id="cross-correlation">Cross-correlation</h3>
<p>The core building block of a CNN is the
<em>Convolutional Layer</em> (or CONV layer). The main operation in this layer
is the cross-product between tensors. We denote the cross-product
between two tensors \(I\) and \(W\) as \(I \star W = O\). \(I\) is the input and
\(W\) is the kernel or the filter. If the two matrices are three
dimensional tensors, then the third dimension of both tensors must have
equal length. Let \(i_1, i_2\) and \(f_1, f_2\) be the width and the height
of the input and the filter, respectively. The output, activation map O,
has dimension \((i_1 - f_1 + 1)  \times (i_2 - f_2 + 1)\). Accordingly,
the cross-product for one output element is given by the following
formula :</p>

\[\begin{align}O_{i,j} = (I \star W)_{i,j} = \sum_{m=1}^{k_1} \sum_{n=1}^{k_2}
W_{m,n} \cdot I_{i+m-1,j+n-1} + b.
\label{eq:cross-correlation}\end{align}\]

<p>Figure <a href="#fig:cross-correlation">3</a> illustrates Equation
<a href="#eq:cross-correlation">1</a> without adding the filter-specific
bias \(b\). We fill the activation map row-wise starting from the first
element in the top-left corner. To compute this element, we place the
filter on top of the input so that the feature and the top-left element
of the filter overlap. Then, we calculate the dot product between the
elements at the same position, i.e. we obtain \(O_{1,1}\) by
\(5 \cdot 1+6 \cdot 2+3 \cdot 8+4 \cdot 9=94\). The cross-correlation
operation is equivalent to the convolution operation with horizontally
and vertically flipped filter.</p>

<figure id="fig:cross-correlation">
<center><img src="/assets/img/dl-series/cross-correlation.png" style="width:80%" /></center>
</figure>
<p><b>Figure 3. Cross-correlation between two matrices.</b> Input matrix <span class="math inline"><em>I</em></span> has shape 4 <span class="math inline">×</span> 4 and kernel matrix W has 3 <span class="math inline">×</span> 3. The colored areas in I show the
receptive field for each output in <span class="math inline"><em>O</em></span>. The matrix elements are numbered
by their matrix indices.</p>

<h3 id="concrete-example">Concrete example</h3>
<p>Let us continue with another example in three
dimensions. Our input I is a \(32 \times 32 \times 3\) tensor that
represents an image with red, green and blue channels. Our filter W is a
\(5 \times 5 \times 3\) tensor. We have one filter channel for each color
channel. Now we <em>convolve</em> this filter by sliding it across the whole
image. With that, we interact the color channels dimensions because we
compute the dot product over three dimensions. The result is an
<em>activation map</em> with dimensions \(28 \times 28\) because we can only
place a \(5 \times 5\) filter only 28 times over a \(32 \times 32\) tensor.
It is common to pad the input with a frame of zeros to control the first
two dimension lengths of the output. For instance, a frame of zeros with
thickness 2 maintains the first two dimension lengths of the input.
Another option is to slide the filter with some stride to reduce the
impact of the relations between close pixels on the filter weights. For
instance, convolving the \(5 \times 5 \times 3\) filter over the
\(32 \times 32 \times 3\) image with no padding and stride 2 results in an
activation map of size \(16 \times 16 \times 3\) instead. Lastly, the CONV
layer does not only use a single filter but a set of filters. E.g., with
a set of seven filter, we obtain the same number of \(16 \times 16\)
activation maps. We stack these activation maps along the third
dimension of the resulting output tensor. Thus, we have transformed a
\(32 \times 32 \times 3\) into a \(16 \times 16 \times 7\) stack of
activation maps. Intuitively, each single filter has the capacity to
detect specific local features in the input tensor that may be of
importance to later layers. The weights in this filter tensor are
parameters that we train with backpropagation.</p>

<h3 id="general-definition">General definition</h3>
<p>A convolutional layer for images that are
represented by a three dimensional input tensor is given by the
following five components:</p>

<ul>
  <li>
    <p><strong>Input:</strong> a tensor \(I\) of size \(W_1 \times H_1 \times D_1\)</p>
  </li>
  <li>
    <p><strong>Hyperparameters:</strong> the number of filters \(K\), the filter’s width
or height \(F\) (assuming both are equal), the stride \(S\) , and the
amount of zero padding, \(P\).</p>
  </li>
  <li>
    <p><strong>Output:</strong> \(D_2\) different activation maps stored in a volume of
size \(W_2 \times H_2 \times D_2\), where \(W_2= (W_1 - F+2P)/S+1\),
\(H_2=(H_1-F+2P)S+1\), and \(D_2=K\).</p>
  </li>
  <li>
    <p><strong>Complexity:</strong> the number of parameters in each filter is
\(F \times F \times D_1\). This gives a total of
\(K \times (F \times F \times D_1) + K\) parameters for the whole
layer. Note that the filter depth always equals the input depth.
Moreover, the last \(K\) represents the bias terms that we add to the
respective filter after each dot product computation with the data.</p>
  </li>
  <li>
    <p><strong>Operation:</strong> Each d-th slice of the output tensor (of size
\(W_2 \times H2\)) is the result of computing the cross-correlation
between the d-th filter over the input tensor with a stride of S and
offsetting the result by d-th bias afterwards.</p>
  </li>
</ul>

<h3 id="parameter-sharing">Parameter sharing</h3>
<p>Cross-correlation slides each filter over the
input with the same weights at every position. As a consequence, the
size of the receptive field, i.e. the set of inputs that impact one
output, is much smaller compared to fully-connected layers (cf. Figure
<a href="https://www.tobiasstenzel.com/blog/2023/dl-fnn/#fig:vanilla_neural_net">2</a>. In particular, a convolutional
layer is a special case of a fully connected layer, where many neurons
have the same (re-arranged) set of weights and where most weights are
set to zero except of a small neighborhood. Hence, the convolutional
layer has much less parameters and is less prone to overfitting. To
illustrate this point, let us consider the example of a
\(128 \times 128 \times 3\) input image that is taken by a convolutional
layer with 32 \(5 \times 5 \times 3\) filters, padding of 2 and a stride
of 1. The output is a \(128 \times 128 \times 32\) volume consisting of
\(524,288\) elements. We compute this volume with only
\(32*5*5*3+32=2,432\) total parameters. In contrast, if this was a fully
connect layer that computes every output element based on its own
specific weights, we would use
\(524,288 * (128 \times 128 \times 3) = 25,769,803,776\) parameters. This
number is not only gigantic but it would also be difficult not to
overfit the data even if we could store the parameters and compute the
result.</p>

<h3 id="pooling-layers">Pooling layers</h3>
<p>Another building block of CNNs are pooling layers.
These layers are used to further reduce overfitting by downsampling the
convolutions output with a fixed scheme and without any parameters.
Specifically, these pooling operations are applied to each activation
map separately and preserve the depth of the output volumes but not
their height and width. As with cross-correlation, we slide the pooling
filter over its input. However, we have to do this for each input
channel separately because the pooling operation has no depth. A common
setting is a \(2 \times 2\) filter with stride 2 where the filter
represents a max operation over four numbers. This filter gives us an
output tensor that is downsampled by \(2 \times 2\) along the first two
dimensions.</p>

<h3 id="backward-pass">Backward pass</h3>
<p>The Jacobian of a convolution layer \(O = I \star W\)
is given by \(I \star J^O\), where \(J^O\) is the Jacobian that contains the
upstream gradients \(\delta^o\) with respect to the activation map
parameters. This is illustrated by Figure <a href="#fig:gradient-cross-correlation">4</a>. In comparison to the
Jacobian for a fully-connected linear layer, the smaller, shared
downstream gradient is only multiplied with the activations of its
adjacent elements from the previous layer. The first derivatives of the
pooling operations average and max are simple. The derivative of the
average with respect to one element is 1 divided by the number of
elements. The derivative of the max is the indicator function of maximum
element’s index.</p>

<figure id="fig:gradient-cross-correlation">
<center><img src="/assets/img/dl-series/gradient-cross-correlation.png" style="width:80%" /></center>
</figure>
<p><b>Figure 4. Backward pass through cross-correlation.</b> The Jacobian <span class="math inline"><em>J</em><sup><em>W</em></sup></span> for the
weight parameters of the cross-correlation <span class="math inline"><em>O</em> = <em>I</em> ⋆ <em>W</em></span> is given
by the cross correlation between the Jacobian <span class="math inline"><em>J</em><sup><em>O</em></sup></span> that contains
the downstream gradients <span class="math inline"><em>δ</em><sup><em>o</em></sup></span> with respect
to the activation map parameters. The shaded area in <span class="math inline"><em>I</em></span> shows the elements that are used
in the dot product with <span class="math inline"><em>J</em><sup><em>O</em></sup></span> to compute
the shaded element in <span class="math inline"><em>J</em><sup><em>W</em></sup></span>.</p>

<h3 id="cnn-architectures">CNN architectures</h3>
<p>We build complete CNNs by stacking convolutional
and pooling layers. A classical architecture is LeNet-5 for digit
classification from black &amp; white images of hand-written digits <d-cite key="lecun_gradient-based_1998"></d-cite>. A slightly simplified version has the form
INPUT, [[CONV, POOL] \(\times\) 2], CONV, FC, FC, SOFTMAX. In this
notation, INPUT stands for a tensor of a batch of images
(\([100 \times 32 \times 32 \times 1]\) for a batch of 100 32 \(\times\) 32
black &amp; white images), CONV denotes six, sixteen, and 120 \(5 \times 5\)
filters with stride 1 and tanh activation, POOL denotes an average
pooling layer with a 2 \(\times\) 2 filter and a stride of 2, and FC
represents fully-connected layers. The first layer has tanh activations
and the last layer calculates the softmax probabilities for ten
different digits. The FC layers are used to extract features not only
locally but globally and because this type of layer is cheaper after
multiple rounds of downsampling. A receptive field of a (hidden) feature
is the set if inputs that influence this feature. In a fully connected
layer, the receptive field of every hidden feature is always the
complete input vector or tensor. By stacking multiple convolutional
layers, we can achieve the same receptive field with much less
parameters. The outputs from higher layers have larger receptive fields
and thus represent higher-level features. One example for these type of
features could be far-reaching edges.</p>

<h3 id="lengthy-network-paths">Lengthy network paths</h3>
<p>In the last paragraph, we learned that
classic CNN architectures are essentially a stack of functions. In
Chapter <a href="https://www.tobiasstenzel.com/blog/2023/dl-backprop/#backpropagation">4</a>, however, we saw that a sequence of function
applications results in a long and linear backpropagation graph given by
a multiplication sequence of partial derivatives. If a number of these
partial derivatives are either very small or very large, their
multiplicative effect can cause either too small or too large gradient
updates during optimization. Especially layers with sigmoid activations
(e.g. logistic, tanh) with derivatives that are flat or extremely steep
for large parts of the domain are problematic. If parameters have once
reached these parts, learning oftentimes stops for larger chunks of the
network for two reasons. First, for these parameters, it requires a
number of unusually large steps to leave these extreme areas. And
second, other parameters with gradients that include multiplications
with the extreme gradients are set to zero or infinity, too.</p>

<h3 id="skip-connections">Skip connections</h3>
<p>We can alleviate the problem by connecting earlier
(or bottom) layers, \(h^{(i)}\), with later (or top) layers, \(h^{(i+k)}\),
via the duplication operation followed by the "+" operator. With that,
we open up a new path past the majority of the stacked functions. We
call this type of link a <em>skip connection</em>. The effect is that, in the
backward pass, \(l^{(i)}\) receives another downstream Jacobian
\(J^{(i+k)}\cdot ... \cdot J^{(o)}\) that we add to the more complex
Jacobian
\(J^{(i+1)} \cdot J^{(i+2)} \cdot ... \cdot J^{(i+k)} \cdot... \cdot J^{(o)}\).
Intuitively, the updates from the more complex Jacobian are used to
learn the difference between the bottom layer and the top layer. As a
result, we can learn a simple representation of the model without
exposing the gradient to further multiplicative transformations and, in
addition, we can learn another representation for more complex relations
between the input features. An illustrative toy model similar to Figure
<a href="https://www.tobiasstenzel.com/blog/2023/dl-backprop/#toy-example">1</a> is \(C(\theta)= \tanh (\theta)^n\) where
\(n \in \mathbb{N}^+\) represents the number of subsequent tanh
operations. The model with skip connection is
\(C_{res}(\theta)= C(\theta) + \theta\). We can observe the described
technical aspects by comparing \(\partial C / \partial \theta\) with
\(\partial C_{res} / \partial \theta\) and the respective computational
graphs. An early implementation of this idea is Microsoft’s ResNet <d-cite key="he_deep_2016"></d-cite>. This architecture uses skip connections that only skip
one layer at a time. We will return to the problem of vanishing and
exploding gradients from lenthy network parths in our discussion of
recurrent neural networks.</p>

<h3 id="inductive-bias">Inductive bias</h3>
<p>In the previous paragraph, we have seen how we can
design neural network architectures to form sensible predictions on a
domain-specific type of input data. We have also learned how to exploit
the peculiar spatial relations in this data in order to save parameters
and training time compared to fully-connected FNNs. The assumptions that
we pose on the relations in the data in our architecture design is
called the <em>inductive bias</em>. In summary, there are three inductive
biases in the convolution layers of CNNs:</p>

<ul>
  <li>
    <p><strong>Translation equivariance:</strong> the convolution operation is translation
eqiuivariant, i.e. \(f(x + dx) = f(x) + dx\) with input \(x\), change \(dx \in \mathbb{R}^n\) and
\(f\) denotes the convolution operation. This means, if you apply a convolution operation to the original input and then apply the same operation to a translated version of the input, the resulting feature maps will have the same translation relationship. As a consequence, CNNs, which rely heavily on convolution operations, can learn to detect features in an image regardless of their spatial position. For example, a CNN trained to recognize cats will be able to detect a cat in different parts of an image because the learned features are translation equivariant. This reduces the need for manually designing position-specific features and allows CNNs to generalize well to various tasks involving spatial data. For other transformations than shifting,
such as rotations and change in color, however, we need to train on
additional augmented images.</p>
  </li>
  <li>
    <p><strong>Locality of features:</strong> the filter sizes are much smaller than the
image because we assume that local relations between the pixels are
more important than global relations.</p>
  </li>
  <li>
    <p><strong>Universality of feature extractors:</strong> we can reuse the same filter
for all regions of the input because we assume that the hidden
features which we extract are similarly important at each position.</p>
  </li>
</ul>

<p>In order to improve our results, we soften the inductive bias regarding
the locality of features with two additional layers: at the beginning of
the network, we include cheap pooling layers and towards the end we add
fully connected layers.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = &quot;Deep Learning Series&quot;,
  author  = &quot;Stenzel, Tobias&quot;,
  year    = &quot;2023&quot;,
  url     = &quot;https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>]]></content><author><name>Tobias Stenzel</name></author><category term="dl-fundamentals" /><category term="neural-net-archetype" /><category term="backprop" /><category term="cross-correlation" /><category term="inductive-bias" /><category term="receptive-field" /><category term="compute-graph" /><summary type="html"><![CDATA[🔑️ Key sections about parameter sharing, inductive biases, skip connections, and cross-correlation.]]></summary></entry><entry><title type="html">5. Feedforward Neural Networks</title><link href="https://www.tobiasstenzel.com/blog/2023/dl-fnn/" rel="alternate" type="text/html" title="5. Feedforward Neural Networks" /><published>2023-03-05T00:00:00+00:00</published><updated>2023-03-05T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/dl-fnn</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/dl-fnn/"><![CDATA[<h2 id="feedforward-neural-networks">Feedforward Neural Networks</h2>

<p>In the last sections we learned that we can compute any differentiable
loss function between an arbitrary differentiable function \(f\) that
takes input \(x\) and outputs predictions \(\hat{y}\) and the data \((x,y)\),
and optimize the model \(f\) with respect to its parameters \(\theta\) with
stochastic gradient descent. In this section, we look at how to
construct \(f\) as a neural network.</p>

<h3 id="vanilla-neural-networks">Vanilla Neural Networks</h3>

<p>In the two examples from Chapter <a href="https://www.tobiasstenzel.com/blog/2023/dl-supervised-learning/#supervised-learning">2</a>, neural network regression and
neural network classification, we have already discovered one main idea
of vanilla neural networks: combining matrix multiplications and
element-wise non-linearities. The other idea is that we can repeat, or
layer, these two transformations multiple times. For instance,
abstracting from the concrete structure of the input and output data, we
would write a neural network with two fully connected layers as
\(f(x)=W_1 \phi (W_1 x)\), where \(\phi\) represents an an element-wise
non-linearity like tanh and \(W_1\), \(W_2\) are matrices that interact,
scale, and shift the inputs. A 3-layer neural networks would be
implemented as \(f(x)=W_3 \phi (W_1 \phi (W_1 x))\), and so forth. The
outputs from the intermediate functions are called hidden layers, and
one output a hidden unit. We can think of hidden units as feature
abstractions from the previous layer, or latent features for the next
layer. During training, the neural network learns which feature
abstractions are useful to the next layer. A network is called deep if
it has more than one hidden layer. Common choices for non-linearity
\(\phi\) are tanh, the rectified linear unit (ReLU) \(\max(0,x)\), and the
logistic function \(1/(1+e^{-x})\). Usually, we add an additional element
\(x_0 = 1\) to the input vector. The corresponding weight, or bias,
\(b:=w_0\) shifts the output. The choice of the last layer depends on the
type of output data \(y\). For instance, we could select the logistic
function for binary classification, the softmax function for multi-class
classification, and a linear layer to predict natural numbers. Similar
to our toy example in Figure <a href="https://www.tobiasstenzel.com/blog/2023/dl-backprop/#toy-example">1</a>
reference=”fig:toy_graph”}, we can depict a vanilla neural network as a
directed acyclical graph and compute its gradient via backpropagation in
a supervised learning setting. Figure
<a href="#fig:vanilla_neural_net">2</a> depicts a two-layer example
architecture for a binary classification task. Note the similarity
between this network and the second example from Chapter <a href="https://www.tobiasstenzel.com/blog/2023/dl-supervised-learning/#supervised-learning">2</a>. Figure
<a href="#fig:vanilla_neural_net">2</a> clarifies how we aggregate dot
product operations on the neuron level to matrix operations on the layer
level. In the last decade, the neural network approach has led to
state-of-the-art results in areas with large amounts of high-quality
data such as computer vision, natural language processing, speech
recognition and others. A theoretical reason for this success is that
neural networks can achieve universal approximation. This means that
they can approximate any continuous function, either via sufficient
depth (number of layers; e.g. <d-cite key="cybenko_approximation_1989"></d-cite> or width
(number of columns in weight matrices; e.g. <d-cite key="hanin_approximating_2017"></d-cite>. A practical reason is that we can optimize
these functions very efficiently with many parallel computations on
modern hardware.</p>

<figure id="fig:vanilla_neural_net">
<center><img src="/assets/img/dl-series/vanilla-neural-net.png" style="width:100%" /></center>
</figure>
<p><b>Figure 2. Vanilla neural network with two fully-connected hidden
layers for binary classification.</b> The first hidden layer, \(h^{(1)}\), is composed of three neurons. Each neuron takes the input vector \(x\) and computes the dot product with its weight vector from its respective column of the layer’s weight matrix \(W^{(1)}\). Moreover, \(h^{(1)}\) introduces non-linearity via an elementwise non-linear operation \(\phi\). The second layer, \(h^{(2)}\), repeats the same process with the previous hidden layer as its input but reduces the number of hidden features. The output layer $o$ has the same size as the last hidden layer and computes the Softmax probabilities for classes one and two.</p>

<h3 id="backward-pass">Backward pass</h3>

<p>To improve our understanding about backpropagation of
neural networks, we look at the partial derivatives of typical layers.
Note that we usually consider the derivatives with respect to the loss
instead of the cost function because the regularization terms are not
complex and simply add up to the more complicated loss derivative at the
end of each update computation.</p>

<ul>
  <li>
    <p><strong>ReLU:</strong> the derivative of the element-wise ReLU is given by
\(\frac{\partial ReLU}{\partial z} = 0\) if \(x \leq\) 0 and 1 if x
\(&gt;\) 0. As a consequence, we propagate only the
gradient for the neurons with positive activations back to the
previous layer.</p>
  </li>
  <li>
    <p><strong>Linear layer:</strong> Let \(z = W x\) be a linear layer with one input
channel and \(n\) elements, \(x \in \mathbb{R}^{n \times 1}\), with
\(W \in \mathbb{R}^{m \times n}\) and let \(z, \delta 
 := \frac{\partial L}{\partial z} \in  \mathbb{R}^{m \times 1}\).
Then
\(\frac{\partial L}{\partial W} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial W} = \delta x^T\).
The same result, \(\delta X^T\), holds for linear layers with \(k\)
features or input channels, i.e. with
\(X \in \mathbb{R}^{n \times k}\) and
\(Z, \delta \in \mathbb{R}^{m \times k}\). Hence, <em>each</em> input
receives the input-specic respective weighted sum of the upstream
gradient of <em>all</em> neurons from the next layer.</p>
  </li>
  <li>
    <p><strong>Softmax:</strong> Let \(\hat{p}=\text{softmax}(z)\) denote the softmax
probabiltities with \(\hat{p}, z \in \mathbb{R}^{n \times m}\) and let
\(L(\hat{p},y)\) with \(y \in \mathbb{R}^{n \times 1}\) be the scalar
cross-entropy loss. We get
\(\frac{\partial L}{\partial z} = \frac{1}{n}(\hat{p} - y \otimes \vec{1}^m
)\), where \(\otimes\) denotes the outer product between two vectors.
If we add another logit layer \(l\in \mathbb{R}^{n \times m}\) below
the softmax, we obtain the following simple result:</p>
  </li>
</ul>

\[\frac{\partial L}{\partial l_{i,j}} =
\begin{cases}
   \hat{p}_{i,j} &amp; \text{if i $\neq$ j} \\
   \hat{p}_{i,j} - 1 &amp; \text{if i = j}.
\end{cases}\]

<ul>
  <li>Intuitively, for each example \(i \in 1,...,n\), the gradients for
each parameter that flow from each predicted probability is
increased by the amount that the predicted probability differs from
the actual label (times \(1/n\)). Therefore, gradient descent in
particular updates parameters \(\theta\) towards the direction of the
gradients from the bad predictions.</li>
</ul>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = &quot;Deep Learning Series&quot;,
  author  = &quot;Stenzel, Tobias&quot;,
  year    = &quot;2023&quot;,
  url     = &quot;https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>]]></content><author><name>Tobias Stenzel</name></author><category term="dl-fundamentals" /><category term="neural-net-archetype" /><category term="backprop" /><summary type="html"><![CDATA[Feedforward Neural Networks]]></summary></entry><entry><title type="html">4. Backpropagation</title><link href="https://www.tobiasstenzel.com/blog/2023/dl-backprop/" rel="alternate" type="text/html" title="4. Backpropagation" /><published>2023-03-04T00:00:00+00:00</published><updated>2023-03-04T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/dl-backprop</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/dl-backprop/"><![CDATA[<h2 id="backpropagation">Backpropagation</h2>

<p>We learned that we can find a mapping \(f \in \mathcal{F}\) that maps
features X to outcome Y consistent with the data by minimizing the cost
function with repeated gradient evaluations using a gradient descent
optimizer.</p>

<p>We compute the gradient with backpropagation. This algorithm allows us
to efficiently compute gradients of functions that 1.) are
scalar-valued, 2.) have many input parameters, and 3.) can be decomposed
into simpler intermediate functions that are differentiable. The
mathematical formula of backpropagation is inspired by the third
property. It is the gradient computation via recursive applications of
the chain rule from calculus. The algorithmic order is inspired by the
first two properties. The idea is to efficiently compute the resulting
derivative starting from the intermediate functions on the parameter
instead of the cost function side.</p>

<h3 id="toy-example">Toy example</h3>

<p>Recall that we would like to compute the gradient of
cost function \(C(\theta)\), which takes not only parameters \(\theta\) but
also multiple examples \((x_i, y_i)\) as input. Specifically, our
first-order optimizer requires the gradient of the cost function with
respect to the model parameters \(\nabla_{\theta}C(\theta)\) in order to
update the parameters \(\theta\). Let us consider the following example.
Let \(C(\theta_1, \theta_2)=\theta_1 \theta_2 + \tanh (\theta_1)\) be the
cost function of a neural network with parameters \(\theta_1\) and
\(\theta_2\). Figure <a href="#fig:toy_graph">1</a> shows that we can view this equation as a
computational graph with cost function \(C\) as root and parameters as
leaf nodes. We introduce intermediate variables to write \(C\) as a
sequence of the intermediate functions \(z_3=\tanh (\theta_1)\),
\(z_4=\theta_1 \theta_2\), \(z_5=z_3+z_4\), and \(C(\theta)=z_5\). For ease of
notation, we will later also write \(\theta_1,\theta_2\) and \(C(\theta)\)
as \(z_1, z_2\) and \(z_6\). We can write the gradient of the cost function
with respect to each parameter as a combination of the gradients of its
parent nodes using the chain rule from calculus and the fact that
multiple occurrences of a term add up in its derivative. This is shown
in Equation <a href="#eq:dtheta_1">1</a> and
<a href="#eq:dtheta_2">2</a>:</p>

\[\begin{align}
\frac{\partial C}{\partial \theta_1}=\frac{\partial C}{\partial z_5} \bigg( \frac{\partial z_5}{\partial z_3} \frac{\partial z_3}{\partial \theta_1} + \frac{\partial z_5}{\partial z_4} \frac{\partial z_4}{\partial \theta_1} \bigg) = 1 - \tanh ( \theta_1)^2 + \theta_2,
\label{eq:dtheta_1}\end{align}\]

\[\begin{align}
\label{eq:dtheta_2}
\quad \frac{\partial C}{\partial \theta_2}=\frac{\partial C}{\partial z_5}\frac{\partial z_5}{\partial z_4}\frac{\partial z_4}{\partial \theta_2}   =\theta_1.\end{align}\]

<figure id="fig:toy_graph">
<center><img src="/assets/img/dl-series/compute-graph.png" style="width:75%" /></center>
</figure>
<p><b>Figure 1. Computational graph for a toy example of a neural network’s
forward pass without vector-valued intermediate functions, data,
regularization and loss.</b> The function given by the graph is <span class="math inline"><em>C</em>(<em>θ</em><sub>1</sub>,<em>θ</em><sub>2</sub>) = <em>θ</em><sub>1</sub><em>θ</em><sub>2</sub> + tanh (<em>θ</em><sub>1</sub>)</span>.
Intermediate functions that are relevant for deriving the gradient with
the chain rule are denoted by <span class="math inline"><em>z</em><sub><em>i</em></sub></span> with <span class="math inline"><em>i</em> ∈ {1, …., 6}</span>. The edges along
which the intermediate evaluations move forth during the forward pass
and along which the gradients move back during the backward pass are
denoted by <span class="math inline"><em>e</em><sub><em>i</em>, <em>j</em></sub></span>
with <span class="math inline"><em>i</em>, <em>j</em> ∈ {1, …, 6}</span>. We can
view two edges leaving one node as a a shortcut depiction for applying
the <span class="math inline"><code>duplicate</code></span> or fork
operation along both edges.</p>

<h3 id="vector-valued-intermediate-functions">Vector-valued intermediate functions</h3>
<p>The toy example deviates from
a realistic neural network application in a few aspects. One of these
aspects is that we usually consider vectors of large parameter groups.
Another aspect is that the intermediate functions that transform the
parameter vector \(\theta\) step-by-step are usually vector-valued (unlike
the final cost function). Apart of confluence operators like "+" and
"*" that usually combine different parameter groups, the final
gradients can be written as a sequence of "local" gradient
computations. Due to vector-valued functions and the number of
parameters, these computations are written as matrix multiplications. We
look at this extension more formally. To this end, let \(z_0\) be the
input vector which we transform through a series of functions be
\(z_i = f_i(z_{i-1})\) where \(i=1,...,k\) and only the last \(z_i\) can be
scalar. Assuming that the functions \(f_i\) are once differentiable, we
can compute the Jacobian matrix \(\frac{\partial z_i}{\partial z_{i-1}}\)
for all intermediate functions. This will give us the values of the
first derivative of every output dimension of \(z_{i}\) depending on each
single input dimension of \(z_{i-1}\). From the multivariable chain rule,
we obtain the result that the gradient of our final function with
respect to input vector equals the product of all intermediate
Jacobians:
\(\frac{\partial z_k}{\partial z_{0}} = \prod_{i=1}^k \frac{\partial z_i}{\partial z_{i-1}}\).</p>

<h3 id="reverse-accumulation">Reverse accumulation</h3>
<p>We can compute
\(\frac{\partial C}{\partial \theta}\) in two different ways. In this
section we learn that one approach is much more efficient. The reason is
the the structure of our problem: we minimize a scalar-valued function
with a large number of parameters. One approach is to compute the
gradient from parameters to output:
\(\frac{\partial C}{\partial \theta} = \frac{\partial z_k}{\partial z_{k-1}} \cdot ... \cdot \frac{\partial z_2}{\partial z_1}\).
This is called forward accumulation. The other approach, reverse
accumulation, is to compute the gradients from output to parameters:
\(\frac{\partial C}{\partial \theta} = \frac{\partial z_2}{\partial z_1}\cdot   ... \cdot \frac{\partial z_k}{\partial z_{k-1}}\).
The first approach is less efficient but more intuitive. It is more
intuitive because the derivatives can be computed in sync with the
evaluation steps. This makes it easier to think about how confluence
operations like "\(+\)", "\(*\)", the fork operation <code class="language-plaintext highlighter-rouge">duplicate</code>, or
filter operations like <code class="language-plaintext highlighter-rouge">max</code> or <code class="language-plaintext highlighter-rouge">average</code> transform the gradients. We
will later see that it is crucial to use these operations in a careful
way in the architecture design of neural networks because they can have
large effects on the model performance. In reverse accumulation, we fix
the dependent variable to be differentiated and compute the derivative
with respect to each intermediate function recursively. Table
<a href="#tab:reverse">1</a> shows
how we can calculate the two gradients of our toy example step-by-step
with this method. There are two important things to note. First,
computing the backward operations corresponding to the forward
operations is prone to error. This is one reason why this should be done
automatically by a graph-based computer program. Second, we can re-use
our intermediate computations \(\bar{z}_3\) to \(\bar{z}_5\) for both
gradients based only on one evaluation of the cost function
\(C(\theta) = z_6\). In realistic neural network applications, we are able
to re-use a large number of intermediate results based on only one
forward evaluation for an even larger number of input parameters. In
contrast, with forward-mode accumulation, computing the gradient
requires to evaluate each intermediate function with the whole parameter
vector. Although re-using the intermediate gradients requires more
storage, the reverse-mode accumulation strategy is much more efficient
for functions like neural networks where the number of output values is
much smaller than the number of input values.</p>

<figure id="tab:reverse">
<center><img src="/assets/img/dl-series/table-backprop.png" style="width:80%" /></center>
</figure>

<p><b>Table 1. Gradient computations in reverse accumulation mode for toy
example <span class="math inline"><em>C</em>(<em>θ</em><sub>1</sub>,<em>θ</em><sub>2</sub>) = <em>θ</em><sub>1</sub><em>θ</em><sub>2</sub> + tanh <em>θ</em><sub>1</sub></span>.</b>
The left column shows the evaluation and the right column depicts the
derivation steps. <span class="math inline"><em>z̄</em></span> denotes
the derivative of the cost function with respect to an intermediate
expression, i.e. <span class="math inline">\(\frac{\partial C}{\partial
z}\)</span>. In the backward pass, we first evaluate the scalar-cost
function and use its derivative with respect to <span class="math inline"><em>z</em><sub>5</sub></span> at the top of the
graph. Here we assume <span class="math inline"><em>z̄</em><sub>5</sub></span> equals <span class="math inline">1</span>. In the next step, we combine the chained
gradients of the intermediate expressions according to the respective
backward functions of the <code>duplicate</code> and the confluence
operations “<span class="math inline">+</span>” and “<span class="math inline">*</span>” from top to bottom. Addition distributes
the upstream gradient down to all its inputs. Multiplication passes the
upstream gradient multiplied with the <em>other</em> input back. I.e.,
<span class="math inline">\(\frac{\partial (z_3 + z_4)}{\partial z_{4}} =
1, \frac{\partial (z_1 * z_2)}{\partial z_{2}} = z_1\)</span>, and the
backward function of <span class="math inline">(<em>z</em><sub>1, 3</sub>,<em>z</em><sub>1, 4</sub>) = <code>duplicate</code>(<em>z</em><sub>1</sub>)</span>
equals <span class="math inline">\(\frac{\partial z_3}{\partial z_{1}} +
\frac{\partial z_4}{\partial z_{1}}\)</span>, where <span class="math inline"><em>z</em><sub>1, 3</sub></span> and <span class="math inline"><em>z</em><sub>1, 4</sub></span> denote abbreviated
nodes from the implicit duplication operation in the third and fourth
evaluation step.</p>

<h3 id="implementation">Implementation</h3>
<p>In our toy example, we have already discovered that
it is easier to think of the evaluation and differentiation of a neural
<em>network</em> in terms of a directed graph of operations instead of a linear
sequence of function applications. In this graph, the nodes stand for
differentiable operations that take a number of vectors from its
incoming edges, transforms and potentially interrelates them, and sends
the result to the next nodes along its outgoing edges. The graph
abstraction translates to most implementations of the backpropagation
algorithm. A <em>Graph</em> object keeps the connections (e.g. <code class="language-plaintext highlighter-rouge">duplicate</code> or
\(+\) ) between the nodes and the collection of operations (e.g. \(\tanh\)).
Both Node and Graph objects implement a <code class="language-plaintext highlighter-rouge">forward()</code> and a <code class="language-plaintext highlighter-rouge">backward()</code>
method. The Graph’s <code class="language-plaintext highlighter-rouge">forward()</code> calls the Nodes’ <code class="language-plaintext highlighter-rouge">forward()</code> methods in
their topological order. With that, every Node computes its operation on
its input, and the Graph sends it to the next Node. The Graph’s
<code class="language-plaintext highlighter-rouge">backward()</code> iterates over the nodes in reverse topological order and
calls their <code class="language-plaintext highlighter-rouge">backward()</code> methods. In the backward pass, each Node is
given the gradient of the cost function with respect to its output and
it returns the "chained" gradients with respect to all its inputs.
"Chained" means that the Node multiplies the Jacobian that it received
from its parent nodes with its own local Jacobian. The Graph sends the
resulting product to the Nodes’ children and the process repeats until
the recursion ends at the last computation which includes the data and
the current parameter values. At last, the optimizer updates the
parameter vector based on the final gradient (Step 3 in Algorithm <a href="https://www.tobiasstenzel.com/blog/2023/dl-optimization/#first-order-methods">1</a>). Note
that the implemented compute graph for Figure
<a href="#fig:toy_graph">1</a> would in practice be larger and rudimentary because we can decompose
many operations, like divide or subtract in tanh, into more basic
operations.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = &quot;Deep Learning Series&quot;,
  author  = &quot;Stenzel, Tobias&quot;,
  year    = &quot;2023&quot;,
  url     = &quot;https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>]]></content><author><name>Tobias Stenzel</name></author><category term="dl-fundamentals" /><category term="backprop" /><category term="reverse-accumulation" /><category term="compute-graph" /><summary type="html"><![CDATA[💡 Includes a nice detail about why we don't 'frontpropagate'. ;-)]]></summary></entry><entry><title type="html">3. Optimization</title><link href="https://www.tobiasstenzel.com/blog/2023/dl-optimization/" rel="alternate" type="text/html" title="3. Optimization" /><published>2023-03-03T00:00:00+00:00</published><updated>2023-03-03T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/dl-optimization</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/dl-optimization/"><![CDATA[<h2 id="optimization">Optimization</h2>

<p>In the previous section we formalized supervised learning of a
predictive model as solving the optimization problem
\(\theta^* = \arg \min_{\theta \in \Theta} C(\theta)\), where \(\theta\)
denotes the model parameters and cost function \(C\) represents the
average loss of all examples plus a regularization penalty. In realistic
settings, there is no closed form solution to this problem. Therefore,
we have to rely on schemes that iteratively proposes new parameters
given the previous choice or the initial guess. We want to choose a
method that proposes new candidates with a high likelihood of reducing
the loss compared to the current parameters, so we can replace them.</p>

<h3 id="first-order-methods">First order methods</h3>
<p>In practice, a neural network is a composition
of many small functions that are easy to differentiate. Therefore, we
can compute the gradient \(\nabla_\theta C\) from our network via the
backpropagation technique. We will discuss this method in detail in the
next section. The gradient of our cost function is a vector of
first-order partial derivatives. It contains the direction of its
fastest increase and its magnitude is the rate of increase in that
direction. The gradient at a minimum of the loss function equals zero.
Therefore, we can use the negative gradient as search direction for
selecting the next proposal \(\theta_{i+1}\) from current candidate
\(\theta_i\). This is the basic idea of the gradient descent algorithm.
The method alternates between two steps: 1.) Compute the gradient
\(\nabla_\theta C(\theta)\). 2.) Update \(\theta\) by subtracting a small
multiple of the gradient. Many applications use very large datasets. For
instance, the number of training images in ImageNet is about 1 million.
Therefore, it is handy to approximate the loss gradient from a small
minibatch of examples (e.g. 100). With that, we can update \(\theta\) many
times for every epoch. An epoch is one iteration over the complete
training set. In practice, a large number of approximate updates works
better than a small number of exact updates. This algorithm is called
Stochastic Gradient Descent (SGD). It is summarized in Algorithm 1.</p>

<figure id="fig:supervised-learning">
<center><img src="/assets/img/dl-series/sgd.png" style="width:90%" /></center>
</figure>

<p>A crucial parameter for gradient descent algorithms is the learning
rate, or step size, \(\eta\). If we set it too small, the optimization
requires too many steps. If we set it too large, the algorithm may not
converge or even diverge. As an illustration, consider the following toy
example with convex objective function: Let \(f=x^2\) with gradient
\(\frac{df}{dx}=2x\) and initial parameter value \(x = 1\). With \(\eta&lt;1\)
the algorithm finds \(x^*=0\). However, with \(\eta=1\), it oscillates
between \(x=-1\) and \(x=1\), and with \(\eta&gt;1\) it diverges to \(x=\infty\).
Generally, finding a useful learning rate depends on the objective
function.</p>

<h3 id="advanced-first-order-methods">Advanced first order methods</h3>

<p>Oftentimes, we can achieve faster
convergence speed with modified formulations of the update direction
\(\Delta \theta\) from Step 3 in Algorithm 1. The
two main ideas are to use weighted moving averages of all gradients so
far and to compute parameter-specific learning rates. Common variants
are Momentum <d-cite key="sutskever_importance_2013"></d-cite>, Adagrad <d-cite key="duchi_adaptive_2011"></d-cite>, RMSProp <d-cite key="hinton_lecture_2012"></d-cite> and Adam <d-cite key="kingma_adam_2014"></d-cite>. The <strong>Momentum</strong> update is inspired by the physics
notion of momentum. The current update direction is not determined by
the current gradient but also by the previous gradients. The impact of
the previous gradients, however, decays exponentially for every previous
iteration. Let \(\alpha\) be the decay parameter and
\(g := \nabla_{\theta}C(\theta)\). We then replace Step 3 by two steps:
First, we compute the "velocity" \(v= \alpha v + g\) (initialized at
zero), and second, we compute the parameter update with
\(\Delta \theta = -\eta v\). <strong>Adagrad</strong> introduces element-wise learning
rates for the gradient. I.e. we weigh every partial derivative with the
moving average of its squared sum. The motivation is two-fold: first the
shape of the objective function can vary between different dimensions.
Therefore, dimension-specific learning rates may improve the algorithm.
Second, we can achieve a more equal exploration of each dimension by
equipping dimensions with a history of small gradients with larger step
sizes and vice versa. Formally, Adagrad first introduces the
intermediate variable \(r = r + g \odot g\) where \(\odot\) denotes
elementwise multiplication. Second, it computes the gradient update with
\(\Delta \theta = -\frac{\epsilon}{\delta + \sqrt{r}} \odot g\). In the
last expression, \(\delta\) is a small number to avoid division by zero.
<strong>RMSProp</strong> introduces an additional hyperparameter to weigh Adagrad’s
running average, i.e. \(r s= \rho r + (1-\rho)g \odot g\). Lastly, the
<strong>Adam</strong> update combines gradient momentum and RMSProp’s
dimension-specific learning rates.</p>

<h3 id="hyperparamter-optimization">Hyperparamter optimization</h3>

<p>In the discussion of supervised learning
and optimization, we encountered many configurations that we did not
specify. Examples are the number of hidden units \(H\) in our neural
network, the regularization strength \(\lambda\), or the learning rate
\(\eta\) in gradient descent. These parameters either change the
composition of the model parameters \(\lambda\) or impact their selection
during training. Therefore, they have to be set before a model is
trained. To this end, we have to select our hyperparameter space
\(\mathcal{H}\). Due to restrictions of time and computational power, this
choice is often based on experiences with similar models from previous
research. Principled approaches range from model-free methods like
random search to global optimization frameworks like Bayesian
optimization. Model-free methods are simple and do not use the
evaluation history whereas global optimization techniques additionally
consider the uncertain trade-off between exploring new values and
exploiting good values that have already been found. I recommend the
textbook chapter by <d-cite key="feurer_hyperparameter_nodate"></d-cite> for more explanations
and concrete examples.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = &quot;Deep Learning Series&quot;,
  author  = &quot;Stenzel, Tobias&quot;,
  year    = &quot;2023&quot;,
  url     = &quot;https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>]]></content><author><name>Tobias Stenzel</name></author><category term="dl-fundamentals" /><category term="optimization" /><category term="first-order-methods" /><summary type="html"><![CDATA[Optimization]]></summary></entry><entry><title type="html">2. Supervised Learning</title><link href="https://www.tobiasstenzel.com/blog/2023/dl-supervised-learning/" rel="alternate" type="text/html" title="2. Supervised Learning" /><published>2023-03-02T00:00:00+00:00</published><updated>2023-03-02T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/dl-supervised-learning</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/dl-supervised-learning/"><![CDATA[<h2 id="supervised-learning">Supervised Learning</h2>

<p>Prediction rules based on some specific set of information can be
written as a mapping \(f:X \rightarrow Y\), where \(X\) is an input space
and \(Y\) is an output space. To recognize a dog on a photo, for example,
\(X\) would be the space of images and \(Y\) would be a probability interval
\([0,1]\) for the presence of a dog. However, it is oftentimes very
difficult to find an explicit function \(f\) from theoretical
considerations about the problem. For problems where it is easy to find
many examples \((x,y) \in X \times Y\), the supervised learning approach
is usually well-suited. In our example, the requirement would be a
dataset that consists of a large number of images which are annotated
with presence or absence of a dog.</p>

<h3 id="loss-function">Loss function</h3>

<p>To be concrete, our dataset
\(\{(x_1,y_1),...(x_n,y_n)\}\) includes \(n\) examples. Our theoretical
assumption about the data is that these examples are drawn from a data
generating distribution \(D\) with independent and identically distributed
(i.i.d.) random variables, i.e.
\((x_i, y_i) \overset{\mathrm{iid}}{\sim} D\). In the supervised learning
context, learning the mapping \(f:X \rightarrow Y\) means selecting the
function \(f\) from a set of candidate functions \(\mathcal{F}\) so that \(f\)
yields the best predictions for \(y\) given any \(x\). I.e., we want to find
the best approximation for \(f(Y|X)\). We achieve this by selecting a
scalar-valued loss function \(L(\hat{y},y)\) which measures the difference
between prediction \(\hat{y}_i\) and actual outcome \(y_i\). In theory, our
objective is to find the function with the lowest expected loss for all
examples from the data generating distribution \(D\). In practice,
however, we have to rely on our dataset. Therefore, we approximate our
theoretical objective using the assumption that our data is drawn from
an i.i.d. distribution. We search for function \(f^*\) with the lowest
average loss over the data:</p>

\[\begin{align}
\label{eq:loss}
f^* \approx \arg \min_{f \in \mathcal{F}} \frac{1}{n} \sum^n_{i=1} L \big  ( \ f(x_i), y_i \big  ).
\end{align}\]

<h3 id="regularization">Regularization</h3>

<p>Oftentimes, the i.i.d. assumption is too strong. In
this case, we usually do not achieve the best result for predicting
unseen outcomes with a function that is optimized solely with regards to
our dataset. In short, this function does not necessarily generalize
well. A further issue is how to choose between multiple functions with
the same minimal loss. The approach that addresses both problems at once
is regularization. The idea is to add a regularization penalty to our
objective function from which we obtain our predictor. The penalization
criterion \(R(f)\) is function complexity. We thus search for the function
that fits the data best <em>and</em> has a low complexity:</p>

<p>\(\begin{align}
\label{eq:cost}
f^* \approx \arg \min_{f \in \mathcal{F}} \frac{1}{n} \sum^n_{i=1} L \big  (f(x_i), y_i \big  ) + R(f),
\end{align}\)
where \(R(f)\) is a scalar-valued function. With regularization, we can
achieve a better generalization and choose between functions that
achieve a similar loss.</p>

<p>Frequently, we have already selected our model but not its parameters.
Hence, we take function \(f\) as given but we want to learn the best
parameters \(\theta\) for this function. In this situation, the concept of
loss and regularization directly translates from finding the optimal
function to finding the optimal parameters:</p>

\[\begin{align}
\label{eq:params}
\theta^* \approx \arg \min_{\theta \in \Theta} \frac{1}{n} \sum^n_{i=1} L \big (f(x_i;\theta), y_i \big  ) + R(\theta),
\end{align}\]

<p>where \(\Theta\) denotes the parameter space. Common examples for
\(R(\theta)\) are multiples of vector norms. In this setting, choices like
the model, the loss or the regularization are called hyperparameters.
These are parameters that are set before the actual model parameters are
learned. Hyperparameter choices are oftentimes critical to the quality
of the learned model. There are several other ways to prevent
overfitting the model besides regularization. Examples are choosing
simpler models, stopping the optimization process early, changing or
disabling some model units during training (dropout), and dataset
augmentations.</p>

<h3 id="model-validation">Model validation</h3>

<p>How do we test whether our model generalizes well
to unseen data? The usual machine learning approach is as follows: at
first, we split our examples in three groups: a large training, and
smaller validation and test sets. Secondly, we learn the model
parameters with the training data. The third step is to compute
evaluation metrics for this model from the unseen test data. In general,
we want to minimize the distance between the prediction and the target
vector. For classification tasks, we can use evaluation metrics based on
the confusion matrix. This enables us to identify single classes which
are more difficult to predict for the model. Optionally, we can repeat
the third step multiple times for different hyperparameter
configurations and compare the generalization errors. Then, we evaluate
the best model once again on the validation data and report its
performance. We will briefly discuss hyperparameter selection in the end
of the optimization section.</p>

<h3 id="example--linear-regression">Example — Linear regression</h3>

<p>Assume we have a dataset with 100
observations \((n=100)\) of two features each \((m=2, X=\mathbb{R}^2)\) and
a scalar annotation \((Y=\mathbb{R})\). According to the input and output
space, we choose to restrict the candidate class \(\mathcal{F}\) to the
family of linear functions, i.e.
\(\mathcal{F}=\{w^Tx + b | w \in \mathbb{R}^2, b\in \mathbb{R}\). With
this choice, we set our hypothesis space from a class of functions to
the set of three parameters \(\theta=\{w_1, w_2, b\}\) with
\(w=[w_1, w_2]\). Next, I list two common hyperparameter choices. First,
the squared difference between predicted value and target,
\(L(\hat{y}, y)=(\hat{y} - y)^2\), is a common loss function. Second, the
L2 norm of the weights multiplied with importance parameter \(\lambda\) is
a typical regularizer choice, i.e. \(R(w,b)=\lambda(w_1^2 + w_2^2)\). The
L2 norm counteracts extreme weights and an excessive effect of one
single weight on the prediction \(\hat{y}\). Taken together, our objective
is</p>

\[\begin{align}
\theta^* =  \arg \min_{w,b} \underbrace{
    \Bigg [ \frac{1}{n} \sum^n_{i=1} (w^T x_i + b - y_i)^2 \Bigg ]}_\text{data fitting}
    + \underbrace{\Bigg [ \lambda (w_1^2 + w_2^2) \Bigg ]}_\text{regularization}.
\end{align}\]

<h3 id="example--neural-network-regression">Example — Neural network regression</h3>

<p>Perhaps the relationship between
features \(x_1, x_2\) and the scalar target \(y\) is not liner and there are
interactions between the two features. A model class that is well-known
for its theoretical capabilities of approximating continuous functions
are feedforward neural networks (FNN). As a preview, we can extend the
previous example to a specific FNN called neural network regression. It
has the form \(f(x;\theta)= w_2 \text{ tanh} (W_1^T x + b_1) + b_2\) with
parameters \(\theta=\{W_1, b_2, w_1, b_1\}\). \(W_1\) is matrix with
dimension \(H \times 2\), \(b_1\) and \(w_2\) are both vectors of length \(H\),
and \(b_2\) is a scalar. \(H\) is an integer-type hyperparameter.
\(W_1^T x + b_1\) is often called a hidden layer. Its elements, or
neurons, are outputs of multiplicative interactions between elements
from previous layers, in this case the two inputs \(x_1\) and \(x_2\). tanh
denotes the hyperbolic tangent that squashes elements from the
real-valued domain to the interval [-1,1]. It is applied element-wise
and introduces non-linearity to the model. The objective is given by:</p>

\[\begin{align}
    \theta^* =  \arg \min_{W_1, b_2, w_1, b_1} \underbrace{
    \Bigg [ \frac{1}{n} \sum^n_{i=1} \big (w_2 \text{ tanh}  ( W_1^T x_i + b_1 ) + b_2 - y_i \big)^2 \Bigg ]}_\text{data fitting}
    + \underbrace{\Bigg [ \lambda \big (||W_1||^2_2 + ||w_2||_2^2 \big ) \Bigg ]}_\text{regularization}.
\end{align}\]

<h3 id="example--neural-network-classification">Example — Neural network classification</h3>

<p>Oftentimes, we do not want
to predict a real number but we want to predict whether an input
corresponds to an output of a specific class \(k\) of \(K\) possible
classes. For instance, we may want to predict whether a dog, a, cat, or
a budgie is shown in a picture. To formalize this problem, we encode our
classes as non-negative integers starting at 0. We also encode our
output as a vector of \(|K|\) zeros, where only the \(k^*\)-th element
representing the actual class encoded as \(k^*\) is set to one. Using the
previous order, a picture that shows a dog is encoded as \(y=[1, 0, 0]\).
We can achieve a model that predicts a vector of this form with two
simple adjustments of the neural network regression model. The first
adjustment is replacing vector \(w_2\) by matrix \(W_2\) with dimensions
\(K \times H\). This gives us a real-valued vector \(z\) with one
real-valued element for every class. The second adjustment is that we
compute class probabilities by applying the softmax function to \(z\),
i.e. \(\hat{p}_k = e^{z_k} / \sum_{i=1}^K e^{z_i}\). Handy properties of
the softmax function in the probability context are, first, that every
element is mapped to \([0,1]\), and second, that the sum of all elements
is normalized to one. Hence, our class prediction would be the class
that is represented by the largest element in vector \(\hat{p}\). For
instance, with \(K=3\) and \(\hat{p}=[0.1, 0.7, 0.2]\), our prediction
\(\hat{y}\) equals \([0, 1, 0]\). The most common loss function for
classification is the cross-entropy loss. It takes the predicted class
probabilities \(\hat{p}\) instead of the predicted class vector \(\hat{y}\).
We denote both options by output \(o\):</p>

<p>\(\begin{align}
L(o,y) = L(\hat{p},y) = - \sum_{k=1}^K y_k \log \hat{p}_k = -\log  \hat{p}_{k=k^*}.
\end{align}\)
The first equality is the cross-entropy definition for two distribution,
i.e \(H(q,r) = - \sum_x q(x) \log r(x)\). The second equality shows that
the loss equals the negative log probability from vector \(p\) at the
position of the actual class \(k^*\). It follows from the fact that target
distribution is degenerate. This means that only the true class has
probability one. Additional motivation for choosing the cross-entropy
for classification problems is that minimizing this loss is equivalent
to maximizing the likelihood of observing model parameters \(\theta\)
conditional on predicting the true class label.</p>

<h2 id="summary">Summary</h2>

<p>Supervised learning requires a dataset of \(n\) examples
\(\{(x_1,y_1)\), ..., \((x_n,y_n)\}\), where \((x_i,y_i) \in X \times Y\).
\(y_i\) represents the annotation that we want to predict based on
features \(x_i\). Finding a mapping \(f:X \rightarrow Y\) is a two-step
process. First, we have to formalize the problem by choosing</p>

<ol>
  <li>
    <p>The search space of functions \(\mathcal{F}\) with
\(f \in \mathcal{F}\).</p>
  </li>
  <li>
    <p>The scalar-valued loss function \(L(\hat{y}, y)\) that measures the
difference between the network’s predictions
\(\hat{y} = f_{\theta}(x)\) and the target \(y\).</p>
  </li>
  <li>
    <p>The scalar term \(R(f)\) that penalizes overly complex functions.</p>
  </li>
</ol>

<p>In deep learning without architecture search, the space of functions is
one neural network \(f_\theta\) with some parameters \(\theta \in \Theta\).
Putting these parts together, the second step is to find these
parameters from the optimization problem
\(\theta^* = \arg \min_{\theta \in \Theta} C(\theta)\) with
\(C(\theta) = \frac{1}{n} \sum^n_{i=1} L \big (f_\theta), y_i \big  ) + R(f_{\theta})\).
We call \(C(\theta)\) the cost function.</p>

<figure id="fig:supervised-learning">
    <center><img src="/assets/img/dl-series/supervised-learning.png" style="width:100%" /></center>
</figure>
<p><b>Figure 1: Computational graph for a general supervised learning
approach.</b> Examples \(\{x_i\}_{i=1}^{n}\) and parameters \(\theta\) are taken by model $f$ to predict the targets \(\{y_i\}_{i=1}^{n}\). Data loss $L$ computes the difference between the predictions and the targets \(\{y_i\}_{i=1}^{n}\). Regularization loss $R$ penalizes extreme parameters. The sum of both penalties is given by cost \(C\). Oftentimes, we use predicted class probabilities \(\hat{p}\) instead of (rounded) predictions.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = &quot;Deep Learning Series&quot;,
  author  = &quot;Stenzel, Tobias&quot;,
  year    = &quot;2023&quot;,
  url     = &quot;https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>]]></content><author><name>Tobias Stenzel</name></author><category term="dl-fundamentals" /><category term="supervised-learning" /><summary type="html"><![CDATA[Supervised Learning]]></summary></entry><entry><title type="html">1. Overview</title><link href="https://www.tobiasstenzel.com/blog/2023/dl-overview/" rel="alternate" type="text/html" title="1. Overview" /><published>2023-03-01T00:00:00+00:00</published><updated>2023-03-01T00:00:00+00:00</updated><id>https://www.tobiasstenzel.com/blog/2023/dl-overview</id><content type="html" xml:base="https://www.tobiasstenzel.com/blog/2023/dl-overview/"><![CDATA[<h2 id="overview">Overview</h2>

<p>I wrote most of the content for the Background section of my Master’s Thesis
“Multi-camera Multi-object Tracking with Transformers”. During this time, I discovered some new aspects and thought about
how to best introduce some concepts like backpropagation or the transformer. The purpose of this series is to share
this content about the fundamentals of deep learning with you.</p>

<p>This is the structure:</p>
<ol>
  <li><a href="https://www.tobiasstenzel.com/blog/2023/dl-overview/">Overview</a></li>
  <li><a href="https://www.tobiasstenzel.com/blog/2023/dl-supervised-learning/">Supervised Learning</a></li>
  <li><a href="https://www.tobiasstenzel.com/blog/2023/dl-optimization/">Optimization</a></li>
  <li><a href="https://www.tobiasstenzel.com/blog/2023/dl-backprop/">Backpropgation</a></li>
  <li><a href="https://www.tobiasstenzel.com/blog/2023/dl-fnn/">Feedforward Neural Networks</a></li>
  <li><a href="https://www.tobiasstenzel.com/blog/2023/dl-cnn/">Convolutional Neural Networks</a></li>
  <li><a href="https://www.tobiasstenzel.com/blog/2023/dl-rnn/">Recurrent Neural Networks</a></li>
  <li><a href="https://www.tobiasstenzel.com/blog/2023/dl-transformer/">Transformer</a></li>
</ol>

<h2 id="highlights">Highlights</h2>

<p>Some highlights are:</p>

<ol>
  <li>Preparing the introduction of the transformer carefully via self-made, detailed figures of <a href="https://www.tobiasstenzel.com/blog/2023/dl-rnn/#fig:vanilla-rnn">vanilla RNN</a>, <a href="https://www.tobiasstenzel.com/blog/2023/dl-rnn/#fig:encoder-decoder-rnn">encoder-decoder RNN</a> and <a href="https://www.tobiasstenzel.com/blog/2023/dl-transformer/#fig:attention/">encoder-decoder RNN with attention</a> ✨</li>
  <li>Explicitly spelling out <a href="https://www.tobiasstenzel.com/blog/2023/dl-transformer/#the-complete-transformer-architecture">what makes the transformer great</a> ✨</li>
  <li>Explaining convolution (or rather cross-correlation) with the <a href="https://www.tobiasstenzel.com/blog/2023/dl-cnn/#cross-correlation">right figure and equation</a> ⚖</li>
  <li>Explaining backpropagation with a very simple <a href="https://www.tobiasstenzel.com/blog/2023/dl-backprop/#toy-example">toy example</a> 🪀</li>
  <li>An explanation of <a href="https://www.tobiasstenzel.com/blog/2023/dl-backprop/#reverse-accumulation">why we do not “frontpropagate”</a> although the multiplication in the chain rule is commutative 🤯</li>
  <li>Showing backward passes of important layers. Did you know that the <a href="https://www.tobiasstenzel.com/blog/2023/dl-cnn/#fig:gradient-cross-correlation">backward pass of a convolution</a> is also a convolution? 🤯</li>
</ol>

<p>In my view, many deep learning concepts are usually introduced way too quickly or there is not enough reflection about the properties of some architectures. An example of the first observation is showing the pictures of the architecture from the original transformer paper without clarifying the encoder-decoder
beforehand in greater detail. Examples for the second observation are that it is frequently not stated what makes
the transformer great, what the inductive bias of a convolutional layer is, or why we even propagate gradients back. I took
the time to think about and research these things.</p>

<h2 id="target-group">Target group</h2>

<p>In this series, the content is very dense, although I go into some detail regarding the main concepts. Ideally, there would be much more pictures, too. As a consequence, these posts are not for beginners. Instead, I recommend this series to three groups of people:</p>

<ol>
  <li>as complementary material for students who are taking a deep learning class right now</li>
  <li>people who want to refresh some already present knowledge</li>
  <li>DL professionals who can perhaps fill some small gaps.</li>
</ol>

<h2 id="credits">Credits</h2>

<p>Of course, this series is essentially a recompilation of material from other people.
These are my main references: I recommend the Deep Learning book by Goodfellow <d-cite key="goodfellow_deep_2016"></d-cite>
, the CS231n lecture notes by Fei-Fei Li <d-cite key="li_cs231n_2018"></d-cite>, and Andrej Karpathy’s dissertation <d-cite key="karpathy_connecting_2016"></d-cite> Other references are the Wikipedia article about automatic differentiation <d-cite key="noauthor_automatic_nodate"></d-cite>
, these blogposts about convolutional neural nets <d-cite key="kafunah_backpropagation_2016"></d-cite>, recursive neural nets <d-cite key="arat_backpropagation_2019"></d-cite>, and these posts about transformers <d-cite key="weng_attention_2018"></d-cite>, <d-cite key="karpathy_transformer_2022"></d-cite>, <d-cite key="vaswani_transformers_2021"></d-cite>.</p>

<p>I also want to thank Prof. Rainer Gemulla for his excellent lectures, especially his Deep Learning class, at Mannheim University.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = &quot;Deep Learning Series&quot;,
  author  = &quot;Stenzel, Tobias&quot;,
  year    = &quot;2023&quot;,
  url     = &quot;https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>]]></content><author><name>Tobias Stenzel</name></author><category term="dl-fundamentals" /><category term="highlights" /><category term="motivation" /><category term="target-group" /><category term="credits" /><summary type="html"><![CDATA[🎬 Read this post before any other post.]]></summary></entry></feed>