<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>5. Feedforward Neural Networks | Tobias  Stenzel</title>
    <meta name="author" content="Tobias  Stenzel">
    <meta name="description" content="Tobias' homepage.
">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9D&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://www.tobiasstenzel.com/blog/2023/dl-fnn/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>

    <!-- _layouts/post.html -->
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "5. Feedforward Neural Networks",
      "description": "",
      "published": "March 5, 2023",
      "authors": [
        {
          "author": "Tobias Stenzel",
          "authorURL": "https://www.tobiasstenzel.com",
          "affiliations": [
            {
              "name": "N/A",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tobias </span>Stenzel</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
          <p class="PageNavigation">
          
            <a class="prev" href="/blog/2023/dl-backprop/">	⏮ 4. Backpropagation</a>
          
          
            <a class="next" href="/blog/2023/dl-cnn/">6. Convolutional Neural Networks ⏭</a>
          
        </p>
        <br><br>
        <h1>5. Feedforward Neural Networks</h1>
        <p></p>
        <p class="post-tags">
          <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
            ·  
            <a href="/blog/tag/neural-net-archetype">
              <i class="fas fa-hashtag fa-sm"></i> neural-net-archetype</a>  
              <a href="/blog/tag/backprop">
              <i class="fas fa-hashtag fa-sm"></i> backprop</a>  
              

        </p>
      </d-title>

      <d-byline></d-byline>

      <d-article>

        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#feedforward-neural-networks">Feedforward Neural Networks</a></div>
            <ul>
              <li><a href="#vanilla-neural-networks">Vanilla Neural Networks</a></li>
              <li><a href="#backward-pass">Backward Pass</a></li>
              
            </ul>
          </nav>
        </d-contents>

        <h2 id="feedforward-neural-networks">Feedforward Neural Networks</h2>

<p>In the last sections we learned that we can compute any differentiable
loss function between an arbitrary differentiable function \(f\) that
takes input \(x\) and outputs predictions \(\hat{y}\) and the data \((x,y)\),
and optimize the model \(f\) with respect to its parameters \(\theta\) with
stochastic gradient descent. In this section, we look at how to
construct \(f\) as a neural network.</p>

<h3 id="vanilla-neural-networks">Vanilla Neural Networks</h3>

<p>In the two examples from Chapter <a href="https://www.tobiasstenzel.com/blog/2023/dl-supervised-learning/#supervised-learning">2</a>, neural network regression and
neural network classification, we have already discovered one main idea
of vanilla neural networks: combining matrix multiplications and
element-wise non-linearities. The other idea is that we can repeat, or
layer, these two transformations multiple times. For instance,
abstracting from the concrete structure of the input and output data, we
would write a neural network with two fully connected layers as
\(f(x)=W_1 \phi (W_1 x)\), where \(\phi\) represents an an element-wise
non-linearity like tanh and \(W_1\), \(W_2\) are matrices that interact,
scale, and shift the inputs. A 3-layer neural networks would be
implemented as \(f(x)=W_3 \phi (W_1 \phi (W_1 x))\), and so forth. The
outputs from the intermediate functions are called hidden layers, and
one output a hidden unit. We can think of hidden units as feature
abstractions from the previous layer, or latent features for the next
layer. During training, the neural network learns which feature
abstractions are useful to the next layer. A network is called deep if
it has more than one hidden layer. Common choices for non-linearity
\(\phi\) are tanh, the rectified linear unit (ReLU) \(\max(0,x)\), and the
logistic function \(1/(1+e^{-x})\). Usually, we add an additional element
\(x_0 = 1\) to the input vector. The corresponding weight, or bias,
\(b:=w_0\) shifts the output. The choice of the last layer depends on the
type of output data \(y\). For instance, we could select the logistic
function for binary classification, the softmax function for multi-class
classification, and a linear layer to predict natural numbers. Similar
to our toy example in Figure <a href="https://www.tobiasstenzel.com/blog/2023/dl-backprop/#toy-example">1</a>
reference=”fig:toy_graph”}, we can depict a vanilla neural network as a
directed acyclical graph and compute its gradient via backpropagation in
a supervised learning setting. Figure
<a href="#fig:vanilla_neural_net">2</a> depicts a two-layer example
architecture for a binary classification task. Note the similarity
between this network and the second example from Chapter <a href="https://www.tobiasstenzel.com/blog/2023/dl-supervised-learning/#supervised-learning">2</a>. Figure
<a href="#fig:vanilla_neural_net">2</a> clarifies how we aggregate dot
product operations on the neuron level to matrix operations on the layer
level. In the last decade, the neural network approach has led to
state-of-the-art results in areas with large amounts of high-quality
data such as computer vision, natural language processing, speech
recognition and others. A theoretical reason for this success is that
neural networks can achieve universal approximation. This means that
they can approximate any continuous function, either via sufficient
depth (number of layers; e.g. <d-cite key="cybenko_approximation_1989"></d-cite> or width
(number of columns in weight matrices; e.g. <d-cite key="hanin_approximating_2017"></d-cite>. A practical reason is that we can optimize
these functions very efficiently with many parallel computations on
modern hardware.</p>

<figure id="fig:vanilla_neural_net">
<center><img src="/assets/img/dl-series/vanilla-neural-net.png" style="width:100%"></center>
</figure>
<p><b>Figure 2. Vanilla neural network with two fully-connected hidden
layers for binary classification.</b> The first hidden layer, <span class="math inline"><em>h</em><sup>(1)</sup></span>, is composed of
three neurons. Each neuron takes the input vector <span class="math inline"><em>x</em></span> and compute the dot product with
its weight vector from its respective column of the layer’s weight
matrix <span class="math inline"><em>W</em><sup>(1)</sup></span>.
Moreover, <span class="math inline"><em>h</em><sup>(1)</sup></span>
introduces non-linearity via an elementwise non-linear operation <span class="math inline"><em>ϕ</em></span>. The second layer, <span class="math inline"><em>h</em><sup>(2)</sup></span>, repeats the same
process with the previous hidden layer as its input but reduces the
number of hidden features. The output layer o has the same size as the
last hidden layer and computes the Softmax probabilities for classes one
and two.</p>

<h3 id="backward-pass">Backward pass</h3>

<p>To improve our understanding about backpropagation of
neural networks, we look at the partial derivatives of typical layers.
Note that we usually consider the derivatives with respect to the loss
instead of the cost function because the regularization terms are not
complex and simply add up to the more complicated loss derivative at the
end of each update computation.</p>

<ul>
  <li>
    <p><strong>ReLU:</strong> the derivative of the element-wise ReLU is given by
\(\frac{\partial ReLU}{\partial z} = 0\) if \(x \leq\) 0 and 1 if x
\(&gt;\) 0. As a consequence, we propagate only the
gradient for the neurons with positive activations back to the
previous layer.</p>
  </li>
  <li>
    <p><strong>Linear layer:</strong> Let \(z = W x\) be a linear layer with one input
channel and \(n\) elements, \(x \in \mathbb{R}^{n \times 1}\), with
\(W \in \mathbb{R}^{m \times n}\) and let \(z, \delta 
 := \frac{\partial L}{\partial z} \in  \mathbb{R}^{m \times 1}\).
Then
\(\frac{\partial L}{\partial W} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial W} = \delta x^T\).
The same result, \(\delta X^T\), holds for linear layers with \(k\)
features or input channels, i.e. with
\(X \in \mathbb{R}^{n \times k}\) and
\(Z, \delta \in \mathbb{R}^{m \times k}\). Hence, <em>each</em> input
receives the input-specic respective weighted sum of the upstream
gradient of <em>all</em> neurons from the next layer.</p>
  </li>
  <li>
    <p><strong>Softmax:</strong> Let \(\hat{p}=\text{softmax}(z)\) denote the softmax
probabiltities with \(\hat{p}, z \in \mathbb{R}^{n \times m}\) and let
\(L(\hat{p},y)\) with \(y \in \mathbb{R}^{n \times 1}\) be the scalar
cross-entropy loss. We get
\(\frac{\partial L}{\partial z} = \frac{1}{n}(\hat{p} - y \otimes \vec{1}^m
)\), where \(\otimes\) denotes the outer product between two vectors.
If we add another logit layer \(l\in \mathbb{R}^{n \times m}\) below
the softmax, we obtain the following simple result:</p>
  </li>
</ul>

<p>\(\frac{\partial L}{\partial l_{i,j}} =
\begin{cases}
   \hat{p}_{i,j} &amp; \text{if i\)\neq\(j} \\
   \hat{p}_{i,j} - 1 &amp; \text{if i = j}.
\end{cases}\)</p>

<ul>
  <li>Intuitively, for each example \(i \in 1,...,n\), the gradients for
each parameter that flow from each predicted probability is
increased by the amount that the predicted probability differs from
the actual label (times \(1/n\)). Therefore, gradient descent in
particular updates parameters \(\theta\) towards the direction of the
gradients from the bad predictions.</li>
</ul>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = "Deep Learning Series",
  author  = "Stenzel, Tobias",
  year    = "2023",
  url     = "https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>

      <div class="PageNavigation">
        
          <a class="prev" href="/blog/2023/dl-backprop/">	⏮ 4. Backpropagation</a>
        
        
          <a class="next" href="/blog/2023/dl-cnn/">6. Convolutional Neural Networks ⏭</a>
        
      </div>
      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/references.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "tostenzel/tostenzel.github.io",
        "data-repo-id": "MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOA5PmLc4CTBt6",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "bottom",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
