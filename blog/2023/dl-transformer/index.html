<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>8. Transformer | Tobias  Stenzel</title>
    <meta name="author" content="Tobias  Stenzel">
    <meta name="description" content="⏮️ &lt;b&gt;&lt;i&gt;I recommend reading the RNN post first for the encoder-decoder architecture.&lt;/i&gt;&lt;/b&gt;">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9D&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://www.tobiasstenzel.com/blog/2023/dl-transformer/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>

    <!-- _layouts/post.html -->
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "8. Transformer",
      "description": "⏮️ <b><i>I recommend reading the RNN post first for the encoder-decoder architecture.</i></b>",
      "published": "March 8, 2023",
      "authors": [
        {
          "author": "Tobias Stenzel",
          "authorURL": "https://www.tobiasstenzel.com",
          "affiliations": [
            {
              "name": "N/A",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tobias </span>Stenzel</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
          <p class="PageNavigation">
          
            <a class="prev" href="/blog/2023/dl-rnn/">	⏮ 7. Recurrent Neural Networks</a>
          
          
            <a class="next" href="/blog/2023/trackformer/">Trackformer — Multi-Object Tracking with Transformers ⏭</a>
          
        </p>
        <br><br>
        <h1>8. Transformer</h1>
        <p>⏮️ <b><i>I recommend reading the RNN post first for the encoder-decoder architecture.</i></b></p>
        <p class="post-tags">
          <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
            ·  
            <a href="/blog/tag/neural-net-archetype">
              <i class="fas fa-hashtag fa-sm"></i> neural-net-archetype</a>  
              <a href="/blog/tag/attention">
              <i class="fas fa-hashtag fa-sm"></i> attention</a>  
              

        </p>
      </d-title>

      <d-byline></d-byline>

      <d-article>

        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#transformer">Transformer</a></div>
            <ul>
              <li><a href="#attention">Attention</a></li>
              <li><a href="#key-value-and-query">Key, value and query</a></li>
              <li><a href="#multi-head-attention">Multi-head attention</a></li>
              <li><a href="#transformer-encoder">Transformer encoder</a></li>
              <li><a href="#transformer-decoder">Transformer decoder</a></li>
              <li><a href="#the-complete-transformer-architecture">The complete transformer architecture</a></li>
              <li><a href="#complexity-comparison">Complexity comparison</a></li>
              
            </ul>
          </nav>
        </d-contents>

        <h2 id="transformer">Transformer</h2>

<p>In the last section we learned that RNNs have problems with learning
relations between the first parts of an input sequence with the output
and later parts of the input sequence. An architecture without this
structural problem is the transformer. The architecture was published by <d-cite key="vaswani_attention_2017"></d-cite> and applied to machine translation. We will
develop this model step-by-step, starting with its core component,
attention.</p>

<h3 id="attention">Attention</h3>

<p>Let us introduce this concept with an example. Figure
<a href="#fig:attention">7</a> shows an encoder-decoder network with attention similar to the previous
encoder-decoder RNN (Figure
<a href="https://www.tobiasstenzel.com/blog/2023/dl-rnn/#fig:encoder-decoder-rnn">6</a>). The core idea of attention is
defining the hidden state of the decoder-RNN as a function of every
hidden state from the encoder-RNN for every time period without
recursion. The result of the attention function is the context vector.
We will use this vector for every output element. In the specific
network in Figure <a href="#fig:attention">7</a>, the context vector is a function of both the
decoder states \(s\) and the encoder states \(h\). Further, it is
additionally concatenated with \(s\) to predict the output layer.</p>

<figure id="fig:attention">
<center><img src="/assets/img/dl-series/2h-attention.png" style="width:50%"></center>
</figure>
<p><b>Figure 7. Encoder-Decoder with attention.</b> In contrast to the
encoder-decoder RNN, the output layer is a function of the concatenation
of the hidden states and a time-dependent context vector (black boxes).
The main idea is that the context vector <span class="math inline"><em>c</em><sub><em>t</em></sub></span> is a function
of all hidden states <span class="math inline">{<em>h</em>}<sub><em>t</em> = 1</sub><sup><em>T</em></sup></span>
instead of only the last one (and the previous state <span class="math inline"><em>s</em><sub><em>t</em> − 1</sub></span>). This
function is called <em>attention</em> (red). The black box represents
vector concatenation. <span class="math inline"><em>c</em><sub>1</sub></span> is initialized with
<span class="math inline"><em>h</em><sub>4</sub></span>, <span class="math inline"><em>s</em><sub>1</sub></span> with arbitrary values,
and <span class="math inline"><em>o</em><sub>1</sub></span> is
discarded.<br>
<br></p>

<p>The attention function that returns the context vector for output \(y_t\)
wit attention scores for each input is given by the following
expression:</p>

\[\begin{align}c_t = \sum_{i=1}^n \alpha_{t,i} h_i\end{align}\]

<p>\(\alpha_{t,i}\) is a softmax function of another function <em>score</em> that
measure how well output \(y_t\) and input \(x_i\) are aligned through the
encoder state \(h_i\):</p>

\[\begin{align}\alpha_{t,i} = \text{align}(y_t,x_i) = \frac{\exp \text{score}(s_{t-1}, h_i)}{\sum_{j=1}^n\exp\text{score} (s_{t-1}, h_j)}.\end{align}\]

<p>There are many different scoring functions <d-cite key="weng_attention_2018"></d-cite>. A
common choice is the scaled dot-product score
\((s_t, h_i)=\frac{s_t^T h_i}{\sqrt{d}}\), where \(d\) is the hidden state
dimension of both encoder and decoder states. Here, the alignment score
for one sequence element is given by a relative score of the dot-product
between the respective encoder hidden state and the current decoder
hidden state. We scale down the dot product to prevent vanishing
gradients from a pass to a softmax layer. After training the model, we
can analyze how much each output element depends on, or <em>attends</em> to,
each input. We do this by assembling a table with outputs as columns and
the output-specific alignment scores for each input as rows.</p>

<p>Another option for an encoder-decoder with attention is using a
self-attention mechanism to compute the context vector, for example,
with score \((h_j, h_i)=\frac{h_j^T h_i}{\sqrt{d}}\). We can use the
scores, for instance, in machine translation, to model how important the
previous words are for translating the current word in a sentence. Note
that we can execute many of these operations in parallel for the whole
input and output sequences using matrix operations.</p>

<p>Next, we will discuss an expansion of the attention mechanism and how it
is applied to the transformer’s encoder and decoder, separately.
Finally, we will look at the complete model, and how it replaces the
positional information from the encoder RNN in a simple way without any
recurrence.</p>

<h3 id="key-value-and-query">Key, value and query</h3>

<p>As we do not use recurrence of single sequence
elements anymore, let us denote the whole sequence of input embeddings
by \(X \in \mathbb{R}^{L \times D^{(x)}}\). \(L\) can either be the complete
input length \(T_x\) or later only a fraction of it. \(D^{(x)}\) is the
input embedding’s length. Let us denoted the sequence of output
embeddings by \(Y \in \mathbb{R}^{M \times D^{(y)}}\). The transformer
uses an extension of the attention mechanism, the multi-head attention,
as its core building block. The first step is that, instead of using the
softmax of the scaled dot-product between encoder states \(h\) and decoder
states \(s\) directly as in the last section, it uses the scaled
dot-product with two different input encodings,
\(K=XW^k \in \mathbb{R}^{L \times D_k}\) and
\(V=XW^v \in \mathbb{R}^{L \times D_v}\), and an output encoding
\(Q=YW^q \in \mathbb{R}^{M \times D_k}\), with
\(W^k \in \mathbb{R}^{D^{(x)} \times D_k}, W^q \in \mathbb{R}^{D^{(y)} \times D_k}\)
and \(W^v \in \mathbb{R}^{D^{(x)} \times D_v}\). Note that source and
target embeddings are projected linearly into the same space. We compute
attention with</p>

\[\begin{align}c(Q,K,V)=\text{Softmax}\big(\frac{QK^T}{\sqrt{n}}\big)V.\end{align}\]

<p>We call (K,V) key-value pairs and Q the query. Using the interpretation
of the dot product as a similarity measure, the context matrix shows the
(self-)similarity between the input and a representation of the input
that is weighted by its similarity to the output (so far). \(c(Q,K,V)\) is
a matrix because we now compute the attention scores for every target
(query) dimension at once. However, we mask embeddings for unseen target
elements in every period. In the transformer encoder, there is an
important layer where the queries are also source representations and in
the decoder, there is layer where keys and values are also target
representation.</p>

<h3 id="multi-head-attention">Multi-head attention</h3>

<p>Instead of computing the attention once, the
multi-head approach splits the three input matrices into smaller parts
and then computes the scaled dot-product attention for each part in
parallel. The independent attention outputs are then concatenated and
linearly transformed into the next layer’s input dimension. This allows
us to learn from different representations of the current information
simultaneously with high efficiency.</p>

\[\begin{align}\text{MultiHead}(X_q, X_k, X_v)= [ \text{head}_1;...;\text{head}_h ] W^o,\end{align}\]

<p>where \(\text{head}_i=\)Attention\((X_q W^q_i, X_k W^k_i, X_v W^v_i)\) and \(W_i^q  \in \mathbb{R}^{D^{(y)} \times D_v /H}\), \(W_i^k \in \mathbb{R}^{D^{(x)} \times D_k / H}\), \(W_i^v \in \mathbb{R}^{D^{(x)} \times D_v /H}\)
are matrices to map input embeddings of chunk size \(L \times D\) into
query, key and value matrices. \(W^o \in \mathbb{R}^{D_v \times D}\) is
the linear transformation in the output dimensions. These four weight
matrices are learned during training. Target self-attention and cross
attention layers compute outputs in \(\mathbb{R}^{M \times D}\), and
source self-attention calculates outputs in \(\mathbb{R}^{L \times D}\).</p>

<h3 id="transformer-encoder">Transformer encoder</h3>

<p>Figure <a href="#fig:transformer-encoder">8</a> depicts the encoder network. It
computes an input representation based on the self-attention mechanism
that allows it to locate particular pieces of information from a large
context at all positions.</p>

<figure id="fig:transformer-encoder">
<center><img src="/assets/img/dl-series/2i-transformer-encoder.png" style="width:33%"></center>
</figure>
<p><b>Figure 8. Transformer encoder.</b> In the original form, the encoder is a
stack of <span class="math inline"><em>N</em> = 6</span> identical
layers but with different parameters. It consists of two similar
components. The first sub-layer starts with a multi-head
<em>self</em>-attention layer (orange) and the second with a
<em>point-wise</em> fully-connected feed forward network (blue).
Point-wise means that the same weights are applied to each input
element. Afterwards, the respective previous input vector is added to
both outputs and the results are normalized by the normalized residual
layers (yellow). Crucially, in the self-attention layer, the queries are
also functions of the input embeddings. Adapted from <d-cite key="vaswani_attention_2017"></d-cite>.</p>

<h3 id="transformer-decoder">Transformer decoder</h3>

<p>Figure <a href="#fig:transformer-decoder">9</a> shows the decoder network. It is
able to retrieve relevant information from the encoded source
representation to compute feature representations for generating the new
target sequence in autoregressive fashion. The key component is the
multi-head <em>cross</em>-attention layer (in contrast to the other multi-head
<em>self</em>-attention blocks).</p>

<figure id="fig:transformer-decoder">
<center><img src="/assets/img/dl-series/2j-transformer-decoder.png" style="width:33%"></center>
</figure>
<p><b>Figure 9. Transformer decoder. </b> In the original form, the encoder is a
stack of <span class="math inline"><em>N</em> = 6</span> identical
layers. It first encodes the output sequence in the masked multi-head
<em>self</em>-attention layer (orange). The masked elements are the
target representations that are not generated so far. Next, it passes
these target representations as queries to the multi-head attention
layer (orange) together with the output input representations as keys
and values. Finally, the results pass through a fully-connected feed
forward network (blue). Each of these three layers is subsequently
transformed by a normalized residual layer (yellow). Adapted from <d-cite key="vaswani_attention_2017"></d-cite>.</p>

<h3 id="the-complete-transformer-architecture">The complete transformer architecture</h3>

<p>Figure <a href="#fig:transformer-complete">10</a> shows the complete architecture. It
has the following properties:</p>

<ul>
  <li>
    <p><strong>Inductive bias for self-similarity:</strong> The self-attention layers
allow the model to detect reoccuring themes independent of their
distances from each other. This works well in many applications
becaue reoccurence is an important pattern in numerous real-world
domains.</p>
  </li>
  <li>
    <p><strong>Expressive forward pass:</strong> The transformer interacts all elements
of the input and output with themselves and each other in relatively
simple and direct connections. This allows the model to learn many
algorithms in just a few steps.</p>
  </li>
  <li>
    <p><strong>Wide and shallow compute graph:</strong> Due to the residual layers and
the matrix products in the attention layers the compute graph is
wide and shallow. This makes forward and backward passes fast on
parallel hardware. Furthermore, together with layer normalization
and dot product scaling, this mitigates vanishing or exploding
gradients in backpropagation.</p>
  </li>
</ul>

<figure id="fig:transformer-complete">
<center><img src="/assets/img/dl-series/2k-transformer-complete.png" style="width:66%"></center>
</figure>
<p><b>Figure 10. The complete transformer.</b> In the original form, both the
source and target sequence are passed to embedding layers to produce a
vector of length <span class="math inline"><em>D</em> = 512</span> for
every element. To preserve the ordering information of the inputs, we
add a respective sinusoidal positional encoding vector to every
embedding. To compute the probabilities for each element in the output
space at every position, we pass the decoder output through a linear and
a softmax layer. Adapted from <d-cite key="vaswani_attention_2017"></d-cite>.</p>
<h3 id="complexity-comparison">Complexity comparison</h3>

<p>Let us conclude this chapter by comparing the complexities of the three main architectures in deep learning. Table
<a href="#fig:comparison">2</a> shows the differences in the number of FLOPS (floating point operations)
for the main units in the tree main architectures that we have discussed
so far. We observe that attention scales better than the other units if
the length of the input sequence is much smaller than the depth of each
elements embeddings. This applies to tasks like machine translation in <d-cite key="vaswani_attention_2017"></d-cite> or question-answering. However, for direct
applications to image data, this property does not hold. For instance,
the length from a CIFAR image equals \(32 \cdot 32 \cdot 3 = 3072\).
Therefore, applying a transformer to image data in order to profit from
its advantages requires architectures that reduce the input length
beforehand.</p>

<figure id="fig:comparison">
<center><img src="/assets/img/dl-series/complexity-comparison.png" style="width:55%"></center>
</figure>
<p><b>Table 2. Comparison of computation complexity between models.</b> length refers
to the number of elements in the input sequence and dim to the
embedding depth for each element. Entries adapted from <d-cite key="vaswani_transformers_2021"></d-cite>.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = "Deep Learning Series",
  author  = "Stenzel, Tobias",
  year    = "2023",
  url     = "https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>

      <div class="PageNavigation">
        
          <a class="prev" href="/blog/2023/dl-rnn/">	⏮ 7. Recurrent Neural Networks</a>
        
        
          <a class="next" href="/blog/2023/trackformer/">Trackformer — Multi-Object Tracking with Transformers ⏭</a>
        
      </div>
      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/references.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "tostenzel/tostenzel.github.io",
        "data-repo-id": "MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOA5PmLc4CTBt6",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "bottom",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
