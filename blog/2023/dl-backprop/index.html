<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>4. Backpropagation | Tobias  Stenzel</title>
    <meta name="author" content="Tobias  Stenzel">
    <meta name="description" content="üí° &lt;b&gt;&lt;i&gt; Includes a nice detail about why we don't 'frontpropagate'. ;-) &lt;/i&gt;&lt;/b&gt;">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9D&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://www.tobiasstenzel.com/blog/2023/dl-backprop/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>

    <!-- _layouts/post.html -->
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "4. Backpropagation",
      "description": "üí° <b><i> Includes a nice detail about why we don't 'frontpropagate'. ;-) </i></b>",
      "published": "March 4, 2023",
      "authors": [
        {
          "author": "Tobias Stenzel",
          "authorURL": "https://www.tobiasstenzel.com",
          "affiliations": [
            {
              "name": "N/A",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tobias¬†</span>Stenzel</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
          <p class="PageNavigation">
          
            <a class="prev" href="/blog/2023/dl-optimization/">	‚èÆ 3. Optimization</a>
          
          
            <a class="next" href="/blog/2023/dl-fnn/">5. Feedforward Neural Networks ‚è≠</a>
          
        </p>
        <br><br>
        <h1>4. Backpropagation</h1>
        <p>üí° <b><i> Includes a nice detail about why we don't 'frontpropagate'. ;-) </i></b></p>
        <p class="post-tags">
          <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
          ¬† ¬∑ ¬†
            <a href="/blog/tag/backprop">
              <i class="fas fa-hashtag fa-sm"></i> backprop</a> ¬†
              <a href="/blog/tag/reverse-accumulation">
              <i class="fas fa-hashtag fa-sm"></i> reverse-accumulation</a> ¬†
              <a href="/blog/tag/compute-graph">
              <i class="fas fa-hashtag fa-sm"></i> compute-graph</a> ¬†
              

        </p>
      </d-title>

      <d-byline></d-byline>

      <d-article>

        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#backpropagation">Backpropagation</a></div>
            <ul>
              <li><a href="#toy-example">Toy example</a></li>
              <li><a href="#vector-valued-intermediate-functions">Vector-valued intermediate functions</a></li>
              <li><a href="#reverse-accumulation">Reverse accumulation</a></li>
              <li><a href="#implementation">Implementation</a></li>
              
            </ul>
          </nav>
        </d-contents>

        <h2 id="backpropagation">Backpropagation</h2>

<p>We learned that we can find a mapping \(f \in \mathcal{F}\) that maps
features X to outcome Y consistent with the data by minimizing the cost
function with repeated gradient evaluations using a gradient descent
optimizer.</p>

<p>We compute the gradient with backpropagation. This algorithm allows us
to efficiently compute gradients of functions that 1.) are
scalar-valued, 2.) have many input parameters, and 3.) can be decomposed
into simpler intermediate functions that are differentiable. The
mathematical formula of backpropagation is inspired by the third
property. It is the gradient computation via recursive applications of
the chain rule from calculus. The algorithmic order is inspired by the
first two properties. The idea is to efficiently compute the resulting
derivative starting from the intermediate functions on the parameter
instead of the cost function side.</p>

<h3 id="toy-example">Toy example</h3>

<p>Recall that we would like to compute the gradient of
cost function \(C(\theta)\), which takes not only parameters \(\theta\) but
also multiple examples \((x_i, y_i)\) as input. Specifically, our
first-order optimizer requires the gradient of the cost function with
respect to the model parameters \(\nabla_{\theta}C(\theta)\) in order to
update the parameters \(\theta\). Let us consider the following example.
Let \(C(\theta_1, \theta_2)=\theta_1 \theta_2 + \tanh (\theta_1)\) be the
cost function of a neural network with parameters \(\theta_1\) and
\(\theta_2\). Figure <a href="#fig:toy_graph">1</a> shows that we can view this equation as a
computational graph with cost function \(C\) as root and parameters as
leaf nodes. We introduce intermediate variables to write \(C\) as a
sequence of the intermediate functions \(z_3=\tanh (\theta_1)\),
\(z_4=\theta_1 \theta_2\), \(z_5=z_3+z_4\), and \(C(\theta)=z_5\). For ease of
notation, we will later also write \(\theta_1,\theta_2\) and \(C(\theta)\)
as \(z_1, z_2\) and \(z_6\). We can write the gradient of the cost function
with respect to each parameter as a combination of the gradients of its
parent nodes using the chain rule from calculus and the fact that
multiple occurrences of a term add up in its derivative. This is shown
in Equation <a href="#eq:dtheta_1">1</a> and
<a href="#eq:dtheta_2">2</a>:</p>

\[\begin{align}
\frac{\partial C}{\partial \theta_1}=\frac{\partial C}{\partial z_5} \bigg( \frac{\partial z_5}{\partial z_3} \frac{\partial z_3}{\partial \theta_1} + \frac{\partial z_5}{\partial z_4} \frac{\partial z_4}{\partial \theta_1} \bigg) = 1 - \tanh ( \theta_1)^2 + \theta_2,
\label{eq:dtheta_1}\end{align}\]

\[\begin{align}
\label{eq:dtheta_2}
\quad \frac{\partial C}{\partial \theta_2}=\frac{\partial C}{\partial z_5}\frac{\partial z_5}{\partial z_4}\frac{\partial z_4}{\partial \theta_2}   =\theta_1.\end{align}\]

<figure id="fig:toy_graph">
<center><img src="/assets/img/dl-series/compute-graph.png" style="width:75%"></center>
</figure>
<p><b>Figure 1. Computational graph for a toy example of a neural network‚Äôs
forward pass without vector-valued intermediate functions, data,
regularization and loss.</b> The function given by the graph is <span class="math inline"><em>C</em>(<em>Œ∏</em><sub>1</sub>,<em>Œ∏</em><sub>2</sub>)‚ÄÑ=‚ÄÑ<em>Œ∏</em><sub>1</sub><em>Œ∏</em><sub>2</sub>‚ÄÖ+‚ÄÖtanh‚ÄÜ(<em>Œ∏</em><sub>1</sub>)</span>.
Intermediate functions that are relevant for deriving the gradient with
the chain rule are denoted by <span class="math inline"><em>z</em><sub><em>i</em></sub></span> with <span class="math inline"><em>i</em>‚ÄÑ‚àà‚ÄÑ{1,‚ÄÜ‚Ä¶.,‚ÄÜ6}</span>. The edges along
which the intermediate evaluations move forth during the forward pass
and along which the gradients move back during the backward pass are
denoted by <span class="math inline"><em>e</em><sub><em>i</em>,‚ÄÜ<em>j</em></sub></span>
with <span class="math inline"><em>i</em>,‚ÄÜ<em>j</em>‚ÄÑ‚àà‚ÄÑ{1,‚ÄÜ‚Ä¶,‚ÄÜ6}</span>. We can
view two edges leaving one node as a a shortcut depiction for applying
the <span class="math inline"><code>duplicate</code></span> or fork
operation along both edges.</p>

<h3 id="vector-valued-intermediate-functions">Vector-valued intermediate functions</h3>
<p>The toy example deviates from
a realistic neural network application in a few aspects. One of these
aspects is that we usually consider vectors of large parameter groups.
Another aspect is that the intermediate functions that transform the
parameter vector \(\theta\) step-by-step are usually vector-valued (unlike
the final cost function). Apart of confluence operators like "+" and
"*" that usually combine different parameter groups, the final
gradients can be written as a sequence of "local" gradient
computations. Due to vector-valued functions and the number of
parameters, these computations are written as matrix multiplications. We
look at this extension more formally. To this end, let \(z_0\) be the
input vector which we transform through a series of functions be
\(z_i = f_i(z_{i-1})\) where \(i=1,...,k\) and only the last \(z_i\) can be
scalar. Assuming that the functions \(f_i\) are once differentiable, we
can compute the Jacobian matrix \(\frac{\partial z_i}{\partial z_{i-1}}\)
for all intermediate functions. This will give us the values of the
first derivative of every output dimension of \(z_{i}\) depending on each
single input dimension of \(z_{i-1}\). From the multivariable chain rule,
we obtain the result that the gradient of our final function with
respect to input vector equals the product of all intermediate
Jacobians:
\(\frac{\partial z_k}{\partial z_{0}} = \prod_{i=1}^k \frac{\partial z_i}{\partial z_{i-1}}\).</p>

<h3 id="reverse-accumulation">Reverse accumulation</h3>
<p>We can compute
\(\frac{\partial C}{\partial \theta}\) in two different ways. In this
section we learn that one approach is much more efficient. The reason is
the the structure of our problem: we minimize a scalar-valued function
with a large number of parameters. One approach is to compute the
gradient from parameters to output:
\(\frac{\partial C}{\partial \theta} = \frac{\partial z_k}{\partial z_{k-1}} \cdot ... \cdot \frac{\partial z_2}{\partial z_1}\).
This is called forward accumulation. The other approach, reverse
accumulation, is to compute the gradients from output to parameters:
\(\frac{\partial C}{\partial \theta} = \frac{\partial z_2}{\partial z_1}\cdot   ... \cdot \frac{\partial z_k}{\partial z_{k-1}}\).
The first approach is less efficient but more intuitive. It is more
intuitive because the derivatives can be computed in sync with the
evaluation steps. This makes it easier to think about how confluence
operations like "\(+\)", "\(*\)", the fork operation <code class="language-plaintext highlighter-rouge">duplicate</code>, or
filter operations like <code class="language-plaintext highlighter-rouge">max</code> or <code class="language-plaintext highlighter-rouge">average</code> transform the gradients. We
will later see that it is crucial to use these operations in a careful
way in the architecture design of neural networks because they can have
large effects on the model performance. In reverse accumulation, we fix
the dependent variable to be differentiated and compute the derivative
with respect to each intermediate function recursively. Table
<a href="#tab:reverse">1</a> shows
how we can calculate the two gradients of our toy example step-by-step
with this method. There are two important things to note. First,
computing the backward operations corresponding to the forward
operations is prone to error. This is one reason why this should be done
automatically by a graph-based computer program. Second, we can re-use
our intermediate computations \(\bar{z}_3\) to \(\bar{z}_5\) for both
gradients based only on one evaluation of the cost function
\(C(\theta) = z_6\). In realistic neural network applications, we are able
to re-use a large number of intermediate results based on only one
forward evaluation for an even larger number of input parameters. In
contrast, with forward-mode accumulation, computing the gradient
requires to evaluate each intermediate function with the whole parameter
vector. Although re-using the intermediate gradients requires more
storage, the reverse-mode accumulation strategy is much more efficient
for functions like neural networks where the number of output values is
much smaller than the number of input values.</p>

<figure id="tab:reverse">
<center><img src="/assets/img/dl-series/table-backprop.png" style="width:80%"></center>
</figure>

<p><b>Table 1. Gradient computations in reverse accumulation mode for toy
example <span class="math inline"><em>C</em>(<em>Œ∏</em><sub>1</sub>,<em>Œ∏</em><sub>2</sub>)‚ÄÑ=‚ÄÑ<em>Œ∏</em><sub>1</sub><em>Œ∏</em><sub>2</sub>‚ÄÖ+‚ÄÖtanh‚ÄÜ<em>Œ∏</em><sub>1</sub></span>.</b>
The left column shows the evaluation and the right column depicts the
derivation steps. <span class="math inline"><em>zÃÑ</em></span> denotes
the derivative of the cost function with respect to an intermediate
expression, i.e. <span class="math inline">\(\frac{\partial C}{\partial
z}\)</span>. In the backward pass, we first evaluate the scalar-cost
function and use its derivative with respect to <span class="math inline"><em>z</em><sub>5</sub></span> at the top of the
graph. Here we assume <span class="math inline"><em>zÃÑ</em><sub>5</sub></span> equals <span class="math inline">1</span>. In the next step, we combine the chained
gradients of the intermediate expressions according to the respective
backward functions of the <code>duplicate</code> and the confluence
operations ‚Äú<span class="math inline">+</span>‚Äù and ‚Äú<span class="math inline">*</span>‚Äù from top to bottom. Addition distributes
the upstream gradient down to all its inputs. Multiplication passes the
upstream gradient multiplied with the <em>other</em> input back. I.e.,
<span class="math inline">\(\frac{\partial (z_3 + z_4)}{\partial z_{4}} =
1, \frac{\partial (z_1 * z_2)}{\partial z_{2}} = z_1\)</span>, and the
backward function of <span class="math inline">(<em>z</em><sub>1,‚ÄÜ3</sub>,<em>z</em><sub>1,‚ÄÜ4</sub>)‚ÄÑ=‚ÄÑ<code>duplicate</code>(<em>z</em><sub>1</sub>)</span>
equals <span class="math inline">\(\frac{\partial z_3}{\partial z_{1}} +
\frac{\partial z_4}{\partial z_{1}}\)</span>, where <span class="math inline"><em>z</em><sub>1,‚ÄÜ3</sub></span> and <span class="math inline"><em>z</em><sub>1,‚ÄÜ4</sub></span> denote abbreviated
nodes from the implicit duplication operation in the third and fourth
evaluation step.</p>

<h3 id="implementation">Implementation</h3>
<p>In our toy example, we have already discovered that
it is easier to think of the evaluation and differentiation of a neural
<em>network</em> in terms of a directed graph of operations instead of a linear
sequence of function applications. In this graph, the nodes stand for
differentiable operations that take a number of vectors from its
incoming edges, transforms and potentially interrelates them, and sends
the result to the next nodes along its outgoing edges. The graph
abstraction translates to most implementations of the backpropagation
algorithm. A <em>Graph</em> object keeps the connections (e.g. <code class="language-plaintext highlighter-rouge">duplicate</code> or
\(+\) ) between the nodes and the collection of operations (e.g. \(\tanh\)).
Both Node and Graph objects implement a <code class="language-plaintext highlighter-rouge">forward()</code> and a <code class="language-plaintext highlighter-rouge">backward()</code>
method. The Graph‚Äôs <code class="language-plaintext highlighter-rouge">forward()</code> calls the Nodes‚Äô <code class="language-plaintext highlighter-rouge">forward()</code> methods in
their topological order. With that, every Node computes its operation on
its input, and the Graph sends it to the next Node. The Graph‚Äôs
<code class="language-plaintext highlighter-rouge">backward()</code> iterates over the nodes in reverse topological order and
calls their <code class="language-plaintext highlighter-rouge">backward()</code> methods. In the backward pass, each Node is
given the gradient of the cost function with respect to its output and
it returns the "chained" gradients with respect to all its inputs.
"Chained" means that the Node multiplies the Jacobian that it received
from its parent nodes with its own local Jacobian. The Graph sends the
resulting product to the Nodes‚Äô children and the process repeats until
the recursion ends at the last computation which includes the data and
the current parameter values. At last, the optimizer updates the
parameter vector based on the final gradient (Step 3 in Algorithm <a href="https://www.tobiasstenzel.com/blog/2023/dl-optimization/#first-order-methods">1</a>. Note
that the implemented compute graph for Figure
<a href="#fig:toy_graph">1</a> would in practice be larger and rudimentary because we can decompose
many operations, like divide or subtract in tanh, into more basic
operations.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = "Deep Learning Series",
  author  = "Stenzel, Tobias",
  year    = "2023",
  url     = "https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>

      <div class="PageNavigation">
        
          <a class="prev" href="/blog/2023/dl-optimization/">	‚èÆ 3. Optimization</a>
        
        
          <a class="next" href="/blog/2023/dl-fnn/">5. Feedforward Neural Networks ‚è≠</a>
        
      </div>
      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/references.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "tostenzel/tostenzel.github.io",
        "data-repo-id": "MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOA5PmLc4CTBt6",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "bottom",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
