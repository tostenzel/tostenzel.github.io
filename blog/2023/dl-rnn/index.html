<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>7. Recurrent Neural Networks | Tobias  Stenzel</title>
    <meta name="author" content="Tobias  Stenzel">
    <meta name="description" content="üß± &lt;b&gt;&lt;i&gt;The encoder-decoder RNN is quite useful for understanding the transformer.&lt;/b&gt;&lt;/i&gt;">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9D&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://www.tobiasstenzel.com/blog/2023/dl-rnn/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>

    <!-- _layouts/post.html -->
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "7. Recurrent Neural Networks",
      "description": "üß± <b><i>The encoder-decoder RNN is quite useful for understanding the transformer.</b></i>",
      "published": "March 7, 2023",
      "authors": [
        {
          "author": "Tobias Stenzel",
          "authorURL": "https://www.tobiasstenzel.com",
          "affiliations": [
            {
              "name": "N/A",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tobias¬†</span>Stenzel</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
          <p class="PageNavigation">
          
            <a class="prev" href="/blog/2023/dl-cnn/">	‚èÆ 6. Convolutional Neural Networks</a>
          
          
            <a class="next" href="/blog/2023/dl-transformer/">8. Transformer ‚è≠</a>
          
        </p>
        <br><br>
        <h1>7. Recurrent Neural Networks</h1>
        <p>üß± <b><i>The encoder-decoder RNN is quite useful for understanding the transformer.</i></b></p>
        <p class="post-tags">
          <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
          ¬† ¬∑ ¬†
            <a href="/blog/tag/dl-fundamentals">
              <i class="fas fa-hashtag fa-sm"></i> dl-fundamentals</a> ¬†
              <a href="/blog/tag/sneural-net-archetype">
              <i class="fas fa-hashtag fa-sm"></i> sneural-net-archetype</a> ¬†
              <a href="/blog/tag/backprop">
              <i class="fas fa-hashtag fa-sm"></i> backprop</a> ¬†
              <a href="/blog/tag/exploding-vanishing-gradients">
              <i class="fas fa-hashtag fa-sm"></i> exploding-vanishing-gradients</a> ¬†
              

        </p>
      </d-title>

      <d-byline></d-byline>

      <d-article>

        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#recurrent-neural-networks">Recurrent Neural Networks</a></div>
            <ul>
              <li><a href="#vanilla-neural-networks">Vanilla Neural Networks</a></li>
              <li><a href="#the-encoder-decoder-architecture">The Encoder-Decoder Architecture</a></li>
              <li><a href="#encoder-decoder-rnns">Encoder-Decoder RNNs</a></li>
              <li><a href="#the-exploding-and-vanishing-gradient-problem">The Exploding and Vanishing Gradient Problem</a></li>
              
            </ul>
          </nav>
        </d-contents>

        <h1 id="recurrent-neural-networks">Recurrent Neural Networks</h1>

<p>Many tasks require input or output spaces that contain sequences. For example, in translation programs, words are often encoded as sequences of one-hot vectors. Each one-hot vector has a 1 at the position of an integer mapped to a word in a fixed vocabulary.
A simple recurrent neural network (RNN) processes a sequence of vectors \(\{x_1, \dots, x_T\}\) using the recurrence formula:</p>

\[\begin{align}
h_t = f_\theta(h_{t-1}, x_t).
\end{align}\]

<ul>
  <li>\(f\) is a function (detailed below) that uses the same parameters \(\theta\) at every time step. This implies parameter sharing and introduces an inductive bias for modeling sequences.</li>
  <li>We can interpret the hidden vector \(h_t\) as a summary of all previous \(x\)-vectors.</li>
  <li>A common initialization is \(h_0 = \vec{0}\).</li>
</ul>

<p>We can define \(f\) according to <strong>three criteria</strong>:</p>

<ul>
  <li>
    <p><strong>1. Input‚ÄìOutput Space</strong><br>
  Depending on the task, we may want to handle ‚Äúone-to-many,‚Äù ‚Äúmany-to-one,‚Äù or ‚Äúmany-to-many‚Äù mappings between the inputs and outputs.</p>
  </li>
  <li>
<strong>2. Order of Processing</strong><br>
  The nature of the data may require:
    <ul>
      <li>Predicting \(y_t\) immediately after seeing \(x_t\).</li>
      <li>Predicting the entire output sequence \(\{y_1, \dots, y_T\}\) only after reading the full input sequence \(\{x_1, \dots, x_T\}\) in one shot.</li>
      <li>Predicting the output sequence bit by bit after reading the complete input in order to also include the output elements that where generated so far as information for the next prediction.</li>
    </ul>
  </li>
  <li>
<strong>3. Handling Long Sequences</strong><br>
  As sequences grow longer, we encounter <strong>drawbacks</strong> (e.g., vanishing/exploding gradients).  We will address this point later in the section.</li>
</ul>

<p>To illustrate how RNNs work, let us look at two specific examples. The first example is a vanilla RNN for predicting the next character in a text prompt. The second example is a token level encoder-decoder RNN for translation that can also work well with longer text. We will prepare this example with a longer explanation of the encoder-decoder architecture in general. Understanding this concept is crucial for understanding modern transformer based models. Finally, we look at the backward pass of the vanilla RNN to understand fundamental problems of RNNs in a deeper way.</p>

<blockquote>
  <p><strong><em>Tokenization</em></strong></p>

  <p>The examples here use sequences of characters or words. However, in realistic applications text is mapped to a sequence of elements that are instances of a fixed vocabulary. This mapping is achieved by neural networks called tokenizers. Tokens can be anything from words, subword segments, or individual characters.</p>
</blockquote>

<hr>

<h2 id="vanilla-recurrent-neural-networks">Vanilla Recurrent Neural Networks</h2>

<p>Vanilla RNNs use a simple recurrence defined by:</p>

\[\begin{align}
h_t &amp;= \phi \big ( W\begin{bmatrix}
       x_{t} \\
       h_{t-1} 
     \end{bmatrix}
     \big ).
\label{eq:vanilla-rnn}
\end{align}\]

<ol>
  <li>We concatenate the current input \(x_t\) and the previous hidden state \(h_{t-1}\).</li>
  <li>We transform them linearly (by multiplying with \(W\)).</li>
  <li>We apply a non-linear activation \(\phi\).</li>
</ol>

<p>In vector form, this is equivalent to:</p>

<p>\(\)\begin{align}
h_t = \phi \big ( W_{xh}x_t + W_{hh}h_{t-1} \big ).
\(\)\end{align}</p>

<ul>
  <li>\(W_{xh}\) and \(W_{hh}\) are concatenated horizontally to form \(W\).</li>
  <li>If \(x_t \in \mathbb{R}^D\) and \(h_t \in \mathbb{R}^H\), then<br>
\(\,W_{xh} \in \mathbb{R}^{H \times D}\)<br>
\(\,W_{hh} \in \mathbb{R}^{H \times H}\)<br>
\(\,W \in \mathbb{R}^{\,H \times (D+H)}.\)</li>
</ul>

<p>Effectively, a vanilla RNN models the new hidden state \(h_t\) at each time step as a linear function of the previous hidden state \(h_{t-1}\), and the current input \(x_t\). Both inputs are passed through a non-linearity \(\phi\).</p>

<p><strong>Example: Character-level Language Model</strong></p>

<p>In a classification task‚Äîe.g., predicting the next character in a text prompt‚Äîone can apply a Softmax to a linear transformation of the hidden state at every time step:</p>

<p>\(\)\begin{align}
o_t = W_{ho}\,h_t,
\quad
\hat{y}_t = \text{Softmax}(o_t),
\end{align}\(\)</p>

<p>to predict the next character‚Äôs one-hot encoding. The linear transformation projects the hidden state into the alphabet space and the softmax function converts the real vector into the unit space. Then, we can use the character with the highest number as prediction. This is illustrated by Figure
<a href="#fig:vanilla-rnn">5</a>.</p>

<figure id="fig:vanilla-rnn">
<center><img src="/assets/img/dl-series/2f-vanilla-rnn.png" style="width:95%"></center>
</figure>
<p><b>Figure 5. Vanilla RNN as character-level language model.</b> The left side
shows the <em>unrolled RNN</em>. The vocabulary has four characters and
the training sequence is ‚Äúhello‚Äù. Each letter is represented by a 1-hot
encoding (yellow) and the RNN predicts the encodings of the next letter
(green) at each time step. The RNN has a hidden state with three
dimensions (red). The output has four dimensions. The dimensions are the
logits for the next character. They are the softmax of a linear
transformation of the hidden states. During supervised learning, the
model will be trained to increase (decrease) the logits of the correct
(false) characters. The right side shows the <em>rolled-up RNN</em>. The
graph has a cycle that shows that the same hidden states are shared
across time and that the architecture is the same for each
step.</p>

<hr>

<h2 id="the-encoderdecoder-architecture">The Encoder‚ÄìDecoder Architecture</h2>

<p>An encoder‚Äìdecoder model is build for applications that involve generating a sequence of potentially different length from another sequence. One such application is translation or question answering. In its most general case, the model transforms an input sequence
\(\begin{align}
  X = \{x_1, x_2, \dots, x_{T_x}\}
\end{align}\)
into an output sequence
\(\begin{align}
  \hat{Y} = \{\hat{y}_1, \hat{y}_2, \dots, \hat{y}_{T_y}\}.
\end{align}\)</p>

<p>It is structured into two main components:</p>

<p><strong>Encoder</strong><br>
Produces an internal representation \(R\) of \(X\). Formally:</p>

<p>\(\begin{align}
  R = \mathrm{Encoder}(X).
\end{align}\)</p>
<ul>
  <li>In some architectures, \(R\) is a <em>single vector</em> (often called a context).</li>
  <li>In others (e.g., with attention), \(R\) is a <em>sequence of states</em> \(\{h_1,\dots,h_{T_x}\}\).</li>
  <li>Or it may be multiple stacked layers of representations (e.g., in a Transformer).</li>
</ul>

<p><strong>Decoder</strong><br>
Generates each output element \(\hat{y}_t\) (for \(t=1,\dots,T_y\)) one at a time, with an autoregressive dependence on its own partial output and on the encoder representation \(R\). Concretely, we can think of the decoder in three submodules:</p>

<ul>
  <li>
    <p><strong>(a) Self-encoding of partial outputs</strong></p>

    <p>The decoder keeps a hidden state \(s_t\) that ‚Äúremembers‚Äù what it has generated so far:</p>

\[\begin{align}
    s_t^{(\mathrm{self})} = \mathrm{SelfEnc}\bigl(s_{t-1}^{(\mathrm{self})},\, \hat{y}_{t-1}\bigr).
  \end{align}\]

    <p>This submodule accumulates the history \(\{\hat{y}_1, \dots, \hat{y}_{t-1}\}\) into \(s_t^{(\mathrm{self})}\).</p>
  </li>
  <li>
<strong>(b) Cross-encoding (referencing the encoder)</strong>
The decoder also needs to incorporate the <strong>encoder‚Äôs representation</strong> \(R\). We denote this step by:
\(\begin{align}
    s_t^{(\mathrm{cross})} = \mathrm{CrossEnc}\bigl(s_t^{(\mathrm{self})},\, R\bigr).
  \end{align}\)
    <ul>
      <li>In a simple setting, \(\mathrm{CrossEnc}\) might just copy or concatenate \(s_t^{(\mathrm{self})}\) with \(R\).</li>
      <li>In an attention-based setting, \(\mathrm{CrossEnc}\) might compute a new context \(\tilde{c}_t\) from \(\{h_1,\dots,h_{T_x}\}\) and combine that with \(s_t^{(\mathrm{self})}\).</li>
    </ul>
  </li>
  <li>
<strong>(c) Output module</strong>
Finally, the decoder uses a function \(\mathrm{Output}\) to map \(s_t^{(\mathrm{cross})}\) to a distribution over possible next tokens:
\(\begin{align}
    \hat{y}_t = \mathrm{Output}\bigl(s_t^{(\mathrm{cross})}\bigr).
  \end{align}\)
  Typically, \(\mathrm{Output}\) is a linear layer plus a softmax.</li>
</ul>

<p>Altogether, the generic decoder step might look like:</p>

\[\begin{align}
  \begin{aligned}
    s_t^{(\mathrm{self})} &amp;= \mathrm{SelfEnc}\bigl(s_{t-1}^{(\mathrm{self})},\, \hat{y}_{t-1}\bigr),\\
    s_t^{(\mathrm{cross})} &amp;= \mathrm{CrossEnc}\bigl(s_t^{(\mathrm{self})},\, R\bigr),\\
    \hat{y}_t &amp;= \mathrm{Output}\bigl(s_t^{(\mathrm{cross})}\bigr).
  \end{aligned}
\end{align}\]

<p>where \(\hat{y}_{t-1}\) is the decoder‚Äôs previous prediction.</p>

<p>In many actual implementations, submodules (a) and (b) may be fused into a single recurrent or feed-forward block. The above separation is conceptual, highlighting that the decoder is simultaneously <em>encoding the partial outputs</em> and <em>referencing the encoder representation</em>.</p>

<blockquote>
  <p><strong>Dealing with Sequences of Different Lengths</strong><br>
To allow different input and output lengths, we introduce two special tokens:</p>
  <ul>
    <li>Start-of-sentence: \(\langle sos \rangle\)</li>
    <li>End-of-sentence: \(\langle eos \rangle\)</li>
  </ul>

  <p>The encoder stops reading the input when it encounters \(\langle eos \rangle\). The decoder starts generating with \(\langle sos \rangle\) as \(y_1\) and stops when it generates \(\langle eos \rangle\).</p>
</blockquote>

<hr>

<h2 id="encoder-decoder-rnns">Encoder-Decoder RNNs</h2>

<p>We now specialize this generic view to an RNN-based encoder‚Äìdecoder. We will (i) specify a fairly general RNN recurrence form, and then (ii) show how it can reduce to simple linear transformations with a standard nonlinearity.</p>

<h3 id="general-rnn-recurrence-for-encoder-and-decoder">General RNN Recurrence for Encoder and Decoder</h3>

<ul>
  <li>
    <p><strong>Encoder RNN</strong>:<br>
\(\begin{align}
  h_t = f_{\mathrm{enc}}(h_{t-1},\, x_t), 
  \quad
  t=1,\dots,T_x,
\end{align}\)
with some initialization \(h_0\). We take the final state \(h_{T_x}\) as
\(\begin{align}
  R = h_{T_x},
\end{align}\)
a single ‚Äúcontext vector.‚Äù</p>
  </li>
  <li>
    <p><strong>Decoder RNN</strong>: merges <strong>self-encoding</strong> and <strong>cross-encoding</strong> into a single update:
\(\begin{align}
  s_t = f_{\mathrm{dec}}\bigl(s_{t-1},\, \hat{y}_{t-1},\, R\bigr),
  \quad
  t=1,\dots,T_y,
\end{align}\)
and then
\(\begin{align}
  \hat{y}_t = \mathrm{Output}\bigl(s_t\bigr).
\end{align}\)
This single function \(f_{\mathrm{dec}}\) effectively does:</p>
    <ol>
      <li>Encode the partial outputs via \(\hat{y}_{t-1}\) and \(s_{t-1}\).</li>
      <li>Cross-encode the encoder‚Äôs context \(R\).</li>
    </ol>
  </li>
</ul>

<h3 id="vanilla-encoder-decoder-rnn">Vanilla Encoder-Decoder RNN</h3>

<p>In the simplest ‚Äúvanilla‚Äù RNN, the encoder might be:</p>

<p>\(\begin{align}
  h_t = \phi\Bigl(
    W_{\mathrm{enc}}^{(hh)}\,h_{t-1}
    \;+\;
    W_{\mathrm{enc}}^{(xh)}\,x_t
  \Bigr),
\end{align}\)
where \(\phi(\cdot)\) is a nonlinearity (e.g. \(\tanh\)). Then we set</p>

\[\begin{align}
  R = h_{T_x}.
\end{align}\]

<p>For the decoder, we can define:</p>

<ol>
  <li>
    <p><strong>Initialization</strong>: 
\(\begin{align}
  s_0 = R = h_{T_x}.
\end{align}\)</p>
  </li>
  <li>
    <p><strong>Recurrent update</strong>:
\(\begin{align}
  s_t = \phi\Bigl(
    W_{\mathrm{dec}}^{(ss)}\,s_{t-1}
    \;+\;
    W_{\mathrm{dec}}^{(ys)}\,\hat{y}_{t-1}
    \;+\;
    W_{\mathrm{dec}}^{(cs)}\,R
  \Bigr).
\end{align}\)
This merges the partial output (via \(\hat{y}_{t-1}\)) with the context \(R\).</p>
  </li>
  <li>
    <p><strong>Output module</strong>:
\(\begin{align}
  \hat{y}_t = \mathrm{Softmax}\Bigl(
    W_{\mathrm{dec}}^{(so)}\,s_t
  \Bigr).
\end{align}\)</p>
  </li>
</ol>

<p>In this concrete instantiation, submodules (a) and (b) from the generic description appear in a single formula for \(s_t\). The <strong>output module</strong> is simply a linear (via \(W_{\mathrm{dec}}^{(so)}\)) plus softmax layer.</p>

<figure id="fig:encoder-decoder-rnn">
<center><img src="/assets/img/dl-series/2g-encoder-decoder-rnn.png" style="width:95%"></center>
</figure>
<p><b>Figure 6. Encoder-Decoder RNN as word-level language model.</b> The input
language has two and the output language has three words. Every word,
the start and the end of a sentence are represented by a one-hot
encoding (yellow) and the RNN predicts the encodings of the translation
of the input sentence. The encoder RNN has a hidden state (green) that
is updated by one word step-by-step to produce the final context vector
(purple). The decoder RNN takes linear functions of the final encoding
and the start embedding to compute the hidden states for the next output
embedding (blue). The one-hot encoding of the output words (red) are a
softmax of a linar function of these hidden states. The decoder RNN
iterates the prediction until it returns the end token. With this
architecture, we can predict sequences of arbitrary length using the
embedded information of a whole input sequence.<br>
<br></p>

<p>The simplicity of the RNN‚Äôs formulation has two drawbacks.</p>

<ol>
  <li>The connections between inputs and hidden states through linear layers and
  element-wise non-linearities is not flexible enough for some tasks.</li>
  <li>The recurrent graph structure leads to problematic dynamics
  during the backward pass.</li>
</ol>

<h2 id="the-exploding-and-vanishing-gradient-problem">The Exploding and Vanishing Gradient Problem</h2>

<p>We now explore the second problem‚Äîvanishing and exploding gradients‚Äîmore formally. Consider the vanilla RNN‚Äôs loss with respect to the weight matrix in Equation \(\ref{eq:vanilla-rnn}\). Its derivative can be written as:</p>

<p>\(\)
\begin{align}
  \frac{\partial L}{\partial W}
  = \sum_{t=1}^T \sum_{k=1}^{t+1}
    \frac{\partial L_{t+1}}{\partial o_{t+1}}
    \frac{\partial o_{t+1}}{\partial h_{t+1}}
    \frac{\partial h_{t+1}}{\partial h_{k}}
    \frac{\partial h_{k}}{\partial W}.
\label{eq:rnn-derivative}
\end{align}
\(\)</p>

<p>A key factor here is the derivative of the hidden state at time \(t+1\) with respect to a <em>past</em> hidden state \(h_k\). From the chain rule, it follows a recursive product:</p>

<p>\(\)
\begin{align}
\label{eq:rnn-derivative-recursion}
  \frac{\partial h_{t+1}}{\partial h_k}
  = \prod_{j=k}^t
    \frac{\partial h_{j+1}}{\partial h_j}
  = \frac{\partial h_{t+1}}{\partial h_t}
    \cdot
    \frac{\partial h_t}{\partial h_{t-1}}
    \cdots
    \frac{\partial h_{k+1}}{\partial h_k}
  = \prod_{j=k}^t \mathrm{diag} \bigl(
      W_{hh} \,\phi‚Äô\bigl(W[x_{j+1} ;\,h_j]\bigr)
    \bigr).
\end{align}
\(\)</p>

<p>In essence, \(\frac{\partial h_{t+1}}{\partial h_k}\) is the product of Jacobians from time \(t\) down to \(k\). Because an RNN processes sequences, this product can become very large if \((t-k)\) is large (i.e., if the sequence is long).</p>

<h3 id="vanishing-and-exploding-effects">Vanishing and Exploding Effects</h3>

<p>To see why gradients may vanish or explode, assume for simplicity that each local Jacobian \(\tfrac{\partial h_{t+1}}{\partial h_t}\) is constant over time. Let us denote this constant Jacobian by \(J\). Suppose we decompose \(J\) via its eigendecomposition, yielding eigenvalues \(\lambda_1, \lambda_2, \dots, \lambda_n\) and corresponding eigenvectors \(v_1, v_2, \dots, v_n\), with</p>

<p>\(\)
\begin{align}
|\lambda_1| \geq |\lambda_2| \geq \cdots \geq |\lambda_n|.
\end{align}
\(\)</p>

<ul>
  <li>
<strong>Exploding Gradients</strong>: If \(\lvert\lambda_1\rvert &gt; 1\), then repeated multiplications by \(J\) cause gradients to grow exponentially, leading to exploding gradients.</li>
  <li>
<strong>Vanishing Gradients</strong>: Conversely, if \(\lvert\lambda_1\rvert &lt; 1\), the gradient norms shrink exponentially as they propagate back through time, eventually becoming negligibly small (vanishing).</li>
</ul>

<p>In practice, vanishing gradients are especially common when activation derivatives \(\phi'\) stay below 1 in magnitude (as with sigmoid or tanh). As a result, the earliest hidden states in the sequence receive almost no gradient information, hurting the model‚Äôs ability to capture long-range dependencies and effectively limiting its ‚Äúlong-term memory.‚Äù</p>

<h3 id="mitigation-strategies">Mitigation Strategies</h3>

<p>Various methods aim to alleviate exploding or vanishing gradients. Examples include:</p>

<ul>
  <li>Regularization (e.g., gradient clipping)</li>
  <li>Careful weight initialization</li>
  <li>ReLU activations</li>
  <li>Gated RNNs (LSTM or GRU), which introduce gating mechanisms akin to more sophisticated skip connections from Chapter
<a href="https://www.tobiasstenzel.com/blog/2023/dl-cnn/#skip-connections">5</a>
</li>
  <li>Transformer architectures, which rely on attention instead of purely recurrent connections and thus suffer less from vanishing gradients while providing more expressive long-range interactions.</li>
</ul>

<p>Today, the most widely used approach in many sequence tasks is the <strong>transformer</strong>, which not only avoids many vanishing-gradient issues but also offers more flexible and powerful modeling of contextual information.</p>

<hr>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = "Deep Learning Series",
  author  = "Stenzel, Tobias",
  year    = "2023",
  url     = "https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>

      <div class="PageNavigation">
        
          <a class="prev" href="/blog/2023/dl-cnn/">	‚èÆ 6. Convolutional Neural Networks</a>
        
        
          <a class="next" href="/blog/2023/dl-transformer/">8. Transformer ‚è≠</a>
        
      </div>
      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/references.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "tostenzel/tostenzel.github.io",
        "data-repo-id": "MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOA5PmLc4CTBt6",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "bottom",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
