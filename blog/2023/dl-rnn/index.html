<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>7. Recurrent Neural Networks | Tobias  Stenzel</title>
    <meta name="author" content="Tobias  Stenzel">
    <meta name="description" content="🧱 &lt;b&gt;&lt;i&gt;The encoder-decoder RNN is quite useful for understanding the transformer.&lt;/b&gt;&lt;/i&gt;">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9D&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://www.tobiasstenzel.com/blog/2023/dl-rnn/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>

    <!-- _layouts/post.html -->
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "7. Recurrent Neural Networks",
      "description": "🧱 <b><i>The encoder-decoder RNN is quite useful for understanding the transformer.</b></i>",
      "published": "March 7, 2023",
      "authors": [
        {
          "author": "Tobias Stenzel",
          "authorURL": "https://www.tobiasstenzel.com",
          "affiliations": [
            {
              "name": "N/A",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tobias </span>Stenzel</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>7. Recurrent Neural Networks</h1>
        <p>🧱 <b><i>The encoder-decoder RNN is quite useful for understanding the transformer.</i></b></p>
        <p class="post-tags">
          <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
            ·  
            <a href="/blog/tag/neural-net-archetype">
              <i class="fas fa-hashtag fa-sm"></i> neural-net-archetype</a>  
              <a href="/blog/tag/backprop">
              <i class="fas fa-hashtag fa-sm"></i> backprop</a>  
              <a href="/blog/tag/exploding-vanishing-gradients">
              <i class="fas fa-hashtag fa-sm"></i> exploding-vanishing-gradients</a>  
              

        </p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#recurrent-neural-networks">Recurrent Neural Networks</a></div>
            <ul>
              <li><a href="#vanilla-neural-networks">Vanilla Neural Networks</a></li>
              <li><a href="#encoder-decorder-rnns">Encoder-Decorder RNNs</a></li>
              <li><a href="#backward-pass">Backward Pass</a></li>
              
            </ul>
          </nav>
        </d-contents>

        <h2 id="recurrent-neural-networks">Recurrent Neural Networks</h2>

<p>Many tasks require input or output spaces that contain sequences. For
instance, in translation programs we oftentimes encode words as
sequences of one-hot vectors. These vectors are index vectors with a one
at the position of an integer that maps to a word in a fixed vocabulary.
A simple recurrent neural network (RNN) processes a sequence of vectors
${x_1, …, x_T}$ with a recurrence formula
$h_t = f_\theta(h_{t-1},x_t)$. The function $f$ that we will describe in
more detail below takes the same parameters $\theta$ at every time step
to process an arbitrary number of vectors (cf. parameter sharing and
inductive bias). An interpretation of the hidden vector $h$ is that of a
summary of all previous $x$ vectors. A common initialization is
$h_0=\vec{0}$. We can define $f$ according to three criteria. The first
criterion is the input and output space. We would model $f$ differently
for spaces from one to many, many to one or many to many vectors. The
second criterion is the order in which we process input vectors and
predict output vectors. This depends on the nature of the data. For
example, we may either want to predict $y_t$ directly after processing
$x_t$, or predict the complete output sequence ${y_1, …, y_T}$ after
we have processed the complete input sequence ${x_i}_{i=1}^T$. Another
option is to not only compute a summary of the past but also a summary
of the future and use both vectors in our predictions. The third
criterion is how we want to improve upon drawbacks of the model
formulation for longer sequences. We will come to this point later in
this section. To illustrate how RNNs work, we will look at two examples.</p>

<h3 id="vanilla-recurrent-neural-networks">Vanilla Recurrent Neural Networks</h3>

<p>Vanilla Recurrent Neural Networks use a recurrence defined by</p>

\[\begin{align}
h_t &amp;= \phi \big ( W\begin{bmatrix}
       x_{t} \\
       h_{t-1} 
     \end{bmatrix}
     \big ).
\label{eq:vanilla-rnn}
\end{align}\]

<p>Here, we concatenate the current input and the previous hidden states,
transform both linearly and pass it to a non-linear activation function.
This vector notation is equivalent to
$h_t = \phi (W_{xh}x_t + W_{hh}h_{t-1})$. The two matrices $W_{xh}$ and
$W_{hh}$ are concatenated horizontally to $W$. If the input vectors
$x_t$ have dimension $1 \times D$ and the hidden vectors dimension
$1 \times H$, then $W_{xh} \in \mathbb{R}^{H \times D}$,
$W_{hh} \in \mathbb{R}^{H \times H}$, and weight matrix $W$ is a matrix
with dimensions $[H \times (D+H)]$. Vanilla RNN models the current
hidden states $h_t$ at each time step as a linear function of the
elements in the previous hidden states $h_{t-1}$ and the current input
$x_t$, transformed by a non-linearity. In a classification task, e.g.
where we want to predict the next written letter in a prompt by the
previously types letters, we would apply the Softmax function to a
linear transformation of the hidden state at each time step,
$o_t = W_{ho} h_t$, in order to predict the next character’s one-hot
encoding. This is illustrated by Figure
<a href="#fig:vanilla-rnn">5</a>.</p>

<figure id="fig:vanilla-rnn">
<center><img src="/assets/img/dl-series/2f-vanilla-rnn.png" style="width:95%"></center>
</figure>
<p><b>Figure 5. Vanilla RNN as character-level language model.</b> The left side
shows the <em>unrolled RNN</em>. The vocabulary has four characters and
the training sequence is “hello”. Each letter is represented by a 1-hot
encoding (yellow) and the RNN predicts the encodings of the next letter
(green) at each time step. The RNN has a hidden state with three
dimensions (red). The output has four dimensions. The dimensions are the
logits for the next character. They are the softmax of a linear
transformation of the hidden states. During supervised learning, the
model will be trained to increase (decrease) the logits of the correct
(false) characters. The right side shows the <em>rolled-up RNN</em>. The
graph has a cycle that shows that the same hidden states are shared
across time and that the architecture is the same for each
step.</p>

<h3 id="encoder-decoder-rnns">Encoder-Decoder RNNs</h3>

<p>Encoder-Decoder RNNs use the complete input history
${x_i}_1^{T_x}$ to predict the first output $y_1$. Then, it
additionally uses the complete prediction history
${\hat{y}_i}_1^{t-1}$ to predict the next $y_t$ for $t=2,…,T_y$. The
model is able to generate sequences of arbitrary length that can be
unequal to the length of the input sequence. An examplary task is
translating sentences from English to German. Here, we work with one-hot
encodings of words from a fixed vocabulary instead of letters from an
alphabet. To build this RNN, we use the same recursion from the previous
example as an encoder RNN. However, do not classify the output at each
timestep directly. Instead, we use the last hidden state from the
encoder $h_T$ as a context vector $c_0$. Intuitively, the context vector
is an abstract representation of the entire input sentence. Then, we use
another RNN, the decoder RNN to process the information from the context
and the output from the previous period to generate the hidden states
$s_t$ for the current period:</p>

\[\begin{align}
s_t = \phi (W_{os}o_{t-1} + W_{ss}s_{t-1})
\end{align}\]

<p>with $s_0 = c_0$ and, for instance $y_0 = \vec{0}$. We then predict the
next word’s one-hot encoding from the hidden states like in the last
example with the softmax of $o_t=W_{so}s_t$. To model the fact that
input and output sequences can have different length, we require special
start of sentence, $&lt;$sos$&gt;$, and end of sentence, $&lt;$eos$&gt;$, tokens.</p>

<p>The encoder uses the $&lt;$eos$&gt;$ as $x_{1}$. The decoder takes $&lt;$sos$&gt;$
as $y_1$ and stops the recursion when it returns $&lt;$eos$&gt;$.</p>

<figure id="fig:encoder-decoder-rnn">
<center><img src="/assets/img/dl-series/2g-encoder-decoder-rnn.png" style="width:95%"></center>
</figure>
<p><b>Figure 6. Encoder-Decoder RNN as word-level language model.</b> The input
language has two and the output language has three words. Every word,
the start and the end of a sentence are represented by a one-hot
encoding (yellow) and the RNN predicts the encodings of the translation
of the input sentence. The encoder RNN has a hidden state (green) that
is updated by one word step-by-step to produce the final context vector
(purple). The decoder RNN takes linear functions of the final encoding
and the start embedding to compute the hidden states for the next output
embedding (blue). The one-hot encoding of the output words (red) are a
softmax of a linar function of these hidden states. The decoder RNN
iterates the prediction until it returns the end token. With this
architecture, we can predict sequences of arbitrary length using the
embedded information of a whole input sequence.<br>
<br></p>

<p>The simplicity of the RNN’s formulation has two drawbacks. First, the
connections between inputs and hidden states through linear layers and
element-wise non-linearities is not flexible enough for some tasks.
Second, the recurrent graph structure leads to problematic dynamics
during the backward pass.</p>

<h3 id="exploding-and-vanishing-gradient-problem">Exploding and Vanishing Gradient Problem</h3>

<p>We now explore the second
problem more formally. The vanilla RNN’s loss with respect to the weight
matrix in Equation
<a href="#eq:vanilla-rnn">1</a> is given by:</p>

<p>\(\begin{align}\frac{\partial L}{\partial W} = \sum_{t=1}^T \sum_{k=1}^{t+1} \frac{\partial L_{t+1}}{\partial o_{t+1}} \frac{\partial o_{t+1}}{\partial h_{t+1}} \frac{\partial h_{t+1}}{\partial h_{k}} \frac{\partial h_{k}}{\partial W}.
\label{eq:rnn-derivative}\end{align}\) The crucial part is the derivative of the
hidden layer in the next period with respect to some past layer $k$. It
is given by the recursive product \(\begin{align}\label{eq:rnn-derivative-recursion}
\frac{\partial h_{t+1}}{\partial h_k} = \prod_{j=k}^t\frac{\partial h_{t+1}}{\partial h_{k}} = \frac{\partial h_{t+1}}{\partial h_{t}} \frac{\partial h_{t}}{\partial h_{t-1}} ... \frac{\partial h_{k+1}}{\partial h_{k}} = \prod_{j=k}^t \text{diag} \big(W_{hh}\phi' (W  [x_{j+1};h_j])\big).\end{align}\)
Equation
<a href="#eq:rnn-derivative-recursion">4</a> shows that, in order to compute
the gradients for $W_{xh}$ and $W_{hh}$, we have to multiply a large
number of Jacobians depending on the size of the input sequences. The
reason is that the derivative of a hidden layer $h_{t+1}$ with respect
to some previous layer $h_k$ equals the product of the derivatives from
$t+1$ to $k+1$ with respect to their previous layers. For simplicity,
let us assume that these derivatives are constant. Further, let us
compute the eigendecomposition of the fixed Jacobian matrix
$\frac{\partial h_{t+1}}{\partial h_t}$ to analyze the problem more
formally. We obtain the eigenvalues
$\lambda_1, \lambda_2,…,\lambda_n$, with
$|\lambda_1|&gt; |\lambda_2|&gt;…&gt;|\lambda_n|$, and the respective
eigenvectors, $v_1, v_2, …, v_n$. With these components, we can write
the constant update of the hidden state in the direction of an
eigenvector $v_i$ as $\lambda_i \Delta$. During the backward pass from
$t+1$ to $k$, this change becomes $\lambda_i^{t-k} \Delta h$. As a
result, if the largest eigenvalue $\lambda_1$ is smaller than one, the
gradient will vanish, and if it is larger than one, the gradient
explodes. In practice, the gradients oftentimes rather vanishes due the
contribution of the activation derivative which cannot exceed one. In
this case, the earlier hidden states are barely updated. The effect is
that the front parts of the input do not impact the prediction. In other
words, our model has a weak long-term memory.</p>

<p>There are many approaches to alleviate this problem. Examples are
regularization, careful weight initializations, using ReLU activations
only, and gated recurrent networks like Long Sort-Term Memory (LSTM) or
Gated Recurrent Unit (GRU). We can view these gating mechanisms as much
more sophisticated extensions of the skip connections from Chapter
<a href="https://www.tobiasstenzel.com/blog/2023/dl-cnn/#skip-connections">5</a>. Today, the
most common approach is the transformer architecture. It is not only
less vulnerable to vanishing gradients but also provides a more
expressive coupling of current inputs and previous states compared to
vanilla RNNs.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = "Deep Learning Series",
  author  = "Stenzel, Tobias",
  year    = "2023",
  url     = "https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/references.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "tostenzel/tostenzel.github.io",
        "data-repo-id": "MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOA5PmLc4CTBt6",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "bottom",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
