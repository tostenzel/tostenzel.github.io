<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>2. Supervised Learning | Tobias  Stenzel</title>
    <meta name="author" content="Tobias  Stenzel">
    <meta name="description" content="Tobias' homepage.
">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9D&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://www.tobiasstenzel.com/blog/2023/dl-supervised-learning/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>

    <!-- _layouts/post.html -->
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "2. Supervised Learning",
      "description": "",
      "published": "March 2, 2023",
      "authors": [
        {
          "author": "Tobias Stenzel",
          "authorURL": "https://www.tobiasstenzel.com",
          "affiliations": [
            {
              "name": "N/A",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tobias </span>Stenzel</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
          <p class="PageNavigation">
          
            <a class="prev" href="/blog/2023/dl-overview/">	⏮ 1. Overview</a>
          
          
            <a class="next" href="/blog/2023/dl-optimization/">3. Optimization ⏭</a>
          
        </p>
        <br><br>
        <h1>2. Supervised Learning</h1>
        <p></p>
        <p class="post-tags">
          <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
            ·  
            <a href="/blog/tag/supervised-learning">
              <i class="fas fa-hashtag fa-sm"></i> supervised-learning</a>  
              

        </p>
      </d-title>

      <d-byline></d-byline>

      <d-article>

        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#supervised-learning">Supervised Learning</a></div>
            <ul>
              <li><a href="#loss-function">Loss function</a></li>
              <li><a href="#regularization">Regularization</a></li>
              <li><a href="#model-validation">Model Validation</a></li>
              <li><a href="#example-linear-regression">Example — Linear regression</a></li>
              <li><a href="#example-neural-network-regression">Example — Neural network regression</a></li>
              <li><a href="#example-neural-network-classification">Example — Neural network classification</a></li>
              
            </ul>
<div><a href="#summary">Summary</a></div>
            
          </nav>
        </d-contents>

        <h2 id="supervised-learning">Supervised Learning</h2>

<p>Prediction rules based on some specific set of information can be
written as a mapping \(f:X \rightarrow Y\), where \(X\) is an input space
and \(Y\) is an output space. To recognize a dog on a photo, for example,
\(X\) would be the space of images and \(Y\) would be a probability interval
\([0,1]\) for the presence of a dog. However, it is oftentimes very
difficult to find an explicit function \(f\) from theoretical
considerations about the problem. For problems where it is easy to find
many examples \((x,y) \in X \times Y\), the supervised learning approach
is usually well-suited. In our example, the requirement would be a
dataset that consists of a large number of images which are annotated
with presence or absence of a dog.</p>

<h3 id="loss-function">Loss function</h3>

<p>To be concrete, our dataset
\(\{(x_1,y_1),...(x_n,y_n)\}\) includes \(n\) examples. Our theoretical
assumption about the data is that these examples are drawn from a data
generating distribution \(D\) with independent and identically distributed
(i.i.d.) random variables, i.e.
\((x_i, y_i) \overset{\mathrm{iid}}{\sim} D\). In the supervised learning
context, learning the mapping \(f:X \rightarrow Y\) means selecting the
function \(f\) from a set of candidate functions \(\mathcal{F}\) so that \(f\)
yields the best predictions for \(y\) given any \(x\). I.e., we want to find
the best approximation for \(f(Y|X)\). We achieve this by selecting a
scalar-valued loss function \(L(\hat{y},y)\) which measures the difference
between prediction \(\hat{y}_i\) and actual outcome \(y_i\). In theory, our
objective is to find the function with the lowest expected loss for all
examples from the data generating distribution \(D\). In practice,
however, we have to rely on our dataset. Therefore, we approximate our
theoretical objective using the assumption that our data is drawn from
an i.i.d. distribution. We search for function \(f^*\) with the lowest
average loss over the data:</p>

\[\begin{align}
\label{eq:loss}
f^* \approx \arg \min_{f \in \mathcal{F}} \frac{1}{n} \sum^n_{i=1} L \big  ( \ f(x_i), y_i \big  ).
\end{align}\]

<h3 id="regularization">Regularization</h3>

<p>Oftentimes, the i.i.d. assumption is too strong. In
this case, we usually do not achieve the best result for predicting
unseen outcomes with a function that is optimized solely with regards to
our dataset. In short, this function does not necessarily generalize
well. A further issue is how to choose between multiple functions with
the same minimal loss. The approach that addresses both problems at once
is regularization. The idea is to add a regularization penalty to our
objective function from which we obtain our predictor. The penalization
criterion \(R(f)\) is function complexity. We thus search for the function
that fits the data best <em>and</em> has a low complexity:</p>

<p>\(\begin{align}
\label{eq:cost}
f^* \approx \arg \min_{f \in \mathcal{F}} \frac{1}{n} \sum^n_{i=1} L \big  (f(x_i), y_i \big  ) + R(f),
\end{align}\)
where \(R(f)\) is a scalar-valued function. With regularization, we can
achieve a better generalization and choose between functions that
achieve a similar loss.</p>

<p>Frequently, we have already selected our model but not its parameters.
Hence, we take function \(f\) as given but we want to learn the best
parameters \(\theta\) for this function. In this situation, the concept of
loss and regularization directly translates from finding the optimal
function to finding the optimal parameters:</p>

\[\begin{align}
\label{eq:params}
\theta^* \approx \arg \min_{\theta \in \Theta} \frac{1}{n} \sum^n_{i=1} L \big (f(x_i;\theta), y_i \big  ) + R(\theta),
\end{align}\]

<p>where \(\Theta\) denotes the parameter space. Common examples for
\(R(\theta)\) are multiples of vector norms. In this setting, choices like
the model, the loss or the regularization are called hyperparameters.
These are parameters that are set before the actual model parameters are
learned. Hyperparameter choices are oftentimes critical to the quality
of the learned model. There are several other ways to prevent
overfitting the model besides regularization. Examples are choosing
simpler models, stopping the optimization process early, changing or
disabling some model units during training (dropout), and dataset
augmentations.</p>

<h3 id="model-validation">Model validation</h3>

<p>How do we test whether our model generalizes well
to unseen data? The usual machine learning approach is as follows: at
first, we split our examples in three groups: a large training, and
smaller validation and test sets. Secondly, we learn the model
parameters with the training data. The third step is to compute
evaluation metrics for this model from the unseen test data. In general,
we want to minimize the distance between the prediction and the target
vector. For classification tasks, we can use evaluation metrics based on
the confusion matrix. This enables us to identify single classes which
are more difficult to predict for the model. Optionally, we can repeat
the third step multiple times for different hyperparameter
configurations and compare the generalization errors. Then, we evaluate
the best model once again on the validation data and report its
performance. We will briefly discuss hyperparameter selection in the end
of the optimization section.</p>

<h3 id="example--linear-regression">Example — Linear regression</h3>

<p>Assume we have a dataset with 100
observations \((n=100)\) of two features each \((m=2, X=\mathbb{R}^2)\) and
a scalar annotation \((Y=\mathbb{R})\). According to the input and output
space, we choose to restrict the candidate class \(\mathcal{F}\) to the
family of linear functions, i.e.
\(\mathcal{F}=\{w^Tx + b | w \in \mathbb{R}^2, b\in \mathbb{R}\). With
this choice, we set our hypothesis space from a class of functions to
the set of three parameters \(\theta=\{w_1, w_2, b\}\) with
\(w=[w_1, w_2]\). Next, I list two common hyperparameter choices. First,
the squared difference between predicted value and target,
\(L(\hat{y}, y)=(\hat{y} - y)^2\), is a common loss function. Second, the
L2 norm of the weights multiplied with importance parameter \(\lambda\) is
a typical regularizer choice, i.e. \(R(w,b)=\lambda(w_1^2 + w_2^2)\). The
L2 norm counteracts extreme weights and an excessive effect of one
single weight on the prediction \(\hat{y}\). Taken together, our objective
is</p>

\[\begin{align}
\theta^* =  \arg \min_{w,b} \underbrace{
    \Bigg [ \frac{1}{n} \sum^n_{i=1} (w^T x_i + b - y_i)^2 \Bigg ]}_\text{data fitting}
    + \underbrace{\Bigg [ \lambda (w_1^2 + w_2^2) \Bigg ]}_\text{regularization}.
\end{align}\]

<h3 id="example--neural-network-regression">Example — Neural network regression</h3>

<p>Perhaps the relationship between
features \(x_1, x_2\) and the scalar target \(y\) is not liner and there are
interactions between the two features. A model class that is well-known
for its theoretical capabilities of approximating continuous functions
are feedforward neural networks (FNN). As a preview, we can extend the
previous example to a specific FNN called neural network regression. It
has the form \(f(x;\theta)= w_2 \text{ tanh} (W_1^T x + b_1) + b_2\) with
parameters \(\theta=\{W_1, b_2, w_1, b_1\}\). \(W_1\) is matrix with
dimension \(H \times 2\), \(b_1\) and \(w_2\) are both vectors of length \(H\),
and \(b_2\) is a scalar. \(H\) is an integer-type hyperparameter.
\(W_1^T x + b_1\) is often called a hidden layer. Its elements, or
neurons, are outputs of multiplicative interactions between elements
from previous layers, in this case the two inputs \(x_1\) and \(x_2\). tanh
denotes the hyperbolic tangent that squashes elements from the
real-valued domain to the interval [-1,1]. It is applied element-wise
and introduces non-linearity to the model. The objective is given by:</p>

\[\begin{align}
    \theta^* =  \arg \min_{W_1, b_2, w_1, b_1} \underbrace{
    \Bigg [ \frac{1}{n} \sum^n_{i=1} \big (w_2 \text{ tanh}  ( W_1^T x_i + b_1 ) + b_2 - y_i \big)^2 \Bigg ]}_\text{data fitting}
    + \underbrace{\Bigg [ \lambda \big (||W_1||^2_2 + ||w_2||_2^2 \big ) \Bigg ]}_\text{regularization}.
\end{align}\]

<h3 id="example--neural-network-classification">Example — Neural network classification</h3>

<p>Oftentimes, we do not want
to predict a real number but we want to predict whether an input
corresponds to an output of a specific class \(k\) of \(K\) possible
classes. For instance, we may want to predict whether a dog, a, cat, or
a budgie is shown in a picture. To formalize this problem, we encode our
classes as non-negative integers starting at 0. We also encode our
output as a vector of \(|K|\) zeros, where only the \(k^*\)-th element
representing the actual class encoded as \(k^*\) is set to one. Using the
previous order, a picture that shows a dog is encoded as \(y=[1, 0, 0]\).
We can achieve a model that predicts a vector of this form with two
simple adjustments of the neural network regression model. The first
adjustment is replacing vector \(w_2\) by matrix \(W_2\) with dimensions
\(K \times H\). This gives us a real-valued vector \(z\) with one
real-valued element for every class. The second adjustment is that we
compute class probabilities by applying the softmax function to \(z\),
i.e. \(\hat{p}_k = e^{z_k} / \sum_{i=1}^K e^{z_i}\). Handy properties of
the softmax function in the probability context are, first, that every
element is mapped to \([0,1]\), and second, that the sum of all elements
is normalized to one. Hence, our class prediction would be the class
that is represented by the largest element in vector \(\hat{p}\). For
instance, with \(K=3\) and \(\hat{p}=[0.1, 0.7, 0.2]\), our prediction
\(\hat{y}\) equals \([0, 1, 0]\). The most common loss function for
classification is the cross-entropy loss. It takes the predicted class
probabilities \(\hat{p}\) instead of the predicted class vector \(\hat{y}\).
We denote both options by output \(o\):</p>

<p>\(\begin{align}
L(o,y) = L(\hat{p},y) = - \sum_{k=1}^K y_k \log \hat{p}_k = -\log  \hat{p}_{k=k^*}.
\end{align}\)
The first equality is the cross-entropy definition for two distribution,
i.e \(H(q,r) = - \sum_x q(x) \log r(x)\). The second equality shows that
the loss equals the negative log probability from vector \(p\) at the
position of the actual class \(k^*\). It follows from the fact that target
distribution is degenerate. This means that only the true class has
probability one. Additional motivation for choosing the cross-entropy
for classification problems is that minimizing this loss is equivalent
to maximizing the likelihood of observing model parameters \(\theta\)
conditional on predicting the true class label.</p>

<h2 id="summary">Summary</h2>

<p>Supervised learning requires a dataset of \(n\) examples
\(\{(x_1,y_1)\), ..., \((x_n,y_n)\}\), where \((x_i,y_i) \in X \times Y\).
\(y_i\) represents the annotation that we want to predict based on
features \(x_i\). Finding a mapping \(f:X \rightarrow Y\) is a two-step
process. First, we have to formalize the problem by choosing</p>

<ol>
  <li>
    <p>The search space of functions \(\mathcal{F}\) with
\(f \in \mathcal{F}\).</p>
  </li>
  <li>
    <p>The scalar-valued loss function \(L(\hat{y}, y)\) that measures the
difference between the network’s predictions
\(\hat{y} = f_{\theta}(x)\) and the target \(y\).</p>
  </li>
  <li>
    <p>The scalar term \(R(f)\) that penalizes overly complex functions.</p>
  </li>
</ol>

<p>In deep learning without architecture search, the space of functions is
one neural network \(f_\theta\) with some parameters \(\theta \in \Theta\).
Putting these parts together, the second step is to find these
parameters from the optimization problem
\(\theta^* = \arg \min_{\theta \in \Theta} C(\theta)\) with
\(C(\theta) = \frac{1}{n} \sum^n_{i=1} L \big (f_\theta), y_i \big  ) + R(f_{\theta})\).
We call \(C(\theta)\) the cost function.</p>

<figure id="fig:supervised-learning">
    <center><img src="/assets/img/dl-series/supervised-learning.png" style="width:100%"></center>
</figure>
<p><b>Figure 1: Computational graph for a general supervised learning
approach.</b> Examples <span class="math inline">{<em>x</em><sub><em>i</em></sub>}<sub><em>i</em> = 1</sub><sup><em>n</em></sup></span>
and parameters <span class="math inline"><em>θ</em></span> are taken by
model f to predict the targets by <span class="math inline">{<em>ŷ</em><sub><em>i</em></sub>}<sub><em>i</em> = 1</sub><sup><em>n</em></sup></span>.
Data loss L computes the difference between the predictions and the
targets <span class="math inline">{<em>y</em><sub><em>i</em></sub>}<sub><em>i</em> = 1</sub><sup><em>n</em></sup></span>.
Regularization loss R penalizes extreme parameters. The sum of both
penalties is given by cost C. Oftentimes we use predicted class
probabilities <span class="math inline"><em>p̂</em></span> instead of
(rounded) predictions.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = "Deep Learning Series",
  author  = "Stenzel, Tobias",
  year    = "2023",
  url     = "https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>

      <div class="PageNavigation">
        
          <a class="prev" href="/blog/2023/dl-overview/">	⏮ 1. Overview</a>
        
        
          <a class="next" href="/blog/2023/dl-optimization/">3. Optimization ⏭</a>
        
      </div>
      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "tostenzel/tostenzel.github.io",
        "data-repo-id": "MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOA5PmLc4CTBt6",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "bottom",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
