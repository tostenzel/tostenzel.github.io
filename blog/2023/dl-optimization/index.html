<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>3. Optimization | Tobias  Stenzel</title>
    <meta name="author" content="Tobias  Stenzel">
    <meta name="description" content="Tobias' homepage.
">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9D&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://www.tobiasstenzel.com/blog/2023/dl-optimization/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>

    <!-- _layouts/post.html -->
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "3. Optimization",
      "description": "",
      "published": "March 3, 2023",
      "authors": [
        {
          "author": "Tobias Stenzel",
          "authorURL": "https://www.tobiasstenzel.com",
          "affiliations": [
            {
              "name": "N/A",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Tobias </span>Stenzel</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
          <p class="PageNavigation">
          
            <a class="prev" href="/blog/2023/dl-supervised-learning/">	⏮ 2. Supervised Learning</a>
          
          
            <a class="next" href="/blog/2023/dl-backprop/">4. Backpropagation ⏭</a>
          
        </p>
        <br><br>
        <h1>3. Optimization</h1>
        <p></p>
        <p class="post-tags">
          <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
            ·  
            <a href="/blog/tag/optimization">
              <i class="fas fa-hashtag fa-sm"></i> optimization</a>  
              <a href="/blog/tag/first-order-methods">
              <i class="fas fa-hashtag fa-sm"></i> first-order-methods</a>  
              

        </p>
      </d-title>

      <d-byline></d-byline>

      <d-article>

        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#optimization">Optimization</a></div>
            <ul>
              <li><a href="#first-order-methods">First order methods</a></li>
              <li><a href="#advanced-first-order-methods">Advanced first order methods</a></li>
              <li><a href="#hyperparamter-optimization">Hyperparamter optimization</a></li>
              
            </ul>
          </nav>
        </d-contents>

        <h2 id="optimization">Optimization</h2>

<p>In the previous section we formalized supervised learning of a
predictive model as solving the optimization problem
\(\theta^* = \arg \min_{\theta \in \Theta} C(\theta)\), where \(\theta\)
denotes the model parameters and cost function \(C\) represents the
average loss of all examples plus a regularization penalty. In realistic
settings, there is no closed form solution to this problem. Therefore,
we have to rely on schemes that iteratively proposes new parameters
given the previous choice or the initial guess. We want to choose a
method that proposes new candidates with a high likelihood of reducing
the loss compared to the current parameters, so we can replace them.</p>

<h3 id="first-order-methods">First order methods</h3>
<p>In practice, a neural network is a composition
of many small functions that are easy to differentiate. Therefore, we
can compute the gradient \(\nabla_\theta C\) from our network via the
backpropagation technique. We will discuss this method in detail in the
next section. The gradient of our cost function is a vector of
first-order partial derivatives. It contains the direction of its
fastest increase and its magnitude is the rate of increase in that
direction. The gradient at a minimum of the loss function equals zero.
Therefore, we can use the negative gradient as search direction for
selecting the next proposal \(\theta_{i+1}\) from current candidate
\(\theta_i\). This is the basic idea of the gradient descent algorithm.
The method alternates between two steps: 1.) Compute the gradient
\(\nabla_\theta C(\theta)\). 2.) Update \(\theta\) by subtracting a small
multiple of the gradient. Many applications use very large datasets. For
instance, the number of training images in ImageNet is about 1 million.
Therefore, it is handy to approximate the loss gradient from a small
minibatch of examples (e.g. 100). With that, we can update \(\theta\) many
times for every epoch. An epoch is one iteration over the complete
training set. In practice, a large number of approximate updates works
better than a small number of exact updates. This algorithm is called
Stochastic Gradient Descent (SGD). It is summarized in Algorithm 1.</p>

<figure id="fig:supervised-learning">
<center><img src="/assets/img/dl-series/sgd.png" style="width:90%"></center>
</figure>

<p>A crucial parameter for gradient descent algorithms is the learning
rate, or step size, \(\eta\). If we set it too small, the optimization
requires too many steps. If we set it too large, the algorithm may not
converge or even diverge. As an illustration, consider the following toy
example with convex objective function: Let \(f=x^2\) with gradient
\(\frac{df}{dx}=2x\) and initial parameter value \(x = 1\). With \(\eta&lt;1\)
the algorithm finds \(x^*=0\). However, with \(\eta=1\), it oscillates
between \(x=-1\) and \(x=1\), and with \(\eta&gt;1\) it diverges to \(x=\infty\).
Generally, finding a useful learning rate depends on the objective
function.</p>

<h3 id="advanced-first-order-methods">Advanced first order methods</h3>

<p>Oftentimes, we can achieve faster
convergence speed with modified formulations of the update direction
\(\Delta \theta\) from Step 3 in Algorithm 1. The
two main ideas are to use weighted moving averages of all gradients so
far and to compute parameter-specific learning rates. Common variants
are Momentum <d-cite key="sutskever_importance_2013"></d-cite>, Adagrad <d-cite key="duchi_adaptive_2011"></d-cite>, RMSProp <d-cite key="hinton_lecture_2012"></d-cite> and Adam <d-cite key="kingma_adam_2014"></d-cite>. The <strong>Momentum</strong> update is inspired by the physics
notion of momentum. The current update direction is not determined by
the current gradient but also by the previous gradients. The impact of
the previous gradients, however, decays exponentially for every previous
iteration. Let \(\alpha\) be the decay parameter and
\(g := \nabla_{\theta}C(\theta)\). We then replace Step 3 by two steps:
First, we compute the "velocity" \(v= \alpha v + g\) (initialized at
zero), and second, we compute the parameter update with
\(\Delta \theta = -\eta v\). <strong>Adagrad</strong> introduces element-wise learning
rates for the gradient. I.e. we weigh every partial derivative with the
moving average of its squared sum. The motivation is two-fold: first the
shape of the objective function can vary between different dimensions.
Therefore, dimension-specific learning rates may improve the algorithm.
Second, we can achieve a more equal exploration of each dimension by
equipping dimensions with a history of small gradients with larger step
sizes and vice versa. Formally, Adagrad first introduces the
intermediate variable \(r = r + g \odot g\) where \(\odot\) denotes
elementwise multiplication. Second, it computes the gradient update with
\(\Delta \theta = -\frac{\epsilon}{\delta + \sqrt{r}} \odot g\). In the
last expression, \(\delta\) is a small number to avoid division by zero.
<strong>RMSProp</strong> introduces an additional hyperparameter to weigh Adagrad’s
running average, i.e. \(r s= \rho r + (1-\rho)g \odot g\). Lastly, the
<strong>Adam</strong> update combines gradient momentum and RMSProp’s
dimension-specific learning rates.</p>

<h3 id="hyperparamter-optimization">Hyperparamter optimization</h3>

<p>In the discussion of supervised learning
and optimization, we encountered many configurations that we did not
specify. Examples are the number of hidden units \(H\) in our neural
network, the regularization strength \(\lambda\), or the learning rate
\(\eta\) in gradient descent. These parameters either change the
composition of the model parameters \(\lambda\) or impact their selection
during training. Therefore, they have to be set before a model is
trained. To this end, we have to select our hyperparameter space
\(\mathcal{H}\). Due to restrictions of time and computational power, this
choice is often based on experiences with similar models from previous
research. Principled approaches range from model-free methods like
random search to global optimization frameworks like Bayesian
optimization. Model-free methods are simple and do not use the
evaluation history whereas global optimization techniques additionally
consider the uncertain trade-off between exploring new values and
exploiting good values that have already been found. I recommend the
textbook chapter by <d-cite key="feurer_hyperparameter_nodate"></d-cite> for more explanations
and concrete examples.</p>

<h2 id="citation">Citation</h2>

<p>In case you like this series, cite it with:</p>
<pre tabindex="0"><code class="language-latex">@misc{stenzel2023deeplearning,
  title   = "Deep Learning Series",
  author  = "Stenzel, Tobias",
  year    = "2023",
  url     = "https://www.tobiasstenzel.com/blog/2023/dl-overview/
}
</code></pre>

      <div class="PageNavigation">
        
          <a class="prev" href="/blog/2023/dl-supervised-learning/">	⏮ 2. Supervised Learning</a>
        
        
          <a class="next" href="/blog/2023/dl-backprop/">4. Backpropagation ⏭</a>
        
      </div>
      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/references.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "tostenzel/tostenzel.github.io",
        "data-repo-id": "MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOA5PmLc4CTBt6",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "bottom",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
