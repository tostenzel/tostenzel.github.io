@article{kingma_adam_2014,
	title = {Adam: A method for stochastic optimization},
	journal = {arXiv preprint arXiv:1412.6980},
	author = {Kingma, Diederik P and Ba, Jimmy},
	year = {2014},
}

@online{meinhardt_trackformer_2022,
	title = {TrackFormer: Multi-Object Tracking with Transformers},
	shorttitle = {{TrackFormer}},
	url = {http://arxiv.org/abs/2101.02702},
	doi = {10.48550/arXiv.2101.02702},
	abstract = {The challenging task of multi-object tracking (MOT) requires simultaneous reasoning about track initialization, identity, and spatio-temporal trajectories. We formulate this task as a frame-to-frame set prediction problem and introduce TrackFormer, an end-to-end trainable MOT approach based on an encoder-decoder Transformer architecture. Our model achieves data association between frames via attention by evolving a set of track predictions through a video sequence. The Transformer decoder initializes new tracks from static object queries and autoregressively follows existing tracks in space and time with the conceptually new and identity preserving track queries. Both query types benefit from self- and encoder-decoder attention on global frame-level features, thereby omitting any additional graph optimization or modeling of motion and/or appearance. TrackFormer introduces a new tracking-by-attention paradigm and while simple in its design is able to achieve state-of-the-art performance on the task of multi-object tracking (MOT17 and MOT20) and segmentation (MOTS20). The code is available at https://github.com/timmeinhardt/trackformer .},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Meinhardt, Tim and Kirillov, Alexander and Leal-Taixe, Laura and Feichtenhofer, Christoph},
	month = apr,
	year = {2022},
	note = {arXiv:2101.02702 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/SDGTBEGL/Meinhardt et al. - 2022 - TrackFormer Multi-Object Tracking with Transforme.pdf:application/pdf;arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/G8LQB365/2101.html:text/html},
}

@inproceedings{sutskever_importance_2013,
	title = {On the importance of initialization and momentum in deep learning},
	url = {https://proceedings.mlr.press/v28/sutskever13.html},
	abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
	language = {en},
	urldate = {2023-03-21},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	month = may,
	year = {2013},
	note = {ISSN: 1938-7228},
	pages = {1139--1147},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/SAV359A6/Sutskever et al. - 2013 - On the importance of initialization and momentum i.pdf:application/pdf},
}

@book{murphy_machine_2012,
	title = {Machine Learning: A Probabilistic Perspective},
	isbn = {978-0-262-30432-0},
	shorttitle = {Machine {Learning}},
	abstract = {A comprehensive introduction to machine learning that uses probabilistic models and inference as a unifying approach.Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach.The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package—PMTK (probabilistic modeling toolkit)—that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
	language = {en},
	publisher = {MIT Press},
	author = {Murphy, Kevin P.},
	month = sep,
	year = {2012},
	note = {Google-Books-ID: RC43AgAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Character recognition, Feature extraction, Hidden Markov models, Machine learning, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis},
	pages = {2278--2324},
	file = {IEEE Xplore Abstract Record:/home/tobias/snap/zotero-snap/common/Zotero/storage/ZXLGVNR5/726791.html:text/html;IEEE Xplore Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/JPNHPKNW/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf},
}

@online{kingma_adam_2017,
	title = {Adam: A Method for Stochastic Optimization},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {arXiv Fulltext PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/WLMGLDTB/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/GBUA7JBN/1412.html:text/html},
}

@book{hutter_automated_2019,
	title = {Automated Machine Learning: Methods, Systems, Challenges},
	shorttitle = {Automated Machine Learning},
	url = {https://library.oapen.org/handle/20.500.12657/23012},
	abstract = {This open access book presents the first comprehensive overview of general methods in Automated Machine Learning (AutoML), collects descriptions of existing systems based on these methods, and discusses the first series of international challenges of AutoML systems. The recent success of commercial ML applications and the rapid growth of the field has created a high demand for off-the-shelf ML methods that can be used easily and without expert knowledge. However, many of the recent machine learning successes crucially rely on human experts, who manually select appropriate ML architectures (deep learning architectures or more traditional ML workflows) and their hyperparameters. To overcome this problem, the field of AutoML targets a progressive automation of machine learning, based on principles from optimization and machine learning itself. This book serves as a point of entry into this quickly-developing field for researchers and advanced students alike, as well as providing a reference for practitioners aiming to use AutoML in their work.},
	language = {English},
	urldate = {2023-03-21},
	publisher = {Springer Nature},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	year = {2019},
	doi = {10.1007/978-3-030-05318-5},
	note = {Accepted: 2020-03-18 13:36:15},
	keywords = {Pattern recognition, Artificial intelligence, bic Book Industry Communication::U Computing \& information technology::UY Computer science::UYQ Artificial intelligence, bic Book Industry Communication::U Computing \& information technology::UY Computer science::UYQ Artificial intelligence::UYQP Pattern recognition, bic Book Industry Communication::U Computing \& information technology::UY Computer science::UYT Image processing, Computer science, Optical data processing},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/TJ4SLVPC/Hutter et al. - 2019 - Automated Machine Learning Methods, Systems, Chal.pdf:application/pdf},
}

@online{hanin_approximating_2018,
	title = {Approximating Continuous Functions by ReLU Nets of Minimal Width},
	url = {http://arxiv.org/abs/1710.11278},
	doi = {10.48550/arXiv.1710.11278},
	abstract = {This article concerns the expressive power of depth in deep feed-forward neural nets with ReLU activations. Specifically, we answer the following question: for a fixed \$d\_\{in\}{\textbackslash}geq 1,\$ what is the minimal width \$w\$ so that neural nets with ReLU activations, input dimension \$d\_\{in\}\$, hidden layer widths at most \$w,\$ and arbitrary depth can approximate any continuous, real-valued function of \$d\_\{in\}\$ variables arbitrarily well? It turns out that this minimal width is exactly equal to \$d\_\{in\}+1.\$ That is, if all the hidden layer widths are bounded by \$d\_\{in\}\$, then even in the infinite depth limit, ReLU nets can only express a very limited class of functions, and, on the other hand, any continuous function on the \$d\_\{in\}\$-dimensional unit cube can be approximated to arbitrary precision by ReLU nets in which all hidden layers have width exactly \$d\_\{in\}+1.\$ Our construction in fact shows that any continuous function \$f:[0,1]{\textasciicircum}\{d\_\{in\}\}{\textbackslash}to{\textbackslash}mathbb R{\textasciicircum}\{d\_\{out\}\}\$ can be approximated by a net of width \$d\_\{in\}+d\_\{out\}\$. We obtain quantitative depth estimates for such an approximation in terms of the modulus of continuity of \$f\$.},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Hanin, Boris and Sellke, Mark},
	month = mar,
	year = {2018},
	note = {arXiv:1710.11278 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computational Complexity, Mathematics - Combinatorics, Mathematics - Statistics Theory, Statistics - Machine Learning},
	annote = {Comment: v2. 13p. Extended main result to higher dimensional output. Comments welcome},
	file = {arXiv Fulltext PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/7768V5WE/Hanin and Sellke - 2018 - Approximating Continuous Functions by ReLU Nets of.pdf:application/pdf;arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/55ADGBVA/1710.html:text/html},
}


@incollection{feurer_hyperparameter_2019,
	address = {Cham},
	series = {The Springer Series on Challenges in Machine Learning},
	title = {Hyperparameter {Optimization}},
	isbn = {978-3-030-05318-5},
	url = {https://doi.org/10.1007/978-3-030-05318-5_1},
	abstract = {Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning (AutoML) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization (HPO). In this chapter, we give an overview of the most prominent approaches for HPO. We first discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-fidelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.},
	language = {en},
	urldate = {2023-03-21},
	booktitle = {Automated Machine Learning: Methods, Systems, Challenges},
	publisher = {Springer International Publishing},
	author = {Feurer, Matthias and Hutter, Frank},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	year = {2019},
	doi = {10.1007/978-3-030-05318-5_1},
	pages = {3--33},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/2BLBP6KP/Feurer and Hutter - 2019 - Hyperparameter Optimization.pdf:application/pdf},
}

@article{duchi_adaptive_2011,
	title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
	volume = {12},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	number = {61},
	urldate = {2023-03-21},
	journal = {Journal of Machine Learning Research},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	year = {2011},
	pages = {2121--2159},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/BNDCA6BU/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.pdf:application/pdf},
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {1435-568X},
	url = {https://doi.org/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	language = {en},
	number = {4},
	urldate = {2023-03-21},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	keywords = {Neural networks, Approximation, Completeness},
	pages = {303--314},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/ZTRGLWZ9/Cybenko - 1989 - Approximation by superpositions of a sigmoidal fun.pdf:application/pdf},
}

@inproceedings{carion_end--end_2020,
	address = {Cham},
	series = {Lecture Notes in Computer Science},
	title = {End-to-End Object Detection with Transformers},
	isbn = {978-3-030-58452-8},
	doi = {10.1007/978-3-030-58452-8_13},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {213--229},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/PLLK765L/Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is All you Need},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-03-21},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/9FWGYJL6/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@online{karpathy_transformer_2022,
	type = {Thread},
	title = {The Transformer is a magnificient neural network architecture},
	url = {https://twitter.com/karpathy/status/1582807367988654081?lang=en},
	urldate = {2023-03-03},
	journal = {twitter},
	author = {Karpathy, Andrej},
	month = oct,
	year = {2022},
}

@online{vaswani_transformers_2021,
	address = {Stanford University},
	type = {Guest lecture},
	title = {Transformers and Self-Attention},
	url = {https://www.youtube.com/watch?v=5vcj8kSwBCY},
	urldate = {2023-03-03},
	author = {Vaswani, Ashish and Huang, Anna},
	month = mar,
	year = {2021},
}

@online{kafunah_backpropagation_2016,
	title = {Backpropagation In Convolutional Neural Networks},
	url = {https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/},
	urldate = {2023-02-28},
	journal = {Backpropagation In Convolutional Neural Networks},
	author = {Kafunah, Jefkine},
	month = may,
	year = {2016},
}

@online{arat_backpropagation_2019,
	title = {Backpropagation Through Time for Recurrent Neural Network},
	url = {https://mmuratarat.github.io/2019-02-07/bptt-of-rnn},
	urldate = {2023-02-28},
	author = {Arat, Mustafa Murat},
	month = feb,
	year = {2019},
}

@online{hinton_lecture_2012,
	title = {Lecture 6a. Overview of mini-batch gradient descent},
	url = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
	urldate = {2023-02-03},
	author = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
	year = {2012},
	annote = {Accessed: 2020–11-06},
}

@phdthesis{karpathy_connecting_2016,
	type = {Dissertation},
	title = {Connecting Images and Natural Language},
	url = {https://cs.stanford.edu/people/karpathy/main.pdf},
	school = {Stanford University},
	author = {Karpathy, Andrej},
	month = aug,
	year = {2016},
	file = {Andrej - 2016 - Connecting Images and Natural Language.pdf:/home/tobias/snap/zotero-snap/common/Zotero/storage/L5ZM9L6H/Andrej - 2016 - Connecting Images and Natural Language.pdf:application/pdf},
}

@online{li_cs231n_2018,
	title = {CS231n: Deep Learning for Computer Vision},
	url = {http://cs231n.stanford.edu/index.html},
	urldate = {2023-02-02},
	author = {Li, Fei-Fei and Wu, Jiajun and Gao, Ruohan},
	year = {2018},
	annote = {Accessed: 2020–11-06},
}

@book{goodfellow_deep_2016,
	title = {Deep Learning},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	annote = {http://www.deeplearningbook.org},
	file = {Goodfellow et al. - 2016 - Deep Learning.pdf:/home/tobias/snap/zotero-snap/common/Zotero/storage/C7BJ56AN/Goodfellow et al. - 2016 - Deep Learning.pdf:application/pdf},
}

@online{alammar_illustrated_2018,
	title = {The Illustrated Transformer},
	url = {https://jalammar.github.io/illustrated-transformer/},
	urldate = {2023-01-25},
	author = {Alammar, Jay},
	month = jun,
	year = {2018},
}

@online{weng_attention_2018,
	title = {Attention? Attention!},
	url = {https://lilianweng.github.io/posts/2018-06-24-attention/},
	urldate = {2023-01-25},
	author = {Weng, Lilian},
	month = jun,
	year = {2018},
}

@article{kuhn_hungarian_1955,
	title = {The Hungarian method for the assignment problem},
	volume = {2},
	issn = {1931-9193},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109},
	doi = {10.1002/nav.3800020109},
	abstract = {Assuming that numerical scores are available for the performance of each of n persons on each of n jobs, the “assignment problem” is the quest for an assignment of persons to jobs so that the sum of the n scores so obtained is as large as possible. It is shown that ideas latent in the work of two Hungarian mathematicians may be exploited to yield a new method of solving this problem.},
	language = {en},
	number = {1-2},
	urldate = {2023-03-21},
	journal = {Naval Research Logistics Quarterly},
	author = {Kuhn, H. W.},
	year = {1955},
	pages = {83--97},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/VWKB7TMI/Kuhn - 1955 - The Hungarian method for the assignment problem.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/4KRPGFEM/nav.html:text/html},
}

@misc{noauthor_automatic_nodate,
	title = {Automatic differentiation},
	url = {https://en.wikipedia.org/wiki/Automatic_differentiation},
	urldate = {2023-02-28},
	journal = {Wikipedia},
}


@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	number = {4},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, George},
	year = {1989},
	note = {Publisher: Springer},
	pages = {303--314},
}

@article{hanin_approximating_2017,
	title = {Approximating continuous functions by relu nets of minimal width},
	journal = {arXiv preprint arXiv:1710.11278},
	author = {Hanin, Boris and Sellke, Mark},
	year = {2017},
}


@inproceedings{he_deep_2016,
	title = {Deep Residual Learning for Image Recognition},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
	urldate = {2023-03-21},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/MY2AIRDE/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@online{noauthor_automatic_2023,
	title = {Automatic differentiation},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Automatic_differentiation&oldid=1143708979},
	abstract = {In mathematics and computer algebra, automatic differentiation (auto-differentiation, autodiff, or AD), also called algorithmic differentiation, computational differentiation, is a set of techniques to evaluate the partial derivative of a function specified by a computer program. 
Automatic differentiation exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, partial derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.},
	language = {en},
	urldate = {2023-03-21},
	journal = {Wikipedia},
	month = mar,
	year = {2023},
	note = {Page Version ID: 1143708979},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/2LK32JYP/Automatic_differentiation.html:text/html},
}

@online{zhu_deformable_2021,
	title = {Deformable DETR: Deformable Transformers for End-to-End Object Detection},
	shorttitle = {Deformable {DETR}},
	url = {http://arxiv.org/abs/2010.04159},
	doi = {10.48550/arXiv.2010.04159},
	abstract = {DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
	month = mar,
	year = {2021},
	note = {arXiv:2010.04159 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICLR 2021 Oral},
	file = {arXiv Fulltext PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/PJBIK2AB/Zhu et al. - 2021 - Deformable DETR Deformable Transformers for End-t.pdf:application/pdf;arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/RIFZZAQG/2010.html:text/html},
}

@article{liu_deep_2018,
	title = {Deep learning for generic object detection},
	journal = {A Survey},
	author = {Liu, L. and Ouyang, W. and Wang, X. and Fieguth, P. and Chen, J. and Liu, XJAS},
	year = {2018},
}

@inproceedings{lin_microsoft_2014,
	title = {Microsoft COCO: Common objects in context},
	shorttitle = {Microsoft coco},
	booktitle = {Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
	publisher = {Springer},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C. Lawrence},
	year = {2014},
	pages = {740--755},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/DC6G4YK6/Lin et al. - 2014 - Microsoft coco Common objects in context.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/3R6H3QLR/978-3-319-10602-1_48.html:text/html},
}

@inproceedings{dai_deformable_2017,
	title = {Deformable Convolutional Networks},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Dai_Deformable_Convolutional_Networks_ICCV_2017_paper.html},
	urldate = {2023-05-30},
	author = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
	year = {2017},
	pages = {764--773},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/PA5D55I9/Dai et al. - 2017 - Deformable Convolutional Networks.pdf:application/pdf},
}

@inproceedings{zhu_deformable_2019,
	title = {Deformable convnets v2: More deformable, better results},
	shorttitle = {Deformable convnets v2},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Zhu, Xizhou and Hu, Han and Lin, Stephen and Dai, Jifeng},
	year = {2019},
	pages = {9308--9316},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/JX5BR2S2/Zhu et al. - 2019 - Deformable convnets v2 More deformable, better re.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/FM8JVKQC/Zhu_Deformable_ConvNets_V2_More_Deformable_Better_Results_CVPR_2019_paper.html:text/html},
}

@inproceedings{lin_feature_2017,
	title = {Feature pyramid networks for object detection},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	year = {2017},
	pages = {2117--2125},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/CAECXF2L/Lin et al. - 2017 - Feature pyramid networks for object detection.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/HXKKZNCE/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.html:text/html},
}

@inproceedings{ren_faster_2015,
	title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
	volume = {28},
	shorttitle = {Faster {R}-{CNN}},
	url = {https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster\_rcnn.},
	urldate = {2023-05-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	year = {2015},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/3UJ5QRHS/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf},
}

@inproceedings{feurer_hyperparameter_nodate,
	title = {Hyperparameter Optimization},
	author = {Feurer, Matthias and Hutter, Frank},
	note = {Section: 1},
	pages = {3--38},
}
