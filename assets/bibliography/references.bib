
@misc{meinhardt_trackformer_2022,
	title = {{TrackFormer}: {Multi}-{Object} {Tracking} with {Transformers}},
	shorttitle = {{TrackFormer}},
	url = {http://arxiv.org/abs/2101.02702},
	doi = {10.48550/arXiv.2101.02702},
	abstract = {The challenging task of multi-object tracking (MOT) requires simultaneous reasoning about track initialization, identity, and spatio-temporal trajectories. We formulate this task as a frame-to-frame set prediction problem and introduce TrackFormer, an end-to-end trainable MOT approach based on an encoder-decoder Transformer architecture. Our model achieves data association between frames via attention by evolving a set of track predictions through a video sequence. The Transformer decoder initializes new tracks from static object queries and autoregressively follows existing tracks in space and time with the conceptually new and identity preserving track queries. Both query types benefit from self- and encoder-decoder attention on global frame-level features, thereby omitting any additional graph optimization or modeling of motion and/or appearance. TrackFormer introduces a new tracking-by-attention paradigm and while simple in its design is able to achieve state-of-the-art performance on the task of multi-object tracking (MOT17 and MOT20) and segmentation (MOTS20). The code is available at https://github.com/timmeinhardt/trackformer .},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Meinhardt, Tim and Kirillov, Alexander and Leal-Taixe, Laura and Feichtenhofer, Christoph},
	month = apr,
	year = {2022},
	note = {arXiv:2101.02702 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/SDGTBEGL/Meinhardt et al. - 2022 - TrackFormer Multi-Object Tracking with Transforme.pdf:application/pdf;arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/G8LQB365/2101.html:text/html},
}

@inproceedings{sutskever_importance_2013,
	title = {On the importance of initialization and momentum in deep learning},
	url = {https://proceedings.mlr.press/v28/sutskever13.html},
	abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
	language = {en},
	urldate = {2023-03-21},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	month = may,
	year = {2013},
	note = {ISSN: 1938-7228},
	pages = {1139--1147},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/SAV359A6/Sutskever et al. - 2013 - On the importance of initialization and momentum i.pdf:application/pdf},
}

@book{murphy_machine_2012,
	title = {Machine {Learning}: {A} {Probabilistic} {Perspective}},
	isbn = {978-0-262-30432-0},
	shorttitle = {Machine {Learning}},
	abstract = {A comprehensive introduction to machine learning that uses probabilistic models and inference as a unifying approach.Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach.The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package—PMTK (probabilistic modeling toolkit)—that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
	language = {en},
	publisher = {MIT Press},
	author = {Murphy, Kevin P.},
	month = sep,
	year = {2012},
	note = {Google-Books-ID: RC43AgAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Character recognition, Feature extraction, Hidden Markov models, Machine learning, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis},
	pages = {2278--2324},
	file = {IEEE Xplore Abstract Record:/home/tobias/snap/zotero-snap/common/Zotero/storage/ZXLGVNR5/726791.html:text/html;IEEE Xplore Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/JPNHPKNW/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {arXiv Fulltext PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/WLMGLDTB/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/GBUA7JBN/1412.html:text/html},
}

@book{hutter_automated_2019,
	title = {Automated {Machine} {Learning}: {Methods}, {Systems}, {Challenges}},
	shorttitle = {Automated {Machine} {Learning}},
	url = {https://library.oapen.org/handle/20.500.12657/23012},
	abstract = {This open access book presents the first comprehensive overview of general methods in Automated Machine Learning (AutoML), collects descriptions of existing systems based on these methods, and discusses the first series of international challenges of AutoML systems. The recent success of commercial ML applications and the rapid growth of the field has created a high demand for off-the-shelf ML methods that can be used easily and without expert knowledge. However, many of the recent machine learning successes crucially rely on human experts, who manually select appropriate ML architectures (deep learning architectures or more traditional ML workflows) and their hyperparameters. To overcome this problem, the field of AutoML targets a progressive automation of machine learning, based on principles from optimization and machine learning itself. This book serves as a point of entry into this quickly-developing field for researchers and advanced students alike, as well as providing a reference for practitioners aiming to use AutoML in their work.},
	language = {English},
	urldate = {2023-03-21},
	publisher = {Springer Nature},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	year = {2019},
	doi = {10.1007/978-3-030-05318-5},
	note = {Accepted: 2020-03-18 13:36:15},
	keywords = {Pattern recognition, Artificial intelligence, bic Book Industry Communication::U Computing \& information technology::UY Computer science::UYQ Artificial intelligence, bic Book Industry Communication::U Computing \& information technology::UY Computer science::UYQ Artificial intelligence::UYQP Pattern recognition, bic Book Industry Communication::U Computing \& information technology::UY Computer science::UYT Image processing, Computer science, Optical data processing},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/TJ4SLVPC/Hutter et al. - 2019 - Automated Machine Learning Methods, Systems, Chal.pdf:application/pdf},
}

@misc{hanin_approximating_2018,
	title = {Approximating {Continuous} {Functions} by {ReLU} {Nets} of {Minimal} {Width}},
	url = {http://arxiv.org/abs/1710.11278},
	doi = {10.48550/arXiv.1710.11278},
	abstract = {This article concerns the expressive power of depth in deep feed-forward neural nets with ReLU activations. Specifically, we answer the following question: for a fixed \$d\_\{in\}{\textbackslash}geq 1,\$ what is the minimal width \$w\$ so that neural nets with ReLU activations, input dimension \$d\_\{in\}\$, hidden layer widths at most \$w,\$ and arbitrary depth can approximate any continuous, real-valued function of \$d\_\{in\}\$ variables arbitrarily well? It turns out that this minimal width is exactly equal to \$d\_\{in\}+1.\$ That is, if all the hidden layer widths are bounded by \$d\_\{in\}\$, then even in the infinite depth limit, ReLU nets can only express a very limited class of functions, and, on the other hand, any continuous function on the \$d\_\{in\}\$-dimensional unit cube can be approximated to arbitrary precision by ReLU nets in which all hidden layers have width exactly \$d\_\{in\}+1.\$ Our construction in fact shows that any continuous function \$f:[0,1]{\textasciicircum}\{d\_\{in\}\}{\textbackslash}to{\textbackslash}mathbb R{\textasciicircum}\{d\_\{out\}\}\$ can be approximated by a net of width \$d\_\{in\}+d\_\{out\}\$. We obtain quantitative depth estimates for such an approximation in terms of the modulus of continuity of \$f\$.},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Hanin, Boris and Sellke, Mark},
	month = mar,
	year = {2018},
	note = {arXiv:1710.11278 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computational Complexity, Mathematics - Combinatorics, Mathematics - Statistics Theory, Statistics - Machine Learning},
	annote = {Comment: v2. 13p. Extended main result to higher dimensional output. Comments welcome},
	file = {arXiv Fulltext PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/7768V5WE/Hanin and Sellke - 2018 - Approximating Continuous Functions by ReLU Nets of.pdf:application/pdf;arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/55ADGBVA/1710.html:text/html},
}

@inproceedings{han_mmptrack_2023,
	title = {{MMPTRACK}: {Large}-{Scale} {Densely} {Annotated} {Multi}-{Camera} {Multiple} {People} {Tracking} {Benchmark}},
	shorttitle = {{MMPTRACK}},
	url = {https://openaccess.thecvf.com/content/WACV2023/html/Han_MMPTRACK_Large-Scale_Densely_Annotated_Multi-Camera_Multiple_People_Tracking_Benchmark_WACV_2023_paper.html},
	language = {en},
	urldate = {2023-03-21},
	author = {Han, Xiaotian and You, Quanzeng and Wang, Chunyu and Zhang, Zhizheng and Chu, Peng and Hu, Houdong and Wang, Jiang and Liu, Zicheng},
	year = {2023},
	pages = {4860--4869},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/6UJZ4B5M/Han et al. - 2023 - MMPTRACK Large-Scale Densely Annotated Multi-Came.pdf:application/pdf},
}

@incollection{feurer_hyperparameter_2019,
	address = {Cham},
	series = {The {Springer} {Series} on {Challenges} in {Machine} {Learning}},
	title = {Hyperparameter {Optimization}},
	isbn = {978-3-030-05318-5},
	url = {https://doi.org/10.1007/978-3-030-05318-5_1},
	abstract = {Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning (AutoML) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization (HPO). In this chapter, we give an overview of the most prominent approaches for HPO. We first discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-fidelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.},
	language = {en},
	urldate = {2023-03-21},
	booktitle = {Automated {Machine} {Learning}: {Methods}, {Systems}, {Challenges}},
	publisher = {Springer International Publishing},
	author = {Feurer, Matthias and Hutter, Frank},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	year = {2019},
	doi = {10.1007/978-3-030-05318-5_1},
	pages = {3--33},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/2BLBP6KP/Feurer and Hutter - 2019 - Hyperparameter Optimization.pdf:application/pdf},
}

@inproceedings{nguyen_lmgp_2022,
	title = {{LMGP}: {Lifted} {Multicut} {Meets} {Geometry} {Projections} for {Multi}-{Camera} {Multi}-{Object} {Tracking}},
	shorttitle = {{LMGP}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Nguyen_LMGP_Lifted_Multicut_Meets_Geometry_Projections_for_Multi-Camera_Multi-Object_Tracking_CVPR_2022_paper.html},
	language = {en},
	urldate = {2023-03-21},
	author = {Nguyen, Duy M. H. and Henschel, Roberto and Rosenhahn, Bodo and Sonntag, Daniel and Swoboda, Paul},
	year = {2022},
	pages = {8866--8875},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/9R5UKP2Z/Nguyen et al. - 2022 - LMGP Lifted Multicut Meets Geometry Projections f.pdf:application/pdf},
}

@article{duchi_adaptive_2011,
	title = {Adaptive {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization}},
	volume = {12},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	number = {61},
	urldate = {2023-03-21},
	journal = {Journal of Machine Learning Research},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	year = {2011},
	pages = {2121--2159},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/BNDCA6BU/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.pdf:application/pdf},
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {1435-568X},
	url = {https://doi.org/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	language = {en},
	number = {4},
	urldate = {2023-03-21},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	keywords = {Neural networks, Approximation, Completeness},
	pages = {303--314},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/ZTRGLWZ9/Cybenko - 1989 - Approximation by superpositions of a sigmoidal fun.pdf:application/pdf},
}

@inproceedings{chavdarova_wildtrack_2018,
	title = {{WILDTRACK}: {A} {Multi}-{Camera} {HD} {Dataset} for {Dense} {Unscripted} {Pedestrian} {Detection}},
	shorttitle = {{WILDTRACK}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Chavdarova_WILDTRACK_A_Multi-Camera_CVPR_2018_paper.html},
	urldate = {2023-03-21},
	author = {Chavdarova, Tatjana and Baqué, Pierre and Bouquet, Stéphane and Maksai, Andrii and Jose, Cijo and Bagautdinov, Timur and Lettry, Louis and Fua, Pascal and Van Gool, Luc and Fleuret, François},
	year = {2018},
	pages = {5030--5039},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/ZIFLGCP2/Chavdarova et al. - 2018 - WILDTRACK A Multi-Camera HD Dataset for Dense Uns.pdf:application/pdf},
}

@inproceedings{carion_end--end_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	isbn = {978-3-030-58452-8},
	doi = {10.1007/978-3-030-58452-8_13},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {213--229},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/PLLK765L/Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-03-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/9FWGYJL6/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@misc{karpathy_transformer_2022,
	type = {Thread},
	title = {The {Transformer} is a magnificient neural network architecture},
	url = {https://twitter.com/karpathy/status/1582807367988654081?lang=en},
	urldate = {2023-03-03},
	journal = {twitter},
	author = {Karpathy, Andrej},
	month = oct,
	year = {2022},
}

@misc{vaswani_transformers_2021,
	address = {Stanford University},
	type = {Guest lecture},
	title = {Transformers and {Self}-{Attention}},
	url = {https://www.youtube.com/watch?v=5vcj8kSwBCY},
	urldate = {2023-03-03},
	author = {Vaswani, Ashish and Huang, Anna},
	month = mar,
	year = {2021},
}

@misc{kafunah_backpropagation_2016,
	title = {Backpropagation {In} {Convolutional} {Neural} {Networks}},
	url = {https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/},
	urldate = {2023-02-28},
	journal = {Backpropagation In Convolutional Neural Networks},
	author = {Kafunah, Jefkine},
	month = may,
	year = {2016},
}

@misc{arat_backpropagation_2019,
	title = {Backpropagation {Through} {Time} for {Recurrent} {Neural} {Network}},
	url = {https://mmuratarat.github.io/2019-02-07/bptt-of-rnn},
	urldate = {2023-02-28},
	author = {Arat, Mustafa Murat},
	month = feb,
	year = {2019},
}

@misc{hinton_lecture_2012,
	title = {Lecture	6a {Overview} of	mini-batch gradient descent},
	url = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
	urldate = {2023-02-03},
	author = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
	year = {2012},
	annote = {Accessed: 2020–11-06},
}

@phdthesis{karpathy_connecting_2016,
	type = {Dissertation},
	title = {Connecting {Images} and {Natural} {Language}},
	url = {https://cs.stanford.edu/people/karpathy/main.pdf},
	school = {Stanford University},
	author = {Karpathy, Andrej},
	month = aug,
	year = {2016},
	file = {Andrej - 2016 - Connecting Images and Natural Language.pdf:/home/tobias/snap/zotero-snap/common/Zotero/storage/L5ZM9L6H/Andrej - 2016 - Connecting Images and Natural Language.pdf:application/pdf},
}

@misc{li_cs231n_2018,
	title = {{CS231n}: {Deep} {Learning} for {Computer} {Vision}},
	url = {http://cs231n.stanford.edu/index.html},
	urldate = {2023-02-02},
	author = {Li, Fei-Fei and Wu, Jiajun and Gao, Ruohan},
	year = {2018},
	annote = {Accessed: 2020–11-06},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	annote = {http://www.deeplearningbook.org},
	file = {Goodfellow et al. - 2016 - Deep Learning.pdf:/home/tobias/snap/zotero-snap/common/Zotero/storage/C7BJ56AN/Goodfellow et al. - 2016 - Deep Learning.pdf:application/pdf},
}

@misc{alammar_illustrated_2018,
	title = {The {Illustrated} {Transformer}},
	url = {https://jalammar.github.io/illustrated-transformer/},
	urldate = {2023-01-25},
	author = {Alammar, Jay},
	month = jun,
	year = {2018},
}

@misc{weng_attention_2018,
	title = {Attention? {Attention}!},
	url = {https://lilianweng.github.io/posts/2018-06-24-attention/},
	urldate = {2023-01-25},
	author = {Weng, Lilian},
	month = jun,
	year = {2018},
}

@misc{noauthor_hundred-page_nodate,
	title = {The {Hundred}-{Page} {Machine} {Learning} {Book} : {Burkov}, {Andriy}: {Amazon}.de: {Bücher}},
	url = {https://www.amazon.de/-/en/Andriy-Burkov/dp/199957950X},
	urldate = {2023-05-13},
	file = {The Hundred-Page Machine Learning Book  Burkov, A.pdf:/home/tobias/snap/zotero-snap/common/Zotero/storage/8CCFS9VA/The Hundred-Page Machine Learning Book  Burkov, A.pdf:application/pdf;The Hundred-Page Machine Learning Book \: Burkov, Andriy\: Amazon.de\: Bücher:/home/tobias/snap/zotero-snap/common/Zotero/storage/5WXBLRM8/199957950X.html:text/html},
}

@misc{noauthor_cracking_nodate,
	title = {Cracking {The} {Machine} {Learning} {Interview} : {Suri}, {Nitin}: {Amazon}.de: {Bücher}},
	url = {https://www.amazon.de/-/en/Nitin-Suri/dp/1729223605},
	urldate = {2023-05-13},
	file = {Cracking The Machine Learning Interview  Suri, Ni.pdf:/home/tobias/snap/zotero-snap/common/Zotero/storage/GL4JTCUE/Cracking The Machine Learning Interview  Suri, Ni.pdf:application/pdf;Cracking The Machine Learning Interview \: Suri, Nitin\: Amazon.de\: Bücher:/home/tobias/snap/zotero-snap/common/Zotero/storage/6SS8DF6U/1729223605.html:text/html},
}

@article{kuhn_hungarian_1955,
	title = {The {Hungarian} method for the assignment problem},
	volume = {2},
	issn = {1931-9193},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109},
	doi = {10.1002/nav.3800020109},
	abstract = {Assuming that numerical scores are available for the performance of each of n persons on each of n jobs, the “assignment problem” is the quest for an assignment of persons to jobs so that the sum of the n scores so obtained is as large as possible. It is shown that ideas latent in the work of two Hungarian mathematicians may be exploited to yield a new method of solving this problem.},
	language = {en},
	number = {1-2},
	urldate = {2023-03-21},
	journal = {Naval Research Logistics Quarterly},
	author = {Kuhn, H. W.},
	year = {1955},
	pages = {83--97},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/VWKB7TMI/Kuhn - 1955 - The Hungarian method for the assignment problem.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/4KRPGFEM/nav.html:text/html},
}

@misc{dendorfer_mot20_2020,
	title = {{MOT20}: {A} benchmark for multi object tracking in crowded scenes},
	shorttitle = {{MOT20}},
	url = {http://arxiv.org/abs/2003.09003},
	doi = {10.48550/arXiv.2003.09003},
	abstract = {Standardized benchmarks are crucial for the majority of computer vision applications. Although leaderboards and ranking tables should not be over-claimed, benchmarks often provide the most objective measure of performance and are therefore important guides for research. The benchmark for Multiple Object Tracking, MOTChallenge, was launched with the goal to establish a standardized evaluation of multiple object tracking methods. The challenge focuses on multiple people tracking, since pedestrians are well studied in the tracking community, and precise tracking and detection has high practical relevance. Since the first release, MOT15, MOT16, and MOT17 have tremendously contributed to the community by introducing a clean dataset and precise framework to benchmark multi-object trackers. In this paper, we present our MOT20benchmark, consisting of 8 new sequences depicting very crowded challenging scenes. The benchmark was presented first at the 4thBMTT MOT Challenge Workshop at the Computer Vision and Pattern Recognition Conference (CVPR) 2019, and gives to chance to evaluate state-of-the-art methods for multiple object tracking when handling extremely crowded scenarios.},
	urldate = {2023-05-06},
	publisher = {arXiv},
	author = {Dendorfer, Patrick and Rezatofighi, Hamid and Milan, Anton and Shi, Javen and Cremers, Daniel and Reid, Ian and Roth, Stefan and Schindler, Konrad and Leal-Taixé, Laura},
	month = mar,
	year = {2020},
	note = {arXiv:2003.09003 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: The sequences of the new MOT20 benchmark were previously presented in the CVPR 2019 tracking challenge ( arXiv:1906.04567 ). The differences between the two challenges are: - New and corrected annotations - New sequences, as we had to crop and transform some old sequences to achieve higher quality in the annotations. - New baselines evaluations and different sets of public detections},
	file = {arXiv Fulltext PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/N8QNFIDV/Dendorfer et al. - 2020 - MOT20 A benchmark for multi object tracking in cr.pdf:application/pdf;arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/5MCMP4DG/2003.html:text/html},
}

@inproceedings{alahi_socially-aware_2014,
	address = {Columbus, OH, USA},
	title = {Socially-{Aware} {Large}-{Scale} {Crowd} {Forecasting}},
	isbn = {978-1-4799-5118-5},
	url = {https://ieeexplore.ieee.org/document/6909680},
	doi = {10.1109/CVPR.2014.283},
	abstract = {In crowded spaces such as city centers or train stations, human mobility looks complex, but is often inﬂuenced only by a few causes. We propose to quantitatively study crowded environments by introducing a dataset of 42 million trajectories collected in train stations. Given this dataset, we address the problem of forecasting pedestrians’ destinations, a central problem in understanding large-scale crowd mobility. We need to overcome the challenges posed by a limited number of observations (e.g. sparse cameras), and change in pedestrian appearance cues across different cameras. In addition, we often have restrictions in the way pedestrians can move in a scene, encoded as priors over origin and destination (OD) preferences. We propose a new descriptor coined as Social Afﬁnity Maps (SAM) to link broken or unobserved trajectories of individuals in the crowd, while using the OD-prior in our framework. Our experiments show improvement in performance through the use of SAM features and OD prior. To the best of our knowledge, our work is one of the ﬁrst studies that provides encouraging results towards a better understanding of crowd behavior at the scale of million pedestrians.},
	language = {en},
	urldate = {2023-05-06},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Alahi, Alexandre and Ramanathan, Vignesh and Fei-Fei, Li},
	month = jun,
	year = {2014},
	pages = {2211--2218},
	file = {Alahi et al. - 2014 - Socially-Aware Large-Scale Crowd Forecasting.pdf:/home/tobias/snap/zotero-snap/common/Zotero/storage/6V5XRX3F/Alahi et al. - 2014 - Socially-Aware Large-Scale Crowd Forecasting.pdf:application/pdf},
}

@misc{milan_mot16_2016,
	title = {{MOT16}: {A} {Benchmark} for {Multi}-{Object} {Tracking}},
	shorttitle = {{MOT16}},
	url = {http://arxiv.org/abs/1603.00831},
	abstract = {Standardized benchmarks are crucial for the majority of computer vision applications. Although leaderboards and ranking tables should not be over-claimed, benchmarks often provide the most objective measure of performance and are therefore important guides for reseach. Recently, a new benchmark for Multiple Object Tracking, MOTChallenge, was launched with the goal of collecting existing and new data and creating a framework for the standardized evaluation of multiple object tracking methods. The first release of the benchmark focuses on multiple people tracking, since pedestrians are by far the most studied object in the tracking community. This paper accompanies a new release of the MOTChallenge benchmark. Unlike the initial release, all videos of MOT16 have been carefully annotated following a consistent protocol. Moreover, it not only offers a significant increase in the number of labeled boxes, but also provides multiple object classes beside pedestrians and the level of visibility for every single object of interest.},
	urldate = {2023-05-06},
	publisher = {arXiv},
	author = {Milan, Anton and Leal-Taixe, Laura and Reid, Ian and Roth, Stefan and Schindler, Konrad},
	month = may,
	year = {2016},
	note = {arXiv:1603.00831 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: arXiv admin note: substantial text overlap with arXiv:1504.01942},
	file = {arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/2IRP28DW/1603.html:text/html;Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/4MU37EM7/Milan et al. - 2016 - MOT16 A Benchmark for Multi-Object Tracking.pdf:application/pdf},
}

@misc{leal-taixe_motchallenge_2015,
	title = {{MOTChallenge} 2015: {Towards} a {Benchmark} for {Multi}-{Target} {Tracking}},
	shorttitle = {{MOTChallenge} 2015},
	url = {http://arxiv.org/abs/1504.01942},
	abstract = {In the recent past, the computer vision community has developed centralized benchmarks for the performance evaluation of a variety of tasks, including generic object and pedestrian detection, 3D reconstruction, optical flow, single-object short-term tracking, and stereo estimation. Despite potential pitfalls of such benchmarks, they have proved to be extremely helpful to advance the state of the art in the respective area. Interestingly, there has been rather limited work on the standardization of quantitative benchmarks for multiple target tracking. One of the few exceptions is the well-known PETS dataset, targeted primarily at surveillance applications. Despite being widely used, it is often applied inconsistently, for example involving using different subsets of the available data, different ways of training the models, or differing evaluation scripts. This paper describes our work toward a novel multiple object tracking benchmark aimed to address such issues. We discuss the challenges of creating such a framework, collecting existing and new data, gathering state-of-the-art methods to be tested on the datasets, and finally creating a unified evaluation system. With MOTChallenge we aim to pave the way toward a unified evaluation framework for a more meaningful quantification of multi-target tracking.},
	urldate = {2023-05-06},
	publisher = {arXiv},
	author = {Leal-Taixé, Laura and Milan, Anton and Reid, Ian and Roth, Stefan and Schindler, Konrad},
	month = apr,
	year = {2015},
	note = {arXiv:1504.01942 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/6KLELKB5/1504.html:text/html;Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/YZN8663D/Leal-Taixé et al. - 2015 - MOTChallenge 2015 Towards a Benchmark for Multi-T.pdf:application/pdf},
}

@article{bernardin_evaluating_2008,
	title = {Evaluating multiple object tracking performance: the clear mot metrics},
	volume = {2008},
	shorttitle = {Evaluating multiple object tracking performance},
	journal = {EURASIP Journal on Image and Video Processing},
	author = {Bernardin, Keni and Stiefelhagen, Rainer},
	year = {2008},
	note = {Publisher: Springer},
	pages = {1--10},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/E28IBWJB/Bernardin and Stiefelhagen - 2008 - Evaluating multiple object tracking performance t.pdf:application/pdf},
}

@inproceedings{yamaguchi_who_2011,
	title = {Who are you with and where are you going?},
	doi = {10.1109/CVPR.2011.5995468},
	abstract = {We propose an agent-based behavioral model of pedestrians to improve tracking performance in realistic scenarios. In this model, we view pedestrians as decision-making agents who consider a plethora of personal, social, and environmental factors to decide where to go next. We formulate prediction of pedestrian behavior as an energy minimization on this model. Two of our main contributions are simple, yet effective estimates of pedestrian destination and social relationships (groups). Our final contribution is to incorporate these hidden properties into an energy formulation that results in accurate behavioral prediction. We evaluate both our estimates of destination and grouping, as well as our accuracy at prediction and tracking against state of the art behavioral model and show improvements, especially in the challenging observational situation of infrequent appearance observations-something that might occur in thousands of webcams available on the Internet.},
	booktitle = {{CVPR} 2011},
	author = {Yamaguchi, Kota and Berg, Alexander C. and Ortiz, Luis E. and Berg, Tamara L.},
	month = jun,
	year = {2011},
	note = {ISSN: 1063-6919},
	keywords = {Biological system modeling, Computational modeling, Context modeling, Estimation, Histograms, Predictive models, Trajectory},
	pages = {1345--1352},
	file = {IEEE Xplore Abstract Record:/home/tobias/snap/zotero-snap/common/Zotero/storage/WEB5FSQI/5995468.html:text/html;IEEE Xplore Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/DC9XK69D/Yamaguchi et al. - 2011 - Who are you with and where are you going.pdf:application/pdf},
}

@inproceedings{yang_learning_2011,
	title = {Learning affinities and dependencies for multi-target tracking using a {CRF} model},
	doi = {10.1109/CVPR.2011.5995587},
	abstract = {We propose a learning-based Conditional Random Field (CRF) model for tracking multiple targets by progressively associating detection responses into long tracks. Tracking task is transformed into a data association problem, and most previous approaches developed heuristical parametric models or learning approaches for evaluating independent affinities between track fragments (tracklets). We argue that the independent assumption is not valid in many cases, and adopt a CRF model to consider both tracklet affinities and dependencies among them, which are represented by unary term costs and pairwise term costs respectively. Unlike previous methods, we learn the best global associations instead of the best local affinities between tracklets, and transform the task of finding the best association into an energy minimization problem. A RankBoost algorithm is proposed to select effective features for estimation of term costs in the CRF model, so that better associations have lower costs. Our approach is evaluated on challenging pedestrian data sets, and are compared with state-of-art methods. Experiments show effectiveness of our algorithm as well as improvement in tracking performance.},
	booktitle = {{CVPR} 2011},
	author = {Yang, Bo and Huang, Chang and Nevatia, Ram},
	month = jun,
	year = {2011},
	note = {ISSN: 1063-6919},
	keywords = {Feature extraction, Estimation, Head, Minimization, Target tracking, Training},
	pages = {1233--1240},
	file = {IEEE Xplore Abstract Record:/home/tobias/snap/zotero-snap/common/Zotero/storage/VSAKTYSJ/stamp.html:text/html;IEEE Xplore Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/C5T6NFQ6/Yang et al. - 2011 - Learning affinities and dependencies for multi-tar.pdf:application/pdf},
}

@article{zhao_object_2019,
	title = {Object {Detection} {With} {Deep} {Learning}: {A} {Review}},
	volume = {30},
	issn = {2162-2388},
	shorttitle = {Object {Detection} {With} {Deep} {Learning}},
	doi = {10.1109/TNNLS.2018.2876865},
	abstract = {Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.},
	number = {11},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-Tao and Wu, Xindong},
	month = nov,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Feature extraction, Neural networks, Training, Computer architecture, Deep learning, neural network, object detection, Object detection, Task analysis},
	pages = {3212--3232},
	file = {IEEE Xplore Abstract Record:/home/tobias/snap/zotero-snap/common/Zotero/storage/GQXXBNPM/stamp.html:text/html;IEEE Xplore Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/GQJS75G3/Zhao et al. - 2019 - Object Detection With Deep Learning A Review.pdf:application/pdf},
}

@article{rezatofighi_multi-target_2015,
	title = {Multi-{Target} {Tracking} {With} {Time}-{Varying} {Clutter} {Rate} and {Detection} {Profile}: {Application} to {Time}-{Lapse} {Cell} {Microscopy} {Sequences}},
	volume = {34},
	issn = {1558-254X},
	shorttitle = {Multi-{Target} {Tracking} {With} {Time}-{Varying} {Clutter} {Rate} and {Detection} {Profile}},
	doi = {10.1109/TMI.2015.2390647},
	abstract = {Quantitative analysis of the dynamics of tiny cellular and sub-cellular structures, known as particles, in time-lapse cell microscopy sequences requires the development of a reliable multi-target tracking method capable of tracking numerous similar targets in the presence of high levels of noise, high target density, complex motion patterns and intricate interactions. In this paper, we propose a framework for tracking these structures based on the random finite set Bayesian filtering framework. We focus on challenging biological applications where image characteristics such as noise and background intensity change during the acquisition process. Under these conditions, detection methods usually fail to detect all particles and are often followed by missed detections and many spurious measurements with unknown and time-varying rates. To deal with this, we propose a bootstrap filter composed of an estimator and a tracker. The estimator adaptively estimates the required meta parameters for the tracker such as clutter rate and the detection probability of the targets, while the tracker estimates the state of the targets. Our results show that the proposed approach can outperform state-of-the-art particle trackers on both synthetic and real data in this regime.},
	number = {6},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Rezatofighi, Seyed Hamid and Gould, Stephen and Vo, Ba Tuong and Vo, Ba-Ngu and Mele, Katarina and Hartley, Richard},
	month = jun,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Target tracking, Bayes methods, Bayesian estimation, cardinalized probability hypothesis density (CPHD), Clutter, clutter rate, Degradation, detection probability, fluorescence microscopy, Insulation life, Mathematical model, multi-target tracking, particle tracking, random finite set, Time measurement},
	pages = {1336--1348},
	file = {IEEE Xplore Abstract Record:/home/tobias/snap/zotero-snap/common/Zotero/storage/5KDQB4DN/7006807.html:text/html;IEEE Xplore Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/EF9ILBTG/Rezatofighi et al. - 2015 - Multi-Target Tracking With Time-Varying Clutter Ra.pdf:application/pdf},
}

@article{ahmed_towards_2021,
	title = {Towards {Collaborative} {Robotics} in {Top} {View} {Surveillance}: {A} {Framework} for {Multiple} {Object} {Tracking} by {Detection} {Using} {Deep} {Learning}},
	volume = {8},
	issn = {2329-9266, 2329-9274},
	shorttitle = {Towards {Collaborative} {Robotics} in {Top} {View} {Surveillance}},
	url = {https://ieeexplore.ieee.org/document/9242337/},
	doi = {10.1109/JAS.2020.1003453},
	language = {en},
	number = {7},
	urldate = {2023-05-05},
	journal = {IEEE/CAA Journal of Automatica Sinica},
	author = {Ahmed, Imran and Din, Sadia and Jeon, Gwanggil and Piccialli, Francesco and Fortino, Giancarlo},
	month = jul,
	year = {2021},
	pages = {1253--1270},
	file = {Ahmed et al. - 2021 - Towards Collaborative Robotics in Top View Surveil.pdf:/home/tobias/snap/zotero-snap/common/Zotero/storage/BZMUEGDC/Ahmed et al. - 2021 - Towards Collaborative Robotics in Top View Surveil.pdf:application/pdf},
}

@inproceedings{park_multiple_2008,
	title = {Multiple {3D} {Object} tracking for augmented reality},
	doi = {10.1109/ISMAR.2008.4637336},
	abstract = {We present a method that is able to track several 3D objects simultaneously, robustly, and accurately in real-time. While many applications need to consider more than one object in practice, the existing methods for single object tracking do not scale well with the number of objects, and a proper way to deal with several objects is required. Our method combines object detection and tracking: Frame-to-frame tracking is less computationally demanding but is prone to fail, while detection is more robust but slower. We show how to combine them to take the advantages of the two approaches, and demonstrate our method on several real sequences.},
	booktitle = {2008 7th {IEEE}/{ACM} {International} {Symposium} on {Mixed} and {Augmented} {Reality}},
	author = {Park, Youngmin and Lepetit, Vincent and Woo, Woontack},
	month = sep,
	year = {2008},
	keywords = {Feature extraction, Target tracking, Object detection, Augmented Reality, Computer Vision, Robustness, Solid modeling, Three dimensional displays, Tracking},
	pages = {117--120},
	file = {IEEE Xplore Abstract Record:/home/tobias/snap/zotero-snap/common/Zotero/storage/FE694PA4/4637336.html:text/html;IEEE Xplore Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/FA9TI5C5/Park et al. - 2008 - Multiple 3D Object tracking for augmented reality.pdf:application/pdf},
}

@article{romeas_3d-multiple_2016,
	title = {{3D}-{Multiple} {Object} {Tracking} training task improves passing decision-making accuracy in soccer players},
	volume = {22},
	issn = {1469-0292},
	url = {https://www.sciencedirect.com/science/article/pii/S1469029215000631},
	doi = {10.1016/j.psychsport.2015.06.002},
	abstract = {Objectives
The ability to perform a context-free 3-dimensional multiple object tracking (3D-MOT) task has been highly related to athletic performance. In the present study, we assessed the transferability of a perceptual-cognitive 3D-MOT training from a laboratory setting to a soccer field, a sport in which the capacity to correctly read the dynamic visual scene is a prerequisite to performance.
Design
Throughout pre- and post-training sessions, we looked at three essential skills (passing, dribbling, shooting) that are used to gain the upper hand over the opponent.
Method
We recorded decision-making accuracy during small-sided games in university-level soccer players (n = 23) before and after a training protocol. Experimental (n = 9) and active control (n = 7) groups were respectively trained during 10 sessions of 3D-MOT or 3D soccer videos. A passive control group (n = 7) did not received any particular training or instructions.
Results
Decision-making accuracy in passing, but not in dribbling and shooting, between pre- and post-sessions was superior for the 3D-MOT trained group compared to control groups. This result was correlated with the players' subjective decision-making accuracy, rated after pre- and post-sessions through a visual analogue scale questionnaire.
Conclusions
To our knowledge, this study represents the first evidence in which a non-contextual, perceptual-cognitive training exercise has a transfer effect onto the field in athletes.},
	language = {en},
	urldate = {2023-05-05},
	journal = {Psychology of Sport and Exercise},
	author = {Romeas, Thomas and Guldner, Antoine and Faubert, Jocelyn},
	month = jan,
	year = {2016},
	keywords = {Athletes, Perceptual-cognitive training, Small-sided games, Transfer},
	pages = {1--9},
	file = {ScienceDirect Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/I3UVG7NC/Romeas et al. - 2016 - 3D-Multiple Object Tracking training task improves.pdf:application/pdf;ScienceDirect Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/MSWZMHXG/S1469029215000631.html:text/html},
}

@misc{noauthor_particle_nodate,
	title = {Particle {Filtering} for {Multiple} {Object} {Tracking} in {Dynamic} {Fluorescence} {Microscopy} {Images}: {Application} to {Microtubule} {Growth} {Analysis}},
	shorttitle = {Particle {Filtering} for {Multiple} {Object} {Tracking} in {Dynamic} {Fluorescence} {Microscopy} {Images}},
	url = {https://ieeexplore.ieee.org/abstract/document/4436039/?casa_token=EA753LPR3loAAAAA:14hkEeCDTs-_9qwyh0AWJlFohSaYiYpbvI5wBu1AgqCn7jz4Y2x0TYQSVb-SwLPPYIQf_uDb5A},
	abstract = {Quantitative analysis of dynamic processes in living cells by means of fluorescence microscopy imaging requires tracking of hundreds of bright spots in noisy image sequences. Deterministic approaches, which use object detection prior to tracking, perform poorly in the case of noisy image data. We propose an improved, completely automatic tracker, built within a Bayesian probabilistic framework. It better exploits spatiotemporal information and prior knowledge than common approaches, yielding more robust tracking also in cases of photobleaching and object interaction. The tracking method was evaluated using simulated but realistic image sequences, for which ground truth was available. The results of these experiments show that the method is more accurate and robust than popular tracking methods. In addition, validation experiments were conducted with real fluorescence microscopy image data acquired for microtubule growth analysis. These demonstrate that the method yields results that are in good agreement with manual tracking performed by expert cell biologists. Our findings suggest that the method may replace laborious manual procedures.},
	language = {en-US},
	urldate = {2023-05-05},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/L9JCDGQM/4436039.html:text/html},
}

@misc{chiu_probabilistic_2020,
	title = {Probabilistic {3D} {Multi}-{Object} {Tracking} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2001.05673},
	doi = {10.48550/arXiv.2001.05673},
	abstract = {3D multi-object tracking is a key module in autonomous driving applications that provides a reliable dynamic representation of the world to the planning module. In this paper, we present our on-line tracking method, which made the first place in the NuScenes Tracking Challenge, held at the AI Driving Olympics Workshop at NeurIPS 2019. Our method estimates the object states by adopting a Kalman Filter. We initialize the state covariance as well as the process and observation noise covariance with statistics from the training set. We also use the stochastic information from the Kalman Filter in the data association step by measuring the Mahalanobis distance between the predicted object states and current object detections. Our experimental results on the NuScenes validation and test set show that our method outperforms the AB3DMOT baseline method by a large margin in the Average Multi-Object Tracking Accuracy (AMOTA) metric.},
	urldate = {2023-05-05},
	publisher = {arXiv},
	author = {Chiu, Hsu-kuang and Prioletti, Antonio and Li, Jie and Bohg, Jeannette},
	month = jan,
	year = {2020},
	note = {arXiv:2001.05673 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/GS9Q4YIJ/Chiu et al. - 2020 - Probabilistic 3D Multi-Object Tracking for Autonom.pdf:application/pdf;arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/5RRV6JXD/2001.html:text/html},
}

@article{elhoseny_multi-object_2020,
	title = {Multi-object {Detection} and {Tracking} ({MODT}) {Machine} {Learning} {Model} for {Real}-{Time} {Video} {Surveillance} {Systems}},
	volume = {39},
	issn = {1531-5878},
	url = {https://doi.org/10.1007/s00034-019-01234-7},
	doi = {10.1007/s00034-019-01234-7},
	abstract = {Recently, video surveillance has garnered considerable attention in various real-time applications. Due to advances in the field of machine learning, numerous techniques have been developed for multi-object detection and tracking (MODT). This paper introduces a new MODT methodology. The proposed method uses an optimal Kalman filtering technique to track the moving objects in video frames. The video clips were converted based on the number of frames into morphological operations using the region growing model. After distinguishing the objects, Kalman filtering was applied for parameter optimization using the probability-based grasshopper algorithm. Using the optimal parameters, the selected objects were tracked in each frame by a similarity measure. Finally, the proposed MODT framework was executed, and the results were assessed. The experiments showed that the MODT framework achieved maximum detection and tracking accuracies of 76.23\% and 86.78\%, respectively. The results achieved with Kalman filtering in the MODT process are compared with the results of previous studies.},
	language = {en},
	number = {2},
	urldate = {2023-05-05},
	journal = {Circuits, Systems, and Signal Processing},
	author = {Elhoseny, M.},
	month = feb,
	year = {2020},
	keywords = {Machine learning, Kalman filtering, Multi objection detection and tracking (MODT), Region growing, Video surveillance},
	pages = {611--630},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/SFBWJ66W/Elhoseny - 2020 - Multi-object Detection and Tracking (MODT) Machine.pdf:application/pdf},
}

@misc{leal-taixe_dynamic_2022,
	title = {Dynamic {Vision} and {Learning}: {Computer} {Vision} {III}: {Detection}, {Segmentation} and {Tracking} ({CV3DST}) ({IN2375})},
	url = {https://dvl.in.tum.de/teaching/cv3dst-ss22/},
	urldate = {2023-05-05},
	author = {Leal-Taixé, Laura},
	year = {2022},
	file = {Dynamic Vision and Learning\: Computer Vision III\: Detection, Segmentation and Tracking (CV3DST) (IN2375):/home/tobias/snap/zotero-snap/common/Zotero/storage/A3GHH9MU/cv3dst-ss22.html:text/html},
}

@inproceedings{chavdarova_deep_2017,
	title = {Deep {Multi}-camera {People} {Detection}},
	doi = {10.1109/ICMLA.2017.00-50},
	abstract = {This paper addresses the problem of multi-view people occupancy map estimation. Existing solutions either operate per-view, or rely on a background subtraction preprocessing. Both approaches lessen the detection performance as scenes become more crowded. The former does not exploit joint information, whereas the latter deals with ambiguous input due to the foreground blobs becoming more and more interconnected as the number of targets increases. Although deep learning algorithms have proven to excel on remarkably numerous computer vision tasks, such a method has not been applied yet to this problem. In large part this is due to the lack of large-scale multi-camera data-set. The core of our method is an architecture which makes use of monocular pedestrian data-set, available at larger scale than the multi-view ones, applies parallel processing to the multiple video streams, and jointly utilises it. Our end-to-end deep learning method outperforms existing methods by large margins on the commonly used PETS 2009 data-set. Furthermore, we make publicly available a new three-camera HD data-set.},
	booktitle = {2017 16th {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Chavdarova, Tatjana and Fleuret, Francois},
	month = dec,
	year = {2017},
	keywords = {Machine learning, Estimation, Training, Computer architecture, Object detection, Cameras, CNN, Detectors, Multi Camera People Detection, Occupancy Map},
	pages = {848--853},
	file = {IEEE Xplore Abstract Record:/home/tobias/snap/zotero-snap/common/Zotero/storage/XXVHUJDV/8260742.html:text/html;IEEE Xplore Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/HK8YPGPQ/Chavdarova and Fleuret - 2017 - Deep Multi-camera People Detection.pdf:application/pdf},
}

@article{alameda-pineda_salsa_2016,
	title = {{SALSA}: {A} {Novel} {Dataset} for {Multimodal} {Group} {Behavior} {Analysis}},
	volume = {38},
	issn = {1939-3539},
	shorttitle = {{SALSA}},
	doi = {10.1109/TPAMI.2015.2496269},
	abstract = {Studying free-standing conversational groups (FCGs) in unstructured social settings (e.g., cocktail party) is gratifying due to the wealth of information available at the group (mining social networks) and individual (recognizing native behavioral and personality traits) levels. However, analyzing social scenes involving FCGs is also highly challenging due to the difficulty in extracting behavioral cues such as target locations, their speaking activity and head/body pose due to crowdedness and presence of extreme occlusions. To this end, we propose SALSA, a novel dataset facilitating multimodal and Synergetic sociAL Scene Analysis, and make two main contributions to research on automated social interaction analysis: (1) SALSA records social interactions among 18 participants in a natural, indoor environment for over 60 minutes, under the poster presentation and cocktail party contexts presenting difficulties in the form of low-resolution images, lighting variations, numerous occlusions, reverberations and interfering sound sources; (2) To alleviate these problems we facilitate multimodal analysis by recording the social interplay using four static surveillance cameras and sociometric badges worn by each participant, comprising the microphone, accelerometer, bluetooth and infrared sensors. In addition to raw data, we also provide annotations concerning individuals' personality as well as their position, head, body orientation and F-formation information over the entire event duration. Through extensive experiments with state-of-the-art approaches, we show (a) the limitations of current methods and (b) how the recorded multiple cues synergetically aid automatic analysis of social interactions. SALSA is available at http://tev.fbk.eu/salsa.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Alameda-Pineda, Xavier and Staiano, Jacopo and Subramanian, Ramanathan and Batrinca, Ligia and Ricci, Elisa and Lepri, Bruno and Lanz, Oswald and Sebe, Nicu},
	month = aug,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Head, Cameras, Bluetooth, F-formations, free-standing conversational groups, head and body pose, Magnetic heads, Microphones, Multimodal group behavior analysis, multimodal social data sets, personality traits, Sensors, speaker recognition, Surveillance, tracking},
	pages = {1707--1720},
	file = {IEEE Xplore Abstract Record:/home/tobias/snap/zotero-snap/common/Zotero/storage/S7AKY73M/7313015.html:text/html;IEEE Xplore Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/P5WCV6XL/Alameda-Pineda et al. - 2016 - SALSA A Novel Dataset for Multimodal Group Behavi.pdf:application/pdf},
}

@article{felzenszwalb_efficient_2006,
	title = {Efficient {Belief} {Propagation} for {Early} {Vision}},
	volume = {70},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-006-7899-4},
	doi = {10.1007/s11263-006-7899-4},
	abstract = {Markov random field models provide a robust and unified framework for early vision problems such as stereo and image restoration. Inference algorithms based on graph cuts and belief propagation have been found to yield accurate results, but despite recent advances are often too slow for practical use. In this paper we present some algorithmic techniques that substantially improve the running time of the loopy belief propagation approach. One of the techniques reduces the complexity of the inference algorithm to be linear rather than quadratic in the number of possible labels for each pixel, which is important for problems such as image restoration that have a large label set. Another technique speeds up and reduces the memory requirements of belief propagation on grid graphs. A third technique is a multi-grid method that makes it possible to obtain good results with a small fixed number of message passing iterations, independent of the size of the input images. Taken together these techniques speed up the standard algorithm by several orders of magnitude. In practice we obtain results that are as accurate as those of other global methods (e.g., using the Middlebury stereo benchmark) while being nearly as fast as purely local methods.},
	language = {en},
	number = {1},
	urldate = {2023-05-01},
	journal = {International Journal of Computer Vision},
	author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
	month = oct,
	year = {2006},
	keywords = {belief propagation, efficient algorithms, image restoration, Markov random fields, stereo},
	pages = {41--54},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/H759HQJP/Felzenszwalb and Huttenlocher - 2006 - Efficient Belief Propagation for Early Vision.pdf:application/pdf},
}

@misc{noauthor_efficient_nodate,
	title = {Efficient {Belief} {Propagation} for {Early} {Vision} {\textbar} {SpringerLink}},
	url = {https://link.springer.com/article/10.1007/s11263-006-7899-4},
	urldate = {2023-05-01},
	file = {Efficient Belief Propagation for Early Vision | SpringerLink:/home/tobias/snap/zotero-snap/common/Zotero/storage/UBUF3BZB/s11263-006-7899-4.html:text/html},
}

@inproceedings{yang_exploit_2016,
	title = {Exploit {All} the {Layers}: {Fast} and {Accurate} {CNN} {Object} {Detector} {With} {Scale} {Dependent} {Pooling} and {Cascaded} {Rejection} {Classifiers}},
	shorttitle = {Exploit {All} the {Layers}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Yang_Exploit_All_the_CVPR_2016_paper.html},
	urldate = {2023-05-01},
	author = {Yang, Fan and Choi, Wongun and Lin, Yuanqing},
	year = {2016},
	pages = {2129--2137},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/R4SYJXHP/Yang et al. - 2016 - Exploit All the Layers Fast and Accurate CNN Obje.pdf:application/pdf},
}

@inproceedings{ren_faster_2015,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	volume = {28},
	shorttitle = {Faster {R}-{CNN}},
	url = {https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster\_rcnn.},
	urldate = {2023-05-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	year = {2015},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/3UJ5QRHS/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf},
}

@inproceedings{ellis_pets2010_2010,
	title = {{PETS2010} and {PETS2009} {Evaluation} of {Results} {Using} {Individual} {Ground} {Truthed} {Single} {Views}},
	doi = {10.1109/AVSS.2010.89},
	abstract = {This paper presents the results of the crowd image analysis challenge of the PETS2010 workshop. The evaluation was carried out using a selection of the metrics developed in the Video Analysis and Content Extraction (VACE) program and the CLassification of Events, Activities, and Relationships (CLEAR) consortium. The PETS 2010 evaluation was performed using new ground truthing created from each independant 2D view. In addition, the performance of the submissions to the PETS 2009 and Winter-PETS 2009 were evaluated and included in the results. The evaluation highlights the detection and tracking performance of the authors' systems in areas such as precision, accuracy and robustness.},
	booktitle = {2010 7th {IEEE} {International} {Conference} on {Advanced} {Video} and {Signal} {Based} {Surveillance}},
	author = {Ellis, A. and Ferryman, J.},
	month = aug,
	year = {2010},
	keywords = {Object detection, Robustness, Cameras, Accuracy, Conferences, Measurement, Positron emission tomography},
	pages = {135--142},
	file = {IEEE Xplore Abstract Record:/home/tobias/snap/zotero-snap/common/Zotero/storage/YKW236ZZ/5597134.html:text/html;IEEE Xplore Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/MVC2DAFJ/Ellis and Ferryman - 2010 - PETS2010 and PETS2009 Evaluation of Results Using .pdf:application/pdf},
}

@article{dollar_fast_2014,
	title = {Fast {Feature} {Pyramids} for {Object} {Detection}},
	volume = {36},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2014.2300479},
	abstract = {Multi-resolution image features may be approximated via extrapolation from nearby scales, rather than being computed explicitly. This fundamental insight allows us to design object detection algorithms that are as accurate, and considerably faster, than the state-of-the-art. The computational bottleneck of many modern detectors is the computation of features at every scale of a finely-sampled image pyramid. Our key insight is that one may compute finely sampled feature pyramids at a fraction of the cost, without sacrificing performance: for a broad family of features we find that features computed at octave-spaced scale intervals are sufficient to approximate features on a finely-sampled pyramid. Extrapolation is inexpensive as compared to direct feature computation. As a result, our approximation yields considerable speedups with negligible loss in detection accuracy. We modify three diverse visual recognition systems to use fast feature pyramids and show results on both pedestrian detection (measured on the Caltech, INRIA, TUD-Brussels and ETH data sets) and general object detection (measured on the PASCAL VOC). The approach is general and is widely applicable to vision algorithms requiring fine-grained multi-scale analysis. Our approximation is valid for images with broad spectra (most natural images) and fails for images with narrow band-pass spectra (e.g., periodic textures).},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Dollár, Piotr and Appel, Ron and Belongie, Serge and Perona, Pietro},
	month = aug,
	year = {2014},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Feature extraction, Histograms, object detection, Object detection, Detectors, Accuracy, Approximation methods, image pyramids, natural image statistics, pedestrian detection, real-time systems, Visual features, Visualization},
	pages = {1532--1545},
	file = {IEEE Xplore Abstract Record:/home/tobias/snap/zotero-snap/common/Zotero/storage/KZVGIDZX/6714453.html:text/html;IEEE Xplore Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/8E5TPDE7/Dollár et al. - 2014 - Fast Feature Pyramids for Object Detection.pdf:application/pdf},
}

@article{wen_ua-detrac_2020,
	title = {{UA}-{DETRAC}: {A} new benchmark and protocol for multi-object detection and tracking},
	volume = {193},
	issn = {1077-3142},
	shorttitle = {{UA}-{DETRAC}},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314220300035},
	doi = {10.1016/j.cviu.2020.102907},
	abstract = {Effective multi-object tracking (MOT) methods have been developed in recent years for a wide range of applications including visual surveillance and behavior understanding. Existing performance evaluations of MOT methods usually separate the tracking step from the detection step by using one single predefined setting of object detection for comparisons. In this work, we propose a new University at Albany DEtection and TRACking (UA-DETRAC) dataset for comprehensive performance evaluation of MOT systems especially on detectors. The UA-DETRAC benchmark dataset consists of 100 challenging videos captured from real-world traffic scenes (over 140,000 frames with rich annotations, including illumination, vehicle type, occlusion, truncation ratio, and vehicle bounding boxes) for multi-object detection and tracking. We evaluate complete MOT systems constructed from combinations of state-of-the-art object detection and tracking methods. Our analysis shows the complex effects of detection accuracy on MOT system performance. Based on these observations, we propose effective and informative evaluation metrics for MOT systems that consider the effect of object detection for comprehensive performance analysis.},
	language = {en},
	urldate = {2023-05-01},
	journal = {Computer Vision and Image Understanding},
	author = {Wen, Longyin and Du, Dawei and Cai, Zhaowei and Lei, Zhen and Chang, Ming-Ching and Qi, Honggang and Lim, Jongwoo and Yang, Ming-Hsuan and Lyu, Siwei},
	month = apr,
	year = {2020},
	keywords = {Object detection, Benchmark, Evaluation protocol, Object tracking},
	pages = {102907},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/8X8UIU4H/Wen et al. - 2020 - UA-DETRAC A new benchmark and protocol for multi-.pdf:application/pdf;ScienceDirect Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/M49RHZ67/S1077314220300035.html:text/html},
}

@article{geiger_vision_2013,
	title = {Vision meets robotics: {The} {KITTI} dataset},
	volume = {32},
	issn = {0278-3649},
	shorttitle = {Vision meets robotics},
	url = {https://doi.org/10.1177/0278364913491297},
	doi = {10.1177/0278364913491297},
	abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10?100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.},
	number = {11},
	urldate = {2023-05-01},
	journal = {The International Journal of Robotics Research},
	author = {Geiger, A and Lenz, P and Stiller, C and Urtasun, R},
	month = sep,
	year = {2013},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {1231--1237},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/JC7KK4SB/Geiger et al. - 2013 - Vision meets robotics The KITTI dataset.pdf:application/pdf},
}

@inproceedings{wu_tracking_2006,
	title = {Tracking of {Multiple}, {Partially} {Occluded} {Humans} based on {Static} {Body} {Part} {Detection}},
	volume = {1},
	doi = {10.1109/CVPR.2006.312},
	abstract = {Tracking of humans in videos is important for many applications. A major source of difficulty in performing this task is due to inter-human or scene occlusion. We present an approach based on representing humans as an assembly of four body parts and detection of the body parts in single frames which makes the method insensitive to camera motions. The responses of the body part detectors and a combined human detector provide the "observations" used for tracking. Trajectory initialization and termination are both fully automatic and rely on the confidences computed from the detection responses. An object is tracked by data association if its corresponding detection response can be found; otherwise it is tracked by a meanshift style tracker. Our method can track humans with both inter-object and scene occlusions. The system is evaluated on three sets of videos and compared with previous method.},
	booktitle = {2006 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'06)},
	author = {Wu, Bo and Nevatia, R.},
	month = jun,
	year = {2006},
	note = {ISSN: 1063-6919},
	keywords = {Object detection, Cameras, Detectors, Assembly, Humans, Intelligent robots, Intelligent systems, Layout, Motion detection, Videos},
	pages = {951--958},
	file = {IEEE Xplore Abstract Record:/home/tobias/snap/zotero-snap/common/Zotero/storage/BXYXIHV7/1640854.html:text/html;IEEE Xplore Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/SF49IBCT/Wu and Nevatia - 2006 - Tracking of Multiple, Partially Occluded Humans ba.pdf:application/pdf},
}

@inproceedings{ristani_performance_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Performance {Measures} and a {Data} {Set} for {Multi}-target, {Multi}-camera {Tracking}},
	isbn = {978-3-319-48881-3},
	doi = {10.1007/978-3-319-48881-3_2},
	abstract = {To help accelerate progress in multi-target, multi-camera tracking systems, we present (i) a new pair of precision-recall measures of performance that treats errors of all types uniformly and emphasizes correct identification over sources of error; (ii) the largest fully-annotated and calibrated data set to date with more than 2 million frames of 1080 p, 60 fps video taken by 8 cameras observing more than 2,700 identities over 85 min; and (iii) a reference software system as a comparison baseline. We show that (i) our measures properly account for bottom-line identity match performance in the multi-camera setting; (ii) our data set poses realistic challenges to current trackers; and (iii) the performance of our system is comparable to the state of the art.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Ristani, Ergys and Solera, Francesco and Zou, Roger and Cucchiara, Rita and Tomasi, Carlo},
	editor = {Hua, Gang and Jégou, Hervé},
	year = {2016},
	keywords = {Identity management, Large scale data set, Multi camera data set, Multi camera tracking, Performance evaluation},
	pages = {17--35},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/5EF97JCE/Ristani et al. - 2016 - Performance Measures and a Data Set for Multi-targ.pdf:application/pdf},
}

@article{dendorfer_motchallenge_2021,
	title = {{MOTChallenge}: {A} {Benchmark} for {Single}-{Camera} {Multiple} {Target} {Tracking}},
	volume = {129},
	issn = {1573-1405},
	shorttitle = {{MOTChallenge}},
	url = {https://doi.org/10.1007/s11263-020-01393-0},
	doi = {10.1007/s11263-020-01393-0},
	abstract = {Standardized benchmarks have been crucial in pushing the performance of computer vision algorithms, especially since the advent of deep learning. Although leaderboards should not be over-claimed, they often provide the most objective measure of performance and are therefore important guides for research. We present MOTChallenge, a benchmark for single-camera Multiple Object Tracking (MOT) launched in late 2014, to collect existing and new data and create a framework for the standardized evaluation of multiple object tracking methods. The benchmark is focused on multiple people tracking, since pedestrians are by far the most studied object in the tracking community, with applications ranging from robot navigation to self-driving cars. This paper collects the first three releases of the benchmark: (i) MOT15, along with numerous state-of-the-art results that were submitted in the last years, (ii) MOT16, which contains new challenging videos, and (iii) MOT17, that extends MOT16 sequences with more precise labels and evaluates tracking performance on three different object detectors. The second and third release not only offers a significant increase in the number of labeled boxes, but also provide labels for multiple object classes beside pedestrians, as well as the level of visibility for every single object of interest. We finally provide a categorization of state-of-the-art trackers and a broad error analysis. This will help newcomers understand the related work and research trends in the MOT community, and hopefully shed some light into potential future research directions.},
	language = {en},
	number = {4},
	urldate = {2023-04-25},
	journal = {International Journal of Computer Vision},
	author = {Dendorfer, Patrick and Os̆ep, Aljos̆a and Milan, Anton and Schindler, Konrad and Cremers, Daniel and Reid, Ian and Roth, Stefan and Leal-Taixé, Laura},
	month = apr,
	year = {2021},
	keywords = {Computer vision, Evaluation, MOTA, MOTChallenge, Multi-object-tracking},
	pages = {845--881},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/BE7DGAHZ/Dendorfer et al. - 2021 - MOTChallenge A Benchmark for Single-Camera Multip.pdf:application/pdf},
}

@article{you_real-time_2020,
	title = {Real-time 3d deep multi-camera tracking},
	journal = {arXiv preprint arXiv:2003.11753},
	author = {You, Quanzeng and Jiang, Hao},
	year = {2020},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/5YJ524WU/You and Jiang - 2020 - Real-time 3d deep multi-camera tracking.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/D6MZR2WI/2003.html:text/html},
}

@article{ong_bayesian_2020,
	title = {A {Bayesian} filter for multi-view {3D} multi-object tracking with occlusion handling},
	volume = {44},
	number = {5},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ong, Jonah and Vo, Ba-Tuong and Vo, Ba-Ngu and Kim, Du Yong and Nordholm, Sven},
	year = {2020},
	note = {Publisher: IEEE},
	pages = {2246--2263},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/BUM7VLP3/9242263.html:text/html},
}

@article{fleuret_multicamera_2007,
	title = {Multicamera people tracking with a probabilistic occupancy map},
	volume = {30},
	number = {2},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Fleuret, Francois and Berclaz, Jerome and Lengagne, Richard and Fua, Pascal},
	year = {2007},
	note = {Publisher: IEEE},
	pages = {267--282},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/88SLX4KQ/xplore-shut-page.html:text/html},
}

@inproceedings{khan_multiview_2006,
	title = {A multiview approach to tracking people in crowded scenes using a planar homography constraint},
	booktitle = {Computer {Vision}–{ECCV} 2006: 9th {European} {Conference} on {Computer} {Vision}, {Graz}, {Austria}, {May} 7-13, 2006, {Proceedings}, {Part} {IV} 9},
	publisher = {Springer},
	author = {Khan, Saad M. and Shah, Mubarak},
	year = {2006},
	pages = {133--146},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/AXQDYPHC/Khan and Shah - 2006 - A multiview approach to tracking people in crowded.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/PMXKF5VS/11744085_11.html:text/html},
}

@inproceedings{mittal_m2tracker_2002,
	title = {M2tracker: {A} multi-view approach to segmenting and tracking people in a cluttered scene using region-based stereo},
	shorttitle = {M2tracker},
	booktitle = {{ECCV} (1)},
	author = {Mittal, Anurag and Davis, Larry S.},
	year = {2002},
	pages = {18--36},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/JIJDSIN4/Mittal and Davis - 2002 - M2tracker A multi-view approach to segmenting and.pdf:application/pdf},
}

@inproceedings{khan_multiview_2006-1,
	title = {A multiview approach to tracking people in crowded scenes using a planar homography constraint},
	booktitle = {Computer {Vision}–{ECCV} 2006: 9th {European} {Conference} on {Computer} {Vision}, {Graz}, {Austria}, {May} 7-13, 2006, {Proceedings}, {Part} {IV} 9},
	publisher = {Springer},
	author = {Khan, Saad M. and Shah, Mubarak},
	year = {2006},
	pages = {133--146},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/964KMZ65/Khan and Shah - 2006 - A multiview approach to tracking people in crowded.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/CEHK2RX3/11744085_11.html:text/html},
}

@inproceedings{khan_multiview_2006-2,
	title = {A multiview approach to tracking people in crowded scenes using a planar homography constraint},
	booktitle = {Computer {Vision}–{ECCV} 2006: 9th {European} {Conference} on {Computer} {Vision}, {Graz}, {Austria}, {May} 7-13, 2006, {Proceedings}, {Part} {IV} 9},
	publisher = {Springer},
	author = {Khan, Saad M. and Shah, Mubarak},
	year = {2006},
	pages = {133--146},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/9Q9HZY5L/Khan and Shah - 2006 - A multiview approach to tracking people in crowded.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/QEG9KWEB/11744085_11.html:text/html},
}

@inproceedings{focken_towards_2002,
	title = {Towards vision-based 3-d people tracking in a smart room},
	booktitle = {Proceedings. fourth ieee international conference on multimodal interfaces},
	publisher = {IEEE},
	author = {Focken, Dirk and Stiefelhagen, Rainer},
	year = {2002},
	pages = {400--405},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/4YEJ9DWY/Focken and Stiefelhagen - 2002 - Towards vision-based 3-d people tracking in a smar.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/MHF8WXQ4/1167028.html:text/html},
}

@inproceedings{chavdarova_cijo_2018,
	title = {Cijo {Jose}, {Timur} {Bagautdinov}, {Louis} {Lettry}, {Pascal} {Fua}, {Luc} {Van} {Gool}, and {François} {Fleuret}. {Wildtrack}: {A} multi-camera hd dataset for dense unscripted pedestrian detection},
	shorttitle = {Cijo {Jose}, {Timur} {Bagautdinov}, {Louis} {Lettry}, {Pascal} {Fua}, {Luc} {Van} {Gool}, and {François} {Fleuret}. {Wildtrack}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Chavdarova, Tatjana and Baqué, Pierre and Bouquet, Stéphane and Maksai, Andrii},
	year = {2018},
	pages = {5030--5039},
}

@inproceedings{quach_dyglip_2021,
	title = {Dyglip: {A} dynamic graph model with link prediction for accurate multi-camera multiple object tracking},
	shorttitle = {Dyglip},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Quach, Kha Gia and Nguyen, Pha and Le, Huu and Truong, Thanh-Dat and Duong, Chi Nhan and Tran, Minh-Triet and Luu, Khoa},
	year = {2021},
	pages = {13784--13793},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/9SCQ2J4T/Quach et al. - 2021 - Dyglip A dynamic graph model with link prediction.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/T8HYZSKR/Quach_DyGLIP_A_Dynamic_Graph_Model_With_Link_Prediction_for_Accurate_CVPR_2021_paper.html:text/html},
}

@article{he_multi-target_2020,
	title = {Multi-target multi-camera tracking by tracklet-to-target assignment},
	volume = {29},
	journal = {IEEE Transactions on Image Processing},
	author = {He, Yuhang and Wei, Xing and Hong, Xiaopeng and Shi, Weiwei and Gong, Yihong},
	year = {2020},
	note = {Publisher: IEEE},
	pages = {5191--5205},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/IMCS6Q5B/9042858.html:text/html},
}

@article{boykov_fast_2001,
	title = {Fast approximate energy minimization via graph cuts},
	volume = {23},
	number = {11},
	journal = {IEEE Transactions on pattern analysis and machine intelligence},
	author = {Boykov, Yuri and Veksler, Olga and Zabih, Ramin},
	year = {2001},
	note = {Publisher: IEEE},
	pages = {1222--1239},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/ABQEGGVA/969114.html:text/html},
}

@article{lan_semi-online_2020,
	title = {Semi-online multi-people tracking by re-identification},
	volume = {128},
	number = {7},
	journal = {International Journal of Computer Vision},
	author = {Lan, Long and Wang, Xinchao and Hua, Gang and Huang, Thomas S. and Tao, Dacheng},
	year = {2020},
	note = {Publisher: Springer},
	pages = {1937--1955},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/3F56AH6Q/s11263-020-01314-1.html:text/html},
}

@article{wen_multi-camera_2017,
	title = {Multi-camera multi-target tracking with space-time-view hyper-graph},
	volume = {122},
	journal = {International Journal of Computer Vision},
	author = {Wen, Longyin and Lei, Zhen and Chang, Ming-Ching and Qi, Honggang and Lyu, Siwei},
	year = {2017},
	note = {Publisher: Springer},
	pages = {313--333},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/DZL2QRX9/Wen et al. - 2017 - Multi-camera multi-target tracking with space-time.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/DT3Z3NZT/s11263-016-0943-0.html:text/html},
}

@inproceedings{xu_cross-view_2017,
	title = {Cross-view people tracking by scene-centered spatio-temporal parsing},
	volume = {31},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Xu, Yuanlu and Liu, Xiaobai and Qin, Lei and Zhu, Song-Chun},
	year = {2017},
	note = {Issue: 1},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/42ZXRMUK/Xu et al. - 2017 - Cross-view people tracking by scene-centered spati.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/5GSA9QAA/11190.html:text/html},
}

@inproceedings{xu_multi-view_2016,
	title = {Multi-view people tracking via hierarchical trajectory composition},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Xu, Yuanlu and Liu, Xiaobai and Liu, Yang and Zhu, Song-Chun},
	year = {2016},
	pages = {4256--4265},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/7Y8I6SPZ/Xu et al. - 2016 - Multi-view people tracking via hierarchical trajec.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/NCC782TI/Xu_Multi-View_People_Tracking_CVPR_2016_paper.html:text/html},
}

@inproceedings{parmar_image_2018,
	title = {Image transformer},
	booktitle = {International conference on machine learning},
	publisher = {PMLR},
	author = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
	year = {2018},
	pages = {4055--4064},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/3NYFLM2I/Parmar et al. - 2018 - Image transformer.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/U6K25QMV/parmar18a.html:text/html},
}

@inproceedings{wu_track_2021,
	title = {Track to detect and segment: {An} online multi-object tracker},
	shorttitle = {Track to detect and segment},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Wu, Jialian and Cao, Jiale and Song, Liangchen and Wang, Yu and Yang, Ming and Yuan, Junsong},
	year = {2021},
	pages = {12352--12361},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/GBS5SJDK/Wu et al. - 2021 - Track to detect and segment An online multi-objec.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/N5CRZJYJ/Wu_Track_To_Detect_and_Segment_An_Online_Multi-Object_Tracker_CVPR_2021_paper.html:text/html},
}

@inproceedings{xu_segment_2020,
	title = {Segment as points for efficient online multi-object tracking and segmentation},
	booktitle = {Computer {Vision}–{ECCV} 2020: 16th {European} {Conference}, {Glasgow}, {UK}, {August} 23–28, 2020, {Proceedings}, {Part} {I} 16},
	publisher = {Springer},
	author = {Xu, Zhenbo and Zhang, Wei and Tan, Xiao and Yang, Wei and Huang, Huan and Wen, Shilei and Ding, Errui and Huang, Liusheng},
	year = {2020},
	pages = {264--281},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/R9JV53GB/Xu et al. - 2020 - Segment as points for efficient online multi-objec.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/FZRXTN6V/978-3-030-58452-8_16.html:text/html},
}

@inproceedings{voigtlaender_mots_2019,
	title = {Mots: {Multi}-object tracking and segmentation},
	shorttitle = {Mots},
	booktitle = {Proceedings of the ieee/cvf conference on computer vision and pattern recognition},
	author = {Voigtlaender, Paul and Krause, Michael and Osep, Aljosa and Luiten, Jonathon and Sekar, Berin Balachandar Gnana and Geiger, Andreas and Leibe, Bastian},
	year = {2019},
	pages = {7942--7951},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/N5R2UAZF/Voigtlaender et al. - 2019 - Mots Multi-object tracking and segmentation.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/P5RS79AC/Voigtlaender_MOTS_Multi-Object_Tracking_and_Segmentation_CVPR_2019_paper.html:text/html},
}

@inproceedings{osep_track_2018,
	title = {Track, then decide: {Category}-agnostic vision-based multi-object tracking},
	shorttitle = {Track, then decide},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Ošep, Aljoša and Mehner, Wolfgang and Voigtlaender, Paul and Leibe, Bastian},
	year = {2018},
	pages = {3494--3501},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/EKX8MRCQ/8460975.html:text/html},
}

@inproceedings{liu_gsm_2020,
	title = {{GSM}: {Graph} {Similarity} {Model} for {Multi}-{Object} {Tracking}.},
	shorttitle = {{GSM}},
	booktitle = {{IJCAI}},
	author = {Liu, Qiankun and Chu, Qi and Liu, Bin and Yu, Nenghai},
	year = {2020},
	pages = {530--536},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/Z6PMPXM4/Liu et al. - 2020 - GSM Graph Similarity Model for Multi-Object Track.pdf:application/pdf},
}

@inproceedings{zhou_tracking_2020,
	title = {Tracking objects as points},
	booktitle = {Computer {Vision}–{ECCV} 2020: 16th {European} {Conference}, {Glasgow}, {UK}, {August} 23–28, 2020, {Proceedings}, {Part} {IV}},
	publisher = {Springer},
	author = {Zhou, Xingyi and Koltun, Vladlen and Krähenbühl, Philipp},
	year = {2020},
	pages = {474--490},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/PSWMIU8B/Zhou et al. - 2020 - Tracking objects as points.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/2K9S263V/978-3-030-58548-8_28.html:text/html},
}

@inproceedings{feichtenhofer_detect_2017,
	title = {Detect to track and track to detect},
	booktitle = {Proceedings of the {IEEE} international conference on computer vision},
	author = {Feichtenhofer, Christoph and Pinz, Axel and Zisserman, Andrew},
	year = {2017},
	pages = {3038--3046},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/AKYH4L7D/Feichtenhofer et al. - 2017 - Detect to track and track to detect.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/926TKTBU/Feichtenhofer_Detect_to_Track_ICCV_2017_paper.html:text/html},
}

@inproceedings{tokmakov_learning_2021,
	title = {Learning to track with object permanence},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Tokmakov, Pavel and Li, Jie and Burgard, Wolfram and Gaidon, Adrien},
	year = {2021},
	pages = {10860--10869},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/82VV9AFE/Tokmakov et al. - 2021 - Learning to track with object permanence.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/RK8F5CWD/Tokmakov_Learning_To_Track_With_Object_Permanence_ICCV_2021_paper.html:text/html},
}

@inproceedings{porzi_learning_2020,
	title = {Learning {Multi}-{Object} {Tracking} and {Segmentation} {From} {Automatic} {Annotations}},
	isbn = {978-1-72817-168-5},
	url = {https://www.computer.org/csdl/proceedings-article/cvpr/2020/716800g845/1m3nc7uIquQ},
	doi = {10.1109/CVPR42600.2020.00688},
	abstract = {In this work we contribute a novel pipeline to automatically generate training data, and to improve over state-of-the-art multi-object tracking and segmentation (MOTS) methods. Our proposed track mining algorithm turns raw street-level videos into high-fidelity MOTS training data, is scalable and overcomes the need of expensive and time-consuming manual annotation approaches. We leverage state-of-the-art instance segmentation results in combination with optical flow predictions, also trained on automatically harvested training data. Our second major contribution is MOTSNet - a deep learning, tracking-by-detection architecture for MOTS - deploying a novel mask-pooling layer for improved object association over time. Training MOTSNet with our automatically extracted data leads to significantly improved sMOTSA scores on the novel KITTI MOTS dataset (+1.9\%/+7.5\% on cars/pedestrians), and MOTSNet improves by +4.1\% over previously best methods on the MOTSChallenge dataset. Our most impressive finding is that we can improve over previous best-performing works, even in complete absence of manually annotated MOTS training data.},
	language = {English},
	urldate = {2023-04-24},
	publisher = {IEEE Computer Society},
	author = {Porzi, Lorenzo and Hofinger, Markus and Ruiz, Idoia and Serrat, Joan and Bulò, Samuel Rota and Kontschieder, Peter},
	month = jun,
	year = {2020},
	pages = {6845--6854},
	file = {Submitted Version:/home/tobias/snap/zotero-snap/common/Zotero/storage/4N79EI8U/Porzi et al. - 2020 - Learning Multi-Object Tracking and Segmentation Fr.pdf:application/pdf},
}

@misc{he_mask_2018,
	title = {Mask {R}-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	doi = {10.48550/arXiv.1703.06870},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
	urldate = {2023-04-24},
	publisher = {arXiv},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	month = jan,
	year = {2018},
	note = {arXiv:1703.06870 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: open source; appendix on more results},
	file = {arXiv Fulltext PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/Z67IEAC8/He et al. - 2018 - Mask R-CNN.pdf:application/pdf;arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/ZHS3EQRU/1703.html:text/html},
}

@inproceedings{leal-taixe_learning_2014,
	title = {Learning an image-based motion context for multiple people tracking},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Leal-Taixé, Laura and Fenzi, Michele and Kuznetsova, Alina and Rosenhahn, Bodo and Savarese, Silvio},
	year = {2014},
	pages = {3542--3549},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/B4SUJTFB/Leal-Taixé et al. - 2014 - Learning an image-based motion context for multipl.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/9AHAEL2U/Leal-Taixe_Learning_an_Image-based_2014_CVPR_paper.html:text/html},
}

@inproceedings{yamaguchi_who_2011-1,
	title = {Who are you with and where are you going?},
	booktitle = {{CVPR} 2011},
	publisher = {IEEE},
	author = {Yamaguchi, Kota and Berg, Alexander C. and Ortiz, Luis E. and Berg, Tamara L.},
	year = {2011},
	pages = {1345--1352},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/UZGL6U2S/5995468.html:text/html},
}

@inproceedings{scovanner_learning_2009,
	title = {Learning pedestrian dynamics from the real world},
	booktitle = {2009 {IEEE} 12th {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Scovanner, Paul and Tappen, Marshall F.},
	year = {2009},
	pages = {381--388},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/KENN7BPY/5459224.html:text/html},
}

@inproceedings{pellegrini_youll_2009,
	title = {You'll never walk alone: {Modeling} social behavior for multi-target tracking},
	shorttitle = {You'll never walk alone},
	booktitle = {2009 {IEEE} 12th international conference on computer vision},
	publisher = {IEEE},
	author = {Pellegrini, Stefano and Ess, Andreas and Schindler, Konrad and Van Gool, Luc},
	year = {2009},
	pages = {261--268},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/4SZY7XBN/5459260.html:text/html},
}

@inproceedings{choi_multiple_2010,
	title = {Multiple target tracking in world coordinate with single, minimally calibrated camera},
	booktitle = {Computer {Vision}–{ECCV} 2010: 11th {European} {Conference} on {Computer} {Vision}, {Heraklion}, {Crete}, {Greece}, {September} 5-11, 2010, {Proceedings}, {Part} {IV} 11},
	publisher = {Springer},
	author = {Choi, Wongun and Savarese, Silvio},
	year = {2010},
	pages = {553--567},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/PVZUU4W4/Choi and Savarese - 2010 - Multiple target tracking in world coordinate with .pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/25ZCTV5D/978-3-642-15561-1_40.html:text/html},
}

@inproceedings{andriyenko_multi-target_2011,
	title = {Multi-target tracking by continuous energy minimization},
	booktitle = {{CVPR} 2011},
	publisher = {IEEE},
	author = {Andriyenko, Anton and Schindler, Konrad},
	year = {2011},
	pages = {1265--1272},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/5XDGL4JW/5995311.html:text/html},
}

@inproceedings{robicquet_learning_2016,
	title = {Learning social etiquette: {Human} trajectory understanding in crowded scenes},
	shorttitle = {Learning social etiquette},
	booktitle = {Computer {Vision}–{ECCV} 2016: 14th {European} {Conference}, {Amsterdam}, {The} {Netherlands}, {October} 11-14, 2016, {Proceedings}, {Part} {VIII} 14},
	publisher = {Springer},
	author = {Robicquet, Alexandre and Sadeghian, Amir and Alahi, Alexandre and Savarese, Silvio},
	year = {2016},
	pages = {549--565},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/M4S7SMEL/Robicquet et al. - 2016 - Learning social etiquette Human trajectory unders.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/B3IZV8ZE/978-3-319-46484-8_33.html:text/html},
}

@inproceedings{leal-taixe_everybody_2011,
	title = {Everybody needs somebody: {Modeling} social and grouping behavior on a linear programming multiple people tracker},
	shorttitle = {Everybody needs somebody},
	booktitle = {2011 {IEEE} international conference on computer vision workshops ({ICCV} workshops)},
	publisher = {IEEE},
	author = {Leal-Taixé, Laura and Pons-Moll, Gerard and Rosenhahn, Bodo},
	year = {2011},
	pages = {120--127},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/AVCNNFR3/6130233.html:text/html},
}

@inproceedings{alahi_social_2016,
	title = {Social lstm: {Human} trajectory prediction in crowded spaces},
	shorttitle = {Social lstm},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Alahi, Alexandre and Goel, Kratarth and Ramanathan, Vignesh and Robicquet, Alexandre and Fei-Fei, Li and Savarese, Silvio},
	year = {2016},
	pages = {961--971},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/8BHLBSS4/Alahi et al. - 2016 - Social lstm Human trajectory prediction in crowde.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/WP96HJ2M/Alahi_Social_LSTM_Human_CVPR_2016_paper.html:text/html},
}

@inproceedings{chu_famnet_2019,
	title = {Famnet: {Joint} learning of feature, affinity and multi-dimensional assignment for online multiple object tracking},
	shorttitle = {Famnet},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Chu, Peng and Ling, Haibin},
	year = {2019},
	pages = {6172--6181},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/576AQDF9/Chu and Ling - 2019 - Famnet Joint learning of feature, affinity and mu.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/FPK6LJM3/Chu_FAMNet_Joint_Learning_of_Feature_Affinity_and_Multi-Dimensional_Assignment_for_ICCV_2019_pa.html:text/html},
}

@inproceedings{chen_real-time_2018,
	title = {Real-time multiple people tracking with deeply learned candidate selection and person re-identification},
	booktitle = {2018 {IEEE} international conference on multimedia and expo ({ICME})},
	publisher = {IEEE},
	author = {Chen, Long and Ai, Haizhou and Zhuang, Zijie and Shang, Chong},
	year = {2018},
	pages = {1--6},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/86K6CRT7/8486597.html:text/html},
}

@inproceedings{ristani_features_2018,
	title = {Features for multi-target multi-camera tracking and re-identification},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Ristani, Ergys and Tomasi, Carlo},
	year = {2018},
	pages = {6036--6046},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/QJDXW5K7/Ristani and Tomasi - 2018 - Features for multi-target multi-camera tracking an.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/3AZAXWIK/Ristani_Features_for_Multi-Target_CVPR_2018_paper.html:text/html},
}

@inproceedings{pang_quasi-dense_2021,
	title = {Quasi-dense similarity learning for multiple object tracking},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Pang, Jiangmiao and Qiu, Linlu and Li, Xia and Chen, Haofeng and Li, Qi and Darrell, Trevor and Yu, Fisher},
	year = {2021},
	pages = {164--173},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/LEXRE2JX/Pang et al. - 2021 - Quasi-dense similarity learning for multiple objec.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/V5XPZFV8/Pang_Quasi-Dense_Similarity_Learning_for_Multiple_Object_Tracking_CVPR_2021_paper.html:text/html},
}

@inproceedings{leal-taixe_learning_2016,
	title = {Learning by tracking: {Siamese} {CNN} for robust target association},
	shorttitle = {Learning by tracking},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition workshops},
	author = {Leal-Taixé, Laura and Canton-Ferrer, Cristian and Schindler, Konrad},
	year = {2016},
	pages = {33--40},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/V92XK53Y/Leal-Taixé et al. - 2016 - Learning by tracking Siamese CNN for robust targe.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/XBG9R5US/Leal-Taixe_Learning_by_Tracking_CVPR_2016_paper.html:text/html},
}

@inproceedings{wang_joint_2021,
	title = {Joint object detection and multi-object tracking with graph neural networks},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Wang, Yongxin and Kitani, Kris and Weng, Xinshuo},
	year = {2021},
	pages = {13708--13715},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/8P3M24PF/9561110.html:text/html},
}

@inproceedings{braso_learning_2020,
	title = {Learning a neural solver for multiple object tracking},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Brasó, Guillem and Leal-Taixé, Laura},
	year = {2020},
	pages = {6247--6257},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/GEYU3Y85/Brasó and Leal-Taixé - 2020 - Learning a neural solver for multiple object track.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/MSDGU4LI/Braso_Learning_a_Neural_Solver_for_Multiple_Object_Tracking_CVPR_2020_paper.html:text/html},
}

@inproceedings{hornakova_lifted_2020,
	title = {Lifted disjoint paths with application in multiple object tracking},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hornakova, Andrea and Henschel, Roberto and Rosenhahn, Bodo and Swoboda, Paul},
	year = {2020},
	pages = {4364--4375},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/99W38LMI/Hornakova et al. - 2020 - Lifted disjoint paths with application in multiple.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/SHCNI96P/hornakova20a.html:text/html},
}

@article{henschel_improvements_2017,
	title = {Improvements to frank-wolfe optimization for multi-detector multi-object tracking},
	volume = {8},
	journal = {arXiv preprint arXiv:1705.08314},
	author = {Henschel, Roberto and Leal-Taixé, Laura and Cremers, Daniel and Rosenhahn, Bodo},
	year = {2017},
}

@inproceedings{tang_multiple_2017,
	title = {Multiple people tracking by lifted multicut and person re-identification},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Tang, Siyu and Andriluka, Mykhaylo and Andres, Bjoern and Schiele, Bernt},
	year = {2017},
	pages = {3539--3548},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/RHJL7FIQ/Tang et al. - 2017 - Multiple people tracking by lifted multicut and pe.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/HMVGVAIC/Tang_Multiple_People_Tracking_CVPR_2017_paper.html:text/html},
}

@inproceedings{yu_multiple_2007,
	title = {Multiple target tracking using spatio-temporal markov chain monte carlo data association},
	booktitle = {2007 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Yu, Qian and Medioni, Gérard and Cohen, Isaac},
	year = {2007},
	pages = {1--8},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/KU7JDYZC/4270016.html:text/html},
}

@article{keuper_motion_2018,
	title = {Motion segmentation \& multiple object tracking by correlation co-clustering},
	volume = {42},
	number = {1},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Keuper, Margret and Tang, Siyu and Andres, Bjoern and Brox, Thomas and Schiele, Bernt},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {140--153},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/5ILUVACM/8493320.html:text/html},
}

@inproceedings{kim_multiple_2015,
	title = {Multiple hypothesis tracking revisited},
	booktitle = {Proceedings of the {IEEE} international conference on computer vision},
	author = {Kim, Chanho and Li, Fuxin and Ciptadi, Arridhana and Rehg, James M.},
	year = {2015},
	pages = {4696--4704},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/ZQKCEU2T/Kim et al. - 2015 - Multiple hypothesis tracking revisited.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/NSCGEYEK/Kim_Multiple_Hypothesis_Tracking_ICCV_2015_paper.html:text/html},
}

@article{sheng_heterogeneous_2018,
	title = {Heterogeneous association graph fusion for target association in multiple object tracking},
	volume = {29},
	number = {11},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Sheng, Hao and Zhang, Yang and Chen, Jiahui and Xiong, Zhang and Zhang, Jun},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {3269--3280},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/S987H8K8/8540450.html:text/html},
}

@inproceedings{zhang_global_2008,
	title = {Global data association for multi-object tracking using network flows},
	booktitle = {2008 {IEEE} conference on computer vision and pattern recognition},
	publisher = {IEEE},
	author = {Zhang, Li and Li, Yuan and Nevatia, Ramakant},
	year = {2008},
	pages = {1--8},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/PE7TMAUL/4587584.html:text/html},
}

@inproceedings{pirsiavash_globally-optimal_2011,
	title = {Globally-optimal greedy algorithms for tracking a variable number of objects},
	booktitle = {{CVPR} 2011},
	publisher = {IEEE},
	author = {Pirsiavash, Hamed and Ramanan, Deva and Fowlkes, Charless C.},
	year = {2011},
	pages = {1201--1208},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/SK2PVHHE/5995604.html:text/html},
}

@inproceedings{jiang_linear_2007,
	title = {A linear programming approach for multiple object tracking},
	booktitle = {2007 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Jiang, Hao and Fels, Sidney and Little, James J.},
	year = {2007},
	pages = {1--8},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/YZ6JVG3M/4270205.html:text/html},
}

@article{berclaz_multiple_2011,
	title = {Multiple object tracking using k-shortest paths optimization},
	volume = {33},
	number = {9},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Berclaz, Jerome and Fleuret, Francois and Turetken, Engin and Fua, Pascal},
	year = {2011},
	note = {Publisher: IEEE},
	pages = {1806--1819},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/N7RAS49J/5708151.html:text/html},
}

@inproceedings{bergmann_tracking_2019,
	title = {Tracking without bells and whistles},
	url = {http://arxiv.org/abs/1903.05625},
	doi = {10.1109/ICCV.2019.00103},
	abstract = {The problem of tracking multiple objects in a video sequence poses several challenging tasks. For tracking-by-detection, these include object re-identification, motion prediction and dealing with occlusions. We present a tracker (without bells and whistles) that accomplishes tracking without specifically targeting any of these tasks, in particular, we perform no training or optimization on tracking data. To this end, we exploit the bounding box regression of an object detector to predict the position of an object in the next frame, thereby converting a detector into a Tracktor. We demonstrate the potential of Tracktor and provide a new state-of-the-art on three multi-object tracking benchmarks by extending it with a straightforward re-identification and camera motion compensation. We then perform an analysis on the performance and failure cases of several state-of-the-art tracking methods in comparison to our Tracktor. Surprisingly, none of the dedicated tracking methods are considerably better in dealing with complex tracking scenarios, namely, small and occluded objects or missing detections. However, our approach tackles most of the easy tracking scenarios. Therefore, we motivate our approach as a new tracking paradigm and point out promising future research directions. Overall, Tracktor yields superior tracking performance than any current tracking method and our analysis exposes remaining and unsolved tracking challenges to inspire future research directions.},
	urldate = {2023-04-12},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Bergmann, Philipp and Meinhardt, Tim and Leal-Taixe, Laura},
	month = oct,
	year = {2019},
	note = {arXiv:1903.05625 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {941--951},
	file = {arXiv Fulltext PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/XWDUJMWE/Bergmann et al. - 2019 - Tracking without bells and whistles.pdf:application/pdf;arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/J5Q73V42/1903.html:text/html},
}

@misc{kuis_ai_laura_2021,
	title = {Laura {Leal}-{Taixé}: {Shifting} {Paradigms} in {Multi}-{Object} {Tracking}},
	shorttitle = {Laura {Leal}-{Taixé}},
	url = {https://www.youtube.com/watch?v=kNUwvT4RfTo},
	abstract = {The talk given by Laura Leal-Taixé at KUIS AI Talks on Oct. 21 in 2021.

Title: Shifting paradigms in multi-object tracking

Abstract:
The challenging task of multi-object tracking (MOT) requires simultaneous reasoning about track initialization, identity, and spatiotemporal trajectories. This problem has been traditionally addressed with the tracking-dy-detection paradigm. In this talk, I will discuss more recent paradigms, most notably, tracking-by-regression, and the rise of a new paradigm: tracking-by-attention. In this new paradigm, we formulate MOT as a frame-to-frame set prediction problem and introduce TrackFormer, an end-to-end MOT approach based on an encoder-decoder Transformer architecture. Our model achieves data association between frames via attention by evolving a set of track predictions through a video sequence. The Transformer decoder initializes new tracks from static object queries and autoregressively follows existing tracks in space and time with the new concept of identity preserving track queries. Both decoder query types benefit from self- and encoder-decoder attention to global frame-level features, thereby omitting any additional graph optimization and matching or modeling of motion and appearance.  At the end of the talk, I also want to discuss some of our work in collecting data for tracking with data privacy in mind.

Short Bio:
Laura Leal-Taixé is a tenure-track professor at the Technical University of Munich, leading the Dynamic Vision and Learning group.  She is a recipient of the Sofja Kovalevskaja Award from the Humboldt Foundation and the Google Faculty Award. She has been Area Chair for the major conferences in Computer Vision, was Program Chair of WACV last year, and will be General Chair of ECCV in 2024. Her research interests lie in dynamic scene understanding, visual localization, and object retrieval.},
	urldate = {2023-04-12},
	author = {{KUIS AI}},
	month = oct,
	year = {2021},
}

@article{amosa_multi-camera_nodate,
	title = {Multi-{Camera} {Multi}-{Object} {Tracking}: {A} {Review} of {Current} {Trends} and {Future} {Advances}},
	shorttitle = {Multi-{Camera} {Multi}-{Object} {Tracking}},
	journal = {Available at SSRN 4353604},
	author = {Amosa, Temitope Ibrahim and Sebastian, Patrick and Izhar, Lila Iznita and Ibrahim, Oladimeji and Bala, Abubakar and Ayinla, Lukman Shehu and Samaila, Yau Alhaji},
	file = {Amosa et al. - Multi-Camera Multi-Object Tracking A Review of Cu.pdf:/home/tobias/snap/zotero-snap/common/Zotero/storage/GW7ATTNU/Amosa et al. - Multi-Camera Multi-Object Tracking A Review of Cu.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/LH9CL2G6/papers.html:text/html},
}

@article{luo_multiple_2021,
	title = {Multiple object tracking: {A} literature review},
	volume = {293},
	shorttitle = {Multiple object tracking},
	journal = {Artificial intelligence},
	author = {Luo, Wenhan and Xing, Junliang and Milan, Anton and Zhang, Xiaoqin and Liu, Wei and Kim, Tae-Kyun},
	year = {2021},
	note = {Publisher: Elsevier},
	pages = {103448},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/38JGFLJ5/Luo et al. - 2021 - Multiple object tracking A literature review.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/4YIRN8H6/S0004370220301958.html:text/html},
}

@inproceedings{isaac-medina_multi-view_2022,
	title = {Multi-view {Vision} {Transformers} for {Object} {Detection}},
	doi = {10.1109/ICPR56361.2022.9956443},
	abstract = {Object detection has been thoroughly investigated during the last decade using deep neural networks. However, the inclusion of additional information given by multiple concurrent views of the same scene has not received much attention. In scenarios where objects may appear in obscure poses from certain view points, the use of differing simultaneous views can improve object detection. Therefore, we propose a multi-view fusion network to enrich the backbone features of standard object detection architectures across multiple source and target view points. Our method consists of a transformer decoder for the target view that combines the remaining source views feature maps. In this way, the feature representation of the target view can aggregate feature information from the source view through attention. Our architecture is detector-agnostic, meaning it can be applied across any existing detection backbone. We evaluate performance using YOLOX, Deformable DETR and Swin Transformer baseline detectors, comparing standard single view performance against the addition of our multi-view transformer architecture. Our method achieves a 3\% increase of the COCO AP over a four view X-ray security dataset and a slight 0.7\% increase on a seven view pedestrian dataset. We demonstrate that the integration of different views using attention-based networks improves the detection performance of multi-view datasets.1},
	booktitle = {2022 26th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Isaac-Medina, Brian K. S. and Willcocks, Chris G. and Breckon, Toby P.},
	month = aug,
	year = {2022},
	note = {ISSN: 2831-7475},
	keywords = {Feature extraction, Pattern recognition, Object detection, Detectors, Aggregates, Three-dimensional displays, Transformers},
	pages = {4678--4684},
	file = {IEEE Xplore Abstract Record:/home/tobias/snap/zotero-snap/common/Zotero/storage/XRJJUZ4E/9956443.html:text/html;IEEE Xplore Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/8LFWMI7D/Isaac-Medina et al. - 2022 - Multi-view Vision Transformers for Object Detectio.pdf:application/pdf},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
	urldate = {2023-03-21},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/MY2AIRDE/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@misc{noauthor_automatic_2023,
	title = {Automatic differentiation},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Automatic_differentiation&oldid=1143708979},
	abstract = {In mathematics and computer algebra, automatic differentiation (auto-differentiation, autodiff, or AD), also called algorithmic differentiation, computational differentiation, is a set of techniques to evaluate the partial derivative of a function specified by a computer program. 
Automatic differentiation exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, partial derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.},
	language = {en},
	urldate = {2023-03-21},
	journal = {Wikipedia},
	month = mar,
	year = {2023},
	note = {Page Version ID: 1143708979},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/2LK32JYP/Automatic_differentiation.html:text/html},
}

@misc{zhu_deformable_2021,
	title = {Deformable {DETR}: {Deformable} {Transformers} for {End}-to-{End} {Object} {Detection}},
	shorttitle = {Deformable {DETR}},
	url = {http://arxiv.org/abs/2010.04159},
	doi = {10.48550/arXiv.2010.04159},
	abstract = {DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
	month = mar,
	year = {2021},
	note = {arXiv:2010.04159 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICLR 2021 Oral},
	file = {arXiv Fulltext PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/PJBIK2AB/Zhu et al. - 2021 - Deformable DETR Deformable Transformers for End-t.pdf:application/pdf;arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/RIFZZAQG/2010.html:text/html},
}

@article{liu_deep_2018,
	title = {Deep learning for generic object detection},
	journal = {A Survey},
	author = {Liu, L. and Ouyang, W. and Wang, X. and Fieguth, P. and Chen, J. and Liu, XJAS},
	year = {2018},
}

@article{ren_faster_2015-1,
	title = {Faster r-cnn: {Towards} real-time object detection with region proposal networks},
	volume = {28},
	shorttitle = {Faster r-cnn},
	journal = {Advances in neural information processing systems},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	year = {2015},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/VZE4EWC4/Ren et al. - 2015 - Faster r-cnn Towards real-time object detection w.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/USCSNVEM/14bfa6bb14875e45bba028a21ed38046-Abstract.html:text/html},
}

@inproceedings{lin_microsoft_2014,
	title = {Microsoft coco: {Common} objects in context},
	shorttitle = {Microsoft coco},
	booktitle = {Computer {Vision}–{ECCV} 2014: 13th {European} {Conference}, {Zurich}, {Switzerland}, {September} 6-12, 2014, {Proceedings}, {Part} {V} 13},
	publisher = {Springer},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C. Lawrence},
	year = {2014},
	pages = {740--755},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/DC6G4YK6/Lin et al. - 2014 - Microsoft coco Common objects in context.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/3R6H3QLR/978-3-319-10602-1_48.html:text/html},
}

@inproceedings{ghiasi_nas-fpn_2019,
	title = {Nas-fpn: {Learning} scalable feature pyramid architecture for object detection},
	shorttitle = {Nas-fpn},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Ghiasi, Golnaz and Lin, Tsung-Yi and Le, Quoc V.},
	year = {2019},
	pages = {7036--7045},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/88GDLNFV/Ghiasi et al. - 2019 - Nas-fpn Learning scalable feature pyramid archite.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/E924YBL7/Ghiasi_NAS-FPN_Learning_Scalable_Feature_Pyramid_Architecture_for_Object_Detection_CVPR_2019_pa.html:text/html},
}

@inproceedings{zhu_deformable_2019,
	title = {Deformable convnets v2: {More} deformable, better results},
	shorttitle = {Deformable convnets v2},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Zhu, Xizhou and Hu, Han and Lin, Stephen and Dai, Jifeng},
	year = {2019},
	pages = {9308--9316},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/J3TETHIK/Zhu et al. - 2019 - Deformable convnets v2 More deformable, better re.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/6G5WGMJB/Zhu_Deformable_ConvNets_V2_More_Deformable_Better_Results_CVPR_2019_paper.html:text/html},
}

@inproceedings{dai_deformable_2017,
	title = {Deformable {Convolutional} {Networks}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Dai_Deformable_Convolutional_Networks_ICCV_2017_paper.html},
	urldate = {2023-05-30},
	author = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
	year = {2017},
	pages = {764--773},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/PA5D55I9/Dai et al. - 2017 - Deformable Convolutional Networks.pdf:application/pdf},
}

@inproceedings{zhu_deformable_2019-1,
	title = {Deformable convnets v2: {More} deformable, better results},
	shorttitle = {Deformable convnets v2},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Zhu, Xizhou and Hu, Han and Lin, Stephen and Dai, Jifeng},
	year = {2019},
	pages = {9308--9316},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/JX5BR2S2/Zhu et al. - 2019 - Deformable convnets v2 More deformable, better re.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/FM8JVKQC/Zhu_Deformable_ConvNets_V2_More_Deformable_Better_Results_CVPR_2019_paper.html:text/html},
}

@inproceedings{lin_feature_2017,
	title = {Feature pyramid networks for object detection},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	year = {2017},
	pages = {2117--2125},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/CAECXF2L/Lin et al. - 2017 - Feature pyramid networks for object detection.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/HXKKZNCE/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.html:text/html},
}

@article{duy_m_h_nguyen_lmgp_2022,
	title = {{LMGP}: {Lifted} {Multicut} {Meets} {Geometry} {Projections} for {Multi}-{Camera} {Multi}-{Object} {Tracking}},
	journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	author = {{Duy M. H. Nguyen} and {Roberto Henschel} and {Bodo Rosenhahn} and {Daniel Sonntag} and {Paul Swoboda}},
	year = {2022},
	pages = {8866--8875},
	file = {Duy M. H. Nguyen et al. - 2022 - LMGP Lifted Multicut Meets Geometry Projections f.pdf:/home/tobias/snap/zotero-snap/common/Zotero/storage/I68J6UMB/Duy M. H. Nguyen et al. - 2022 - LMGP Lifted Multicut Meets Geometry Projections f.pdf:application/pdf},
}

@article{tim_meinhardt_trackformer_2022,
	title = {{TrackFormer}: {Multi}-{Object} {Tracking} with {Transformers}},
	journal = {onference on Computer Vision and Pattern Recognition (CVPR)},
	author = {{Tim Meinhardt} and {Alexander Kirillov} and {Laura Leal-Taixe} and {Christoph Feichtenhofer}},
	year = {2022},
	file = {Tim Meinhardt et al. - 2022 - TrackFormer Multi-Object Tracking with Transforme.pdf:/home/tobias/snap/zotero-snap/common/Zotero/storage/TUPPQSBB/Tim Meinhardt et al. - 2022 - TrackFormer Multi-Object Tracking with Transforme.pdf:application/pdf},
}

@book{hartley_multiple_2003,
	title = {Multiple {View} {Geometry} in {Computer} {Vision}},
	isbn = {978-0-521-54051-3},
	abstract = {A basic problem in computer vision is to understand the structure of a real world scene given several images of it. Techniques for solving this problem are taken from projective geometry and photogrammetry. Here, the authors cover the geometric principles and their algebraic representation in terms of camera projection matrices, the fundamental matrix and the trifocal tensor. The theory and methods of computation of these entities are discussed with real examples, as is their use in the reconstruction of scenes from multiple images. The new edition features an extended introduction covering the key ideas in the book (which itself has been updated with additional examples and appendices) and significant new results which have appeared since the first edition. Comprehensive background material is provided, so readers familiar with linear algebra and basic numerical methods can understand the projective geometry and estimation algorithms presented, and implement the algorithms directly from the book.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Hartley, Richard and Zisserman, Andrew},
	year = {2003},
	note = {Google-Books-ID: si3R3Pfa98QC},
	keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition, Computers / Software Development \& Engineering / Computer Graphics, Technology \& Engineering / Robotics},
}

@article{sagerer_entwicklung_nodate,
	title = {Entwicklung eines {Systems} zur visuellen {Positionsbestimmung} von {Interaktionspartnern}},
	author = {Sagerer, Ing G. and Savas, Ilker and Schmidt, Dipl-Inform Joachim},
}

@article{sagerer_entwicklung_nodate-1,
	title = {Entwicklung eines {Systems} zur visuellen {Positionsbestimmung} von {Interaktionspartnern}},
	author = {Sagerer, Ing G. and Savas, Ilker and Schmidt, Dipl-Inform Joachim},
}

@book{hartley_multiple_2003-1,
	title = {Multiple view geometry in computer vision},
	publisher = {Cambridge university press},
	author = {Hartley, Richard and Zisserman, Andrew},
	year = {2003},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/DMGD3HX6/Hartley and Zisserman - 2003 - Multiple view geometry in computer vision.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/RDGCY6G6/books.html:text/html},
}

@misc{noauthor_3d_nodate,
	title = {{3D} {Photography} / {Camera} {Calibration}},
	url = {http://mesh.brown.edu/3DP-2018/calibration.html},
	urldate = {2023-08-06},
	file = {3D Photography / Camera Calibration:/home/tobias/snap/zotero-snap/common/Zotero/storage/44IDEYYI/calibration.html:text/html},
}

@article{evlero_problema_1770,
	title = {Problema algebraicvm ob affectiones prorsvs singvlares memorabile},
	volume = {15},
	journal = {Novi Commentarii Academiae imperialis scientiarum Petropolitanae},
	author = {EVLERO, Auctore L. and Petropolitanae, Novi Commentarii Academiae Scientiarum Imperialis},
	year = {1770},
	pages = {75--106},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/6TQVMKAQ/EVLERO and Petropolitanae - 1770 - Problema algebraicvm ob affectiones prorsvs singvl.pdf:application/pdf},
}

@article{euler_problema_1770,
	title = {Problema algebraicum ob affectiones prorsus singulares},
	volume = {6},
	journal = {Opera omnia 1st series},
	author = {Euler, L.},
	year = {1770},
	pages = {287--315},
}

@article{shao_crowdhuman_2018,
	title = {Crowdhuman: {A} benchmark for detecting human in a crowd. {arXiv} 2018},
	shorttitle = {Crowdhuman},
	journal = {arXiv preprint arXiv:1805.00123},
	author = {Shao, Shuai and Zhao, Zijian and Li, Boxun and Xiao, Tete and Yu, Gang and Zhang, Xiangyu and Sun, Jian},
	year = {2018},
}

@inproceedings{rezatofighi_generalized_2019,
	address = {Long Beach, CA, USA},
	title = {Generalized {Intersection} {Over} {Union}: {A} {Metric} and a {Loss} for {Bounding} {Box} {Regression}},
	isbn = {978-1-72813-293-8},
	shorttitle = {Generalized {Intersection} {Over} {Union}},
	url = {https://ieeexplore.ieee.org/document/8953982/},
	doi = {10.1109/CVPR.2019.00075},
	abstract = {Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axisaligned 2D bounding boxes, it can be shown that IoU can be directly used as a regression loss. However, IoU has a plateau making it infeasible to optimize in the case of nonoverlapping bounding boxes. In this paper, we address the weaknesses of IoU by introducing a generalized version as both a new loss and a new metric. By incorporating this generalized IoU (GIoU ) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, IoU based, and new, GIoU based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Rezatofighi, Hamid and Tsoi, Nathan and Gwak, JunYoung and Sadeghian, Amir and Reid, Ian and Savarese, Silvio},
	month = jun,
	year = {2019},
	pages = {658--666},
	file = {Rezatofighi et al. - 2019 - Generalized Intersection Over Union A Metric and .pdf:/home/tobias/snap/zotero-snap/common/Zotero/storage/I7WTQ4RN/Rezatofighi et al. - 2019 - Generalized Intersection Over Union A Metric and .pdf:application/pdf},
}

@article{lin_survey_2022,
	title = {A survey of transformers},
	volume = {3},
	issn = {2666-6510},
	url = {https://www.sciencedirect.com/science/article/pii/S2666651022000146},
	doi = {10.1016/j.aiopen.2022.10.001},
	abstract = {Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.},
	urldate = {2023-12-01},
	journal = {AI Open},
	author = {Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
	month = jan,
	year = {2022},
	keywords = {Deep learning, Pre-trained models, Self-attention, Transformer},
	pages = {111--132},
	file = {ScienceDirect Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/H5H72BWA/S2666651022000146.html:text/html},
}

@article{kingma_adam_2014,
	title = {Adam: {A} method for stochastic optimization},
	journal = {arXiv preprint arXiv:1412.6980},
	author = {Kingma, Diederik P and Ba, Jimmy},
	year = {2014},
}

@misc{meinhardt_trackformer_2022-1,
	title = {{TrackFormer}: {Multi}-{Object} {Tracking} with {Transformers}},
	shorttitle = {{TrackFormer}},
	url = {http://arxiv.org/abs/2101.02702},
	abstract = {The challenging task of multi-object tracking (MOT) requires simultaneous reasoning about track initialization, identity, and spatio-temporal trajectories. We formulate this task as a frame-to-frame set prediction problem and introduce TrackFormer, an end-to-end trainable MOT approach based on an encoder-decoder Transformer architecture. Our model achieves data association between frames via attention by evolving a set of track predictions through a video sequence. The Transformer decoder initializes new tracks from static object queries and autoregressively follows existing tracks in space and time with the conceptually new and identity preserving track queries. Both query types benefit from self- and encoder-decoder attention on global frame-level features, thereby omitting any additional graph optimization or modeling of motion and/or appearance. TrackFormer introduces a new tracking-by-attention paradigm and while simple in its design is able to achieve state-of-the-art performance on the task of multi-object tracking (MOT17 and MOT20) and segmentation (MOTS20). The code is available at https://github.com/timmeinhardt/trackformer .},
	urldate = {2023-03-21},
	author = {Meinhardt, Tim and Kirillov, Alexander and Leal-Taixe, Laura and Feichtenhofer, Christoph},
	month = apr,
	year = {2022},
	doi = {10.48550/arXiv.2101.02702},
	note = {Publisher: arXiv},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv:2101.02702 [cs]},
	file = {arXiv Fulltext PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/M4ATMELU/Meinhardt et al. - 2022 - TrackFormer Multi-Object Tracking with Transforme.pdf:application/pdf;arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/M9RLXQ7X/2101.html:text/html},
}

@inproceedings{sutskever_importance_2013-1,
	title = {On the importance of initialization and momentum in deep learning},
	url = {https://proceedings.mlr.press/v28/sutskever13.html},
	abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
	language = {en},
	urldate = {2023-03-21},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	month = may,
	year = {2013},
	pages = {1139--1147},
	annote = {ISSN: 1938-7228},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/SKS9LV8B/Sutskever et al. - 2013 - On the importance of initialization and momentum i.pdf:application/pdf},
}

@book{murphy_machine_2012-1,
	title = {Machine {Learning}: {A} {Probabilistic} {Perspective}},
	isbn = {978-0-262-30432-0},
	shorttitle = {Machine {Learning}},
	abstract = {A comprehensive introduction to machine learning that uses probabilistic models and inference as a unifying approach.Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach.The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package—PMTK (probabilistic modeling toolkit)—that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
	language = {en},
	publisher = {MIT Press},
	author = {Murphy, Kevin P.},
	month = sep,
	year = {2012},
	keywords = {Computers / Artificial Intelligence / General},
	annote = {Google-Books-ID: RC43AgAAQBAJ},
}

@article{lecun_gradient-based_1998-1,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	keywords = {Character recognition, Feature extraction, Hidden Markov models, Machine learning, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis},
	pages = {2278--2324},
	annote = {Conference Name: Proceedings of the IEEE},
	file = {IEEE Xplore Abstract Record:/home/tobias/snap/zotero-snap/common/Zotero/storage/V8JAP5U2/726791.html:text/html;IEEE Xplore Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/KZ8R5BDK/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf},
}

@misc{kingma_adam_2017-1,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2023-03-21},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	doi = {10.48550/arXiv.1412.6980},
	note = {Publisher: arXiv},
	keywords = {Computer Science - Machine Learning},
	annote = {arXiv:1412.6980 [cs]},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {arXiv Fulltext PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/KRVD3LI3/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/H9SCRCW9/1412.html:text/html},
}

@book{hutter_automated_2019-1,
	title = {Automated {Machine} {Learning}: {Methods}, {Systems}, {Challenges}},
	shorttitle = {Automated {Machine} {Learning}},
	url = {https://library.oapen.org/handle/20.500.12657/23012},
	abstract = {This open access book presents the first comprehensive overview of general methods in Automated Machine Learning (AutoML), collects descriptions of existing systems based on these methods, and discusses the first series of international challenges of AutoML systems. The recent success of commercial ML applications and the rapid growth of the field has created a high demand for off-the-shelf ML methods that can be used easily and without expert knowledge. However, many of the recent machine learning successes crucially rely on human experts, who manually select appropriate ML architectures (deep learning architectures or more traditional ML workflows) and their hyperparameters. To overcome this problem, the field of AutoML targets a progressive automation of machine learning, based on principles from optimization and machine learning itself. This book serves as a point of entry into this quickly-developing field for researchers and advanced students alike, as well as providing a reference for practitioners aiming to use AutoML in their work.},
	language = {English},
	urldate = {2023-03-21},
	publisher = {Springer Nature},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	year = {2019},
	doi = {10.1007/978-3-030-05318-5},
	keywords = {Artificial intelligence, bic Book Industry Communication::U Computing \& information technology::UY Computer science::UYQ Artificial intelligence, bic Book Industry Communication::U Computing \& information technology::UY Computer science::UYQ Artificial intelligence::UYQP Pattern recognition, bic Book Industry Communication::U Computing \& information technology::UY Computer science::UYT Image processing, Computer science, Optical data processing, Pattern recognition},
	annote = {Accepted: 2020-03-18 13:36:15},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/GMBQYX57/Hutter et al. - 2019 - Automated Machine Learning Methods, Systems, Chal.pdf:application/pdf},
}

@misc{hanin_approximating_2018-1,
	title = {Approximating {Continuous} {Functions} by {ReLU} {Nets} of {Minimal} {Width}},
	url = {http://arxiv.org/abs/1710.11278},
	abstract = {This article concerns the expressive power of depth in deep feed-forward neural nets with ReLU activations. Specifically, we answer the following question: for a fixed {\textbackslash}d\_\{in\}{\textbackslash}textbackslashgeq 1,{\textbackslash} what is the minimal width {\textbackslash}w{\textbackslash} so that neural nets with ReLU activations, input dimension {\textbackslash}d\_\{in\}{\textbackslash}, hidden layer widths at most {\textbackslash}w,{\textbackslash} and arbitrary depth can approximate any continuous, real-valued function of {\textbackslash}d\_\{in\}{\textbackslash} variables arbitrarily well? It turns out that this minimal width is exactly equal to {\textbackslash}d\_\{in\}+1.{\textbackslash} That is, if all the hidden layer widths are bounded by {\textbackslash}d\_\{in\}{\textbackslash}, then even in the infinite depth limit, ReLU nets can only express a very limited class of functions, and, on the other hand, any continuous function on the {\textbackslash}d\_\{in\}{\textbackslash}-dimensional unit cube can be approximated to arbitrary precision by ReLU nets in which all hidden layers have width exactly {\textbackslash}d\_\{in\}+1.{\textbackslash} Our construction in fact shows that any continuous function {\textbackslash}f:[0,1]ˆ\{d\_\{in\}\}{\textbackslash}textbackslashto{\textbackslash}textbackslashmathbb Rˆ\{d\_\{out\}\}{\textbackslash} can be approximated by a net of width {\textbackslash}d\_\{in\}+d\_\{out\}{\textbackslash}. We obtain quantitative depth estimates for such an approximation in terms of the modulus of continuity of {\textbackslash}f{\textbackslash}.},
	urldate = {2023-03-21},
	author = {Hanin, Boris and Sellke, Mark},
	month = mar,
	year = {2018},
	doi = {10.48550/arXiv.1710.11278},
	note = {Publisher: arXiv},
	keywords = {Computer Science - Computational Complexity, Computer Science - Machine Learning, Mathematics - Combinatorics, Mathematics - Statistics Theory, Statistics - Machine Learning},
	annote = {arXiv:1710.11278 [cs, math, stat]},
	annote = {Comment: v2. 13p. Extended main result to higher dimensional output. Comments welcome},
	file = {arXiv Fulltext PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/YGKLRL9M/Hanin and Sellke - 2018 - Approximating Continuous Functions by ReLU Nets of.pdf:application/pdf;arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/UR7VSNWR/1710.html:text/html},
}

@incollection{feurer_hyperparameter_2019-1,
	address = {Cham},
	series = {The {Springer} {Series} on {Challenges} in {Machine} {Learning}},
	title = {Hyperparameter {Optimization}},
	isbn = {978-3-030-05318-5},
	url = {https://doi.org/10.1007/978-3-030-05318-5_1},
	abstract = {Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning (AutoML) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization (HPO). In this chapter, we give an overview of the most prominent approaches for HPO. We first discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-fidelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.},
	language = {en},
	urldate = {2023-03-21},
	booktitle = {Automated {Machine} {Learning}: {Methods}, {Systems}, {Challenges}},
	publisher = {Springer International Publishing},
	author = {Feurer, Matthias and Hutter, Frank},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	year = {2019},
	doi = {10.1007/978-3-030-05318-5_1},
	pages = {3--33},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/ZBAJEC9I/Feurer and Hutter - 2019 - Hyperparameter Optimization.pdf:application/pdf},
}

@article{duchi_adaptive_2011-1,
	title = {Adaptive {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization}},
	volume = {12},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	number = {61},
	urldate = {2023-03-21},
	journal = {Journal of Machine Learning Research},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	year = {2011},
	pages = {2121--2159},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/9J39X7E7/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.pdf:application/pdf},
}

@article{cybenko_approximation_1989-1,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {1435-568X},
	url = {https://doi.org/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	language = {en},
	number = {4},
	urldate = {2023-03-21},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	keywords = {Approximation, Completeness, Neural networks},
	pages = {303--314},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/X45EW6L2/Cybenko - 1989 - Approximation by superpositions of a sigmoidal fun.pdf:application/pdf},
}

@inproceedings{carion_end--end_2020-1,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	isbn = {978-3-030-58452-8},
	doi = {10.1007/978-3-030-58452-8_13},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {213--229},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/8MD9CFRB/Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017-1,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-03-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/2VKEUES4/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@misc{karpathy_transformer_2022-1,
	type = {Thread},
	title = {The {Transformer} is a magnificient neural network architecture},
	url = {https://twitter.com/karpathy/status/1582807367988654081?lang=en},
	urldate = {2023-03-03},
	journal = {twitter},
	author = {Karpathy, Andrej},
	month = oct,
	year = {2022},
}

@misc{vaswani_transformers_2021-1,
	type = {Guest lecture},
	title = {Transformers and {Self}-{Attention}},
	url = {https://www.youtube.com/watch?v=5vcj8kSwBCY},
	urldate = {2023-03-03},
	author = {Vaswani, Ashish and Huang, Anna},
	month = mar,
	year = {2021},
	note = {Place: Stanford University},
}

@misc{kafunah_backpropagation_2016-1,
	title = {Backpropagation {In} {Convolutional} {Neural} {Networks}},
	url = {https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/},
	urldate = {2023-02-28},
	journal = {Backpropagation In Convolutional Neural Networks},
	author = {Kafunah, Jefkine},
	month = may,
	year = {2016},
}

@misc{arat_backpropagation_2019-1,
	title = {Backpropagation {Through} {Time} for {Recurrent} {Neural} {Network}},
	url = {https://mmuratarat.github.io/2019-02-07/bptt-of-rnn},
	urldate = {2023-02-28},
	author = {Arat, Mustafa Murat},
	month = feb,
	year = {2019},
}

@misc{hinton_lecture_2012-1,
	title = {Lecture 6a. {Overview} of mini-batch gradient descent},
	url = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
	urldate = {2023-02-03},
	author = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
	year = {2012},
	annote = {Accessed: 2020–11-06},
}

@phdthesis{karpathy_connecting_2016-1,
	type = {Dissertation},
	title = {Connecting {Images} and {Natural} {Language}},
	url = {https://cs.stanford.edu/people/karpathy/main.pdf},
	school = {Stanford University},
	author = {Karpathy, Andrej},
	month = aug,
	year = {2016},
	file = {Andrej - 2016 - Connecting Images and Natural Language.pdf:/home/tobias/snap/zotero-snap/common/Zotero/storage/CPAU5NXW/Andrej - 2016 - Connecting Images and Natural Language.pdf:application/pdf},
}

@misc{li_cs231n_2018-1,
	title = {{CS231n}: {Deep} {Learning} for {Computer} {Vision}},
	url = {http://cs231n.stanford.edu/index.html},
	urldate = {2023-02-02},
	author = {Li, Fei-Fei and Wu, Jiajun and Gao, Ruohan},
	year = {2018},
	annote = {Accessed: 2020–11-06},
}

@book{goodfellow_deep_2016-1,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	annote = {http://www.deeplearningbook.org},
	file = {Goodfellow et al. - 2016 - Deep Learning.pdf:/home/tobias/snap/zotero-snap/common/Zotero/storage/K3XKUFFC/Goodfellow et al. - 2016 - Deep Learning.pdf:application/pdf},
}

@misc{alammar_illustrated_2018-1,
	title = {The {Illustrated} {Transformer}},
	url = {https://jalammar.github.io/illustrated-transformer/},
	urldate = {2023-01-25},
	author = {Alammar, Jay},
	month = jun,
	year = {2018},
}

@misc{weng_attention_2018-1,
	title = {Attention? {Attention}!},
	url = {https://lilianweng.github.io/posts/2018-06-24-attention/},
	urldate = {2023-01-25},
	author = {Weng, Lilian},
	month = jun,
	year = {2018},
}

@article{kuhn_hungarian_1955-1,
	title = {The {Hungarian} method for the assignment problem},
	volume = {2},
	issn = {1931-9193},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109},
	doi = {10.1002/nav.3800020109},
	abstract = {Assuming that numerical scores are available for the performance of each of n persons on each of n jobs, the “assignment problem” is the quest for an assignment of persons to jobs so that the sum of the n scores so obtained is as large as possible. It is shown that ideas latent in the work of two Hungarian mathematicians may be exploited to yield a new method of solving this problem.},
	language = {en},
	number = {1-2},
	urldate = {2023-03-21},
	journal = {Naval Research Logistics Quarterly},
	author = {Kuhn, H. W.},
	year = {1955},
	pages = {83--97},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/QA35SNHB/Kuhn - 1955 - The Hungarian method for the assignment problem.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/KC5SY2SI/nav.html:text/html},
}

@misc{noauthor_automatic_nodate,
	title = {Automatic differentiation},
	url = {https://en.wikipedia.org/wiki/Automatic_differentiation},
	urldate = {2023-02-28},
	note = {Publication Title: Wikipedia},
}

@article{cybenko_approximation_1989-2,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	number = {4},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, George},
	year = {1989},
	pages = {303--314},
	annote = {Publisher: Springer},
}

@article{hanin_approximating_2017,
	title = {Approximating continuous functions by relu nets of minimal width},
	journal = {arXiv preprint arXiv:1710.11278},
	author = {Hanin, Boris and Sellke, Mark},
	year = {2017},
}

@inproceedings{he_deep_2016-1,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
	urldate = {2023-03-21},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/LKYCNK52/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@misc{noauthor_automatic_2023-1,
	title = {Automatic differentiation},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Automatic_differentiation&oldid=1143708979},
	abstract = {In mathematics and computer algebra, automatic differentiation (auto-differentiation, autodiff, or AD), also called algorithmic differentiation, computational differentiation, is a set of techniques to evaluate the partial derivative of a function specified by a computer program. Automatic differentiation exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, partial derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.},
	language = {en},
	urldate = {2023-03-21},
	journal = {Wikipedia},
	month = mar,
	year = {2023},
	annote = {Page Version ID: 1143708979},
	file = {Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/HBDDZFS8/Automatic_differentiation.html:text/html},
}

@misc{zhu_deformable_2021-1,
	title = {Deformable {DETR}: {Deformable} {Transformers} for {End}-to-{End} {Object} {Detection}},
	shorttitle = {Deformable {DETR}},
	url = {http://arxiv.org/abs/2010.04159},
	abstract = {DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.},
	urldate = {2023-03-21},
	author = {Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
	month = mar,
	year = {2021},
	doi = {10.48550/arXiv.2010.04159},
	note = {Publisher: arXiv},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv:2010.04159 [cs]},
	annote = {Comment: ICLR 2021 Oral},
	file = {arXiv Fulltext PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/VTAVYF62/Zhu et al. - 2021 - Deformable DETR Deformable Transformers for End-t.pdf:application/pdf;arXiv.org Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/HRU7TVSJ/2010.html:text/html},
}

@article{liu_deep_2018-1,
	title = {Deep learning for generic object detection},
	journal = {A Survey},
	author = {Liu, L. and Ouyang, W. and Wang, X. and Fieguth, P. and Chen, J. and Liu, XJAS},
	year = {2018},
}

@inproceedings{lin_microsoft_2014-1,
	title = {Microsoft {COCO}: {Common} objects in context},
	shorttitle = {Microsoft coco},
	booktitle = {Computer {Vision}–{ECCV} 2014: 13th {European} {Conference}, {Zurich}, {Switzerland}, {September} 6-12, 2014, {Proceedings}, {Part} {V} 13},
	publisher = {Springer},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C. Lawrence},
	year = {2014},
	pages = {740--755},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/4LLDPFF3/Lin et al. - 2014 - Microsoft coco Common objects in context.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/CMIWKLXM/978-3-319-10602-1_48.html:text/html},
}

@inproceedings{dai_deformable_2017-1,
	title = {Deformable {Convolutional} {Networks}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Dai_Deformable_Convolutional_Networks_ICCV_2017_paper.html},
	urldate = {2023-05-30},
	author = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
	year = {2017},
	pages = {764--773},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/F9WQ5X6C/Dai et al. - 2017 - Deformable Convolutional Networks.pdf:application/pdf},
}

@inproceedings{zhu_deformable_2019-2,
	title = {Deformable convnets v2: {More} deformable, better results},
	shorttitle = {Deformable convnets v2},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Zhu, Xizhou and Hu, Han and Lin, Stephen and Dai, Jifeng},
	year = {2019},
	pages = {9308--9316},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/VPIXQ39K/Zhu et al. - 2019 - Deformable convnets v2 More deformable, better re.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/GVV86BVQ/Zhu_Deformable_ConvNets_V2_More_Deformable_Better_Results_CVPR_2019_paper.html:text/html},
}

@inproceedings{lin_feature_2017-1,
	title = {Feature pyramid networks for object detection},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	year = {2017},
	pages = {2117--2125},
	file = {Full Text:/home/tobias/snap/zotero-snap/common/Zotero/storage/YWL5A49E/Lin et al. - 2017 - Feature pyramid networks for object detection.pdf:application/pdf;Snapshot:/home/tobias/snap/zotero-snap/common/Zotero/storage/WRQFWI2K/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.html:text/html},
}

@inproceedings{ren_faster_2015-2,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	volume = {28},
	shorttitle = {Faster {R}-{CNN}},
	url = {https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster\_rcnn.},
	urldate = {2023-05-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	year = {2015},
	file = {Full Text PDF:/home/tobias/snap/zotero-snap/common/Zotero/storage/VPEVFMJS/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf},
}

@inproceedings{feurer_hyperparameter_nodate,
	title = {Hyperparameter {Optimization}},
	author = {Feurer, Matthias and Hutter, Frank},
	pages = {3--38},
	annote = {Section: 1},
}

@article{lin_survey_2022-1,
	title = {A survey of transformers},
	journal = {AI Open},
	author = {Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
	year = {2022},
	note = {Publisher: Elsevier},
}
